
DOCUMENTATION

The project has gone through three different directions, all of which were decided as unviable for various reasons ranging from my lack of ability to a lack of possibility and a lack of sensibility. The following three sub-headers are dedicated to the projects name at that point in time.


Traumagotchi

The first project conception was as follows: an AI which dynamically learns to create associations from its own active behaviours to whatever subject-matter is detected in video content, this was achieved with ofxDarknet and a dopamine inspired system I designed. Ultimately there was no real utility in mind at this point nor did there seem to be a sensible reason for the AI to create associations from video content. The desired outcome of the system was to design an AI which creates associations between types of detected objects such as a dog and a dog bark utilizing machine learning. With a link between re-occuring associations made, the AI would be able to react to a dog bark in the same way it had reacted to dog imagery, as well as dog-like imagry and dog-like sounds. In reality this task was enormously difficult, but tantalizingly possible with current technology. By leveraging powerful neural network algorithms the similarities between content such as dogs and raccoons could in theory allow for realistic behaviour from the AI, such that, the AI which reacts negativly to dogs would also react negatively to raccoons (as they are relatively similar) in proporation to the level of similarity. The approach of utilizing ofxDarknet and ofxSelfOrganizingMap failed to accomplish this lofty goal. The speed of the program was dauntingly slow thanks to Darknets GPU requirements, Windows installation process for things such as tensorflow, Darknet, Dlib, Nvidia drivers, OpenCV was difficult and prone to error. While the fundamental idea behind the system made sense, in pratice the sheer scope of the task caused a sharp increase in difficulty. Attempting to dive headfirst into every level of system design created confusing code and a muddy direction for feature design. Fundamental questions such as 'What causes an association between a dog and a dog-bark to be made?' were still unanswered and had to be answered, time-based proximity was the only thing which made sense with such a large amount of incoming subject related data from Darknet. The logical direction for development that this would entail would be a very difficult to follow through. Take for instance the association between a dog and a dog-bark, in order for an association to be made there would need to be a lot of time spent inspecting dogs barking, enough that the association between a dog and a human talking would be minimal despite this being a very regular occurance. Because the system has no preceptive cababilities beyond camera, microphone, audio and visual classification algorithms, it cannot take into account locality. While it seems possible to design a system which associates movement(a dog opening its mouth) from a recognized subject(in this case a dog), followed by a sound (in this case a bark), the sheer enormity of the scope of such a system is beyond my abilities. The horizon for an AI such as this which can learn potentially any association of sound and subject, as well as subjects with other subjects, motion with certain subjects and motion with certain sound, appears infinite, almost neccesitating feature creep to occur. In practice Darknets poor detection at the low resolution needed for running it was truly the archiles heel of the project at this point, raccoons were detected as elephants often, lions as dogs. There is no doubt that with a much more powerful system, one which is capable of running a plethora of computer vision  and audio feature extraction algorithms in realtime would have inspired a continued effort in this direction (hoping to achieve some semblance of an association system which, while not perfect, could be a good example of the potential power behind the idea).  Regardless, the desired utility of this system was not firmly rooted in reality, a system for users to enjoy as a digital pet should not need mounths of training to comprehend simple facts and a serious robotics engineer should not need a slow user-in-mind system which slowly watches content to learn. With a lack of the ultimate utility and a completely failing system (due to the aforementioned errors and slowness) a shift in direction was needed. 

Euius

Abandoning the idea of the digital pet in favour of an 'AI aquarium' which generates art and music based on its own learned preferences the project was re-named 'Euius'. The association-learner system from the previous iteration was kept with the change that associations would be created by the emotional state in the system and its input (be it from audio or visual content). Additionally, develop was done on Windows 10 as before, utilizing ofxDarknet and ofxSelfOrganizingMap. Lesson finally learned, a solid foundation of basic working systems is needed.
