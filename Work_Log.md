WORK LOG https://github.com/ratmother

This is the work log which I write in at least once a week to keep track of progress and changes in development, it is not documentation of changes to the concept or theory of my application, although I will make mention of it. Documentation of that will be found in the Documentation.md file. 

June: Learned how to use Linux as it supported more addons I wanted to use and it seemed easier to work with python and tensorflow in it, attempted to debug ofxDarknet in Linux and failed (the addon seems to be abandoned by the creators, got a persistent segmentation fault issue which was unresolvable) and decided how to tackle the project. Change from Euius to Kodama (as well as project goals). Failed to debug ofxFacetracker 2, did not feel ofxFacetracker 1 or 2 could actually serve the projects goals well enough so sought other methods, discovered python emotional detection script. Had a rudimentary idea about what kind of emotional tracking I wanted to do. At this point I have changed the direction of the project from digital pet to desktop emotionally-aware DJ companion (detects emotion in user and plays music associated with that emotion, as well as music like it using audio feaute analysis). 

July 5th-18th: Because of the problems with ofxFacetracker 2 and its lack of negative emotion detection, decided to search for a new algorithm. I found a python emotion detection script and a Darknet python script, both work perfectly. I've linked the python emotion script to a rudimentary emotion-balance system in openframeworks with ofxFifio. The emotion-balance system keeps track of the average 'happiness' and 'sadness' of the user, e.g more 'happiness' detected causes a slower decline of a 'happiness' balance value, e.g sudden change from 'sad' to 'happy' causes drastic value change. The emotion-balance system tracks both sudden changes to mood and the stable mood. This is different from the python script which only checks for the immediate mood. I have succeeded  in debugging ofxAudioanalyer after following the errors through. I am now planning to figure out how to get realtime vocal information to the python emotion detection script with ofxFifo (python script only takes in one .wav file and does one analysis). I have started a project planner.

July 18th 2019: Sent pull request to ofxAudioAnalzyer at request of creator after discovering a fix for the addon (mentioned in last entry), it was not working on Linux because a functions input did not much the header functions input -- somehow this went by without notice on the working Mac version. Thread here: https://github.com/leozimmerman/ofxAudioAnalyzer/issues/17 . I've downloaded Bitwig (planning to create genrative music with OSC messages), direction of project changed slightly towards generative music desktop companion that can also act as a party/installation type system. 

Working idea: System tracks emotion and movement information in user(s) and decides which sounds (type of synth, beat tempo, brightness, key) to play based on this information (heavy fast music for high energy movement and 'happiness', slow ambient for light movement and 'sadness'. System can learn (using audio feature extraction and emotion-balance system) what types of sounds cause changes from high-emotion to low-emotion and leverage this learned information.

July 19th 2019: Got Bitwig up and running on Linux (the current plan is to utilize it for generative music creation), all the bells and whistles are going fine -- the audio engine crashes often however, I need to fix it before I start working seriously in Bitwig. The python tensorflow voice emotion detector is working and recieving information from ofxAudioanalyzer from openframeworks! I worked literally all day on this and am very pleased everything is working. Changed MELBANDS_BANDS_NUM from 24 to 54 and DCT_COEFF_NUM from 13 to 40 in order for it to fit the requirements of the keras model used in the python script LivePredictions.py. Most of my time was put into the modifications I made to this script in order to get it to work with ofxFifo. I created a new function called write_array to ofxFifo, this function transforms a vector<float> to a string and writes it to the pipe file for python to read. I plan to make more modifications to make it so that it does not set up the keras model upon every detection which is ineffiecent. 

July 20th-27th: I changed direction for the plans for the project again, going from party assisting AI DJ to content-association learner. I think I'll leave the ultimate utility of the application for now, but I'm heavily leaning towards a content-association learner. By that I mean a program which learns to associate certain behaviours with certain outcomes e.g behaviour 'showing a video of a lion' produces smiling or wakefulness (detected with Python tensorflow scripts). The target audience would seem to be the age group toddler and under. The video and audio content should be customizable however, which means it could be for any age group. Similar content can be learned about with machine learning, utilizing MFCCs and other features for audio, however for video I am unsure about using a 'similar content' finder as it could end up utilizing too much computer power. I figured it might be useful to try developing on Android as the new direction for the application seems suited to a mobile device with recording capabilities. With all but one of the python scripts using MIT license, I had the passing thought that this could be potentially commericially viable. I have gotten Openframeworks, along with ofxAudioanalyzer and ofxFifo, to build in Android Studio 3 and run very breifly on my Android phone -- however there is an error (I started a topic here for it https://forum.openframeworks.cc/t/android-studio-builds-but-crashes-on-run-couldnt-find-libofandroidapp-so/33089). With the hope that this will be fixed I am going to continue to develop for Linux for now. I have recently discovered a laughter and drowsiness detector Python script which I will merge with my main emotion detection script. 

Working idea: A collection of .wav files is stored in a folder, as is a collection of .mp4 files. These files are analyzed for similarities between eachother and labeled accordingly using machine learning. The program creates behaviours such as 'present ___.wav" or "play ____.mp3", then, using the emotional detection algorithm (using the Python scripts) decides which emotional label the behaviour gets e.g 'sad', 'happy', 'funny', 'exciting', 'relaxing' based on the reaction of the user(s). Mp4 and wav files get labeled with an emotion and excitibility. During the programs resting periods it adds any new mp4 or wav files as samples in the content classification algorithms (output is emotion and excitibility, this can be used to predict effect of unused sounds and video). 

July 28th: Researh into Audio feature extraction and what is known as 'audio TSNE' has been going okay, I've found some good resoures (https://medium.com/@LeonFedden/comparative-audio-analysis-with-wavenet-mfccs-umap-t-sne-and-pca-cb8237bfce2f and https://ml4a.github.io/guides/AudioTSNEViewer/), the challenge of audio analysis of longer clips may be a problem for me so I intend to follow these guides and create an audio TSNE. If I can manage to successfully create a script which processes variant length audio clips the fundamental building blocks of my application would be complete (as the other required building block, the emotion detection, is working perfectly currently). 

July 29th-August 5th: I've added a drowsiness detector to the emotion detection python script. Altogether, building on from the previous iterations of the script, the script now returns a classification for the users drowsiness amount, vocal emotion, and facial emotion. The drowsiness detector is usually used for detecting sleepiness in drivers, I think my utilization of it is quite novel. I should be able to label video files and sound files with their tendancy towards causing drowsiness and certain emotions as intended. Frustratingly I've recieved no help from the OF forums yet regarding my problem with Android Studio. I've managed to get a PCA analyzer of MFCCs working in a python script (I got it from the medium article in the July 28th entry), this will allow the system to be access similar sounds, e.g kicks with other kicks and high hats with other high hats. The usage in my program will be to give sounds in the data bin which are near in eachother in the PCA analysis a similar set of emotion labels (relative to the distance in the feature space). I am worried this method will not work with longer audio files, such as songs, it does however handle variant sizes in my current library of drum sounds just fine. I opted for PCA/MFCC because I am already using MFCC's and PCA seemed to be the fastest.

Augest 5th-Sep 23rd: I had some problems managing time for this period but I managed to get some stuff done. The list of emotion-detection attributes from python comes into the C++ program as a block of text 'happy, sad, 6', it is converted to a vector now, ['happy','sad', 6]. I've started designing the content (currently content is only sound files) learning algorithm, it currently works as follows: while content is playing add the current emotion level (e.g happiness being a positive number, sadness a negative) to a growing vector at regular intervals (or snapshots). This vector is averaged producing one number, at the end of the contents playtime (or when a certain amount of snapshots have been captured) add the averaged vector number as a data point in the contents struct object (called a synapse). As each sound has its own x, y position from the PCA analyzer, these emotion data points will be helpful in the learning system to be designed next. At this point, assuming nothing goes wrong with the voice emotion detection, which is untested, the emotion detection part of this project is mostly done.

25th Sep: Refactored code, now there are two .cpp/.h files, one is for a class called synapse which contains the name of the contents source file as well as emotion levels captured during its play time. The other file is the central environment for the application, containing the main update, setup and draw functions as well as audio analysis. Previously the audio player was contained within the synapse, this was changed due to the ineffiencitcy of each synapse having a player. A player system which has its own functions for playing synapses should be implemented as each synapse needs to be alerted to the start and end of content and multiple synapses should be able to run at the same time (not possible with one player, possibly inefficent with each synapse having a player).

27th Sep: I've added two ways of averaging the synapses data points (the data points are the averaged emotion values that are stored at the end of the content), one is a simplistic averaging function which is weighted towards new entries (e.g newest entry is multiplied by 1 and oldest by 0.1, if there are ten entries, before being averaged together), the other is an EMA with a period of one. I'm not certain which one is better, so I've commited both to github for future reference. The EMA function has shown to be better so far in testing and it is more intuitive. With EMA a new data point that is higher than average pulls the average up (or down if the new data point is lower than average).

28th Sep: Neither of the two averaging functions I added worked very well, so I spent some time making a modified EMA(exponential moving average), my function simply is weighted in favour of the newest 15 (or however many is set) entries. This is done with the following formula (((avgNewEntries - avgAllEntries)/2) + avgAllEntries). I believe to make it more like a proper EMA I would need to replace 'avgAllEntries' with the result of this formula after the first pass.
Regardless, I now have a working averaging system for the input emotions (currently just facial expressions and drowsiness) during content playing, content (in this case a sound file) is now associated with an estimated emotional state and a drowsiness state (aquired by the emotion averaging system which is active when the sound file is playing). I've condencesed and refactored the code as well, making it easier to understand and use. My plan next is to work on movement detection, attentioned detection and finish vocal emotion detection as well as add a simple function to set values for different emotional states. It will be easy to plug in these new detection inputs into the averaging system. 

28th Sep to Oct 21st: I forgot about my work log! Silly me. I got a lot done. Movement detection is working fine and the moving average is more refined, each synapse has its own collection of vectors which each have a weighted average wrt user detected information when that synapse was active (facial expression, movement, drowsiness). Each detected type, facial, drowsy, movement, has its own object which stores the value of that input as well as its stability. To process stability I made a function inpDiff which compares the distance between current input to the expression input types and compares it with the average value of that particlar input type in the recent past, and then adds that to the types object -- which is then saved to whatever synapse is running, to be added to its internal averaging system. In effect, if there is a large amount of insatbility in the expression input the current running synapse will save the amount of instability -- meaning this synapse being active correlates to instability in that type of expression. 

Additionally, I've gotten the audio-comparative-analysis Python script function as originally intended, luckily, things worked relatively well without too much stress. After trying to make my own distance checker with the PCA coordinates I had from the previous iteration of the script, I decided to instead use a nearest nehighbour algo (I was already using sklearn for the PCA, nearest neighbour is conviently a part of the libary). The script now functions to return similar sounds to microphone input (an array of floats is what librosa expects so I used pyaudio and then convert microhpone input to an array of floats), it does this with PCA and nearest neighbour algorithms in sklearn. Other than the sklearn functions, overall structure, and buffering algorithm, the script is two scripts hacked together -- one from a medium article about feature extraction for ML and another basic one for pyaudio mic recording. 

The script works like this: 
Set up:
Directory of sound-content --> features extracted by librosa --> PCA --> Collection of coordinates with file names (kick drums nearest to each other, etc)
Usage:
Input microphone recorded and placed into buffer --> converted to floats appended to list of float arrays --> concatenated --> features exctracted by librosa --> PCA --> nearest neighbour gives 5 nearest sounds

I have some potential worries about scaling, there might be problems down the road unless PCA remains very quick on a larger amount of sound files -- its fine if the inital set up takes a long time, but the real time PCA needs to be quick. At the moment, my microphone is not working and only picks up noise so I need to get a new mic, but I was able to (happily) confirm that the script works because it detected that the nearest sound was noise.wav. It was a very happy moment.

22nd Oct: Cleaned up code and added comments in prepartion for some changes. First, I want to try to move the PCA/NNeighbour algos into C++, as I could use those algos with the synapses as it is too convoluted to move data back and forth to Python from C++.

Ok, so Ive devided to use essentia to accomplish this goal and I've run into problems with building the library and using it, declaring an issue here https://github.com/MTG/essentia/issues/921 ... sadly in the process of installing ffemeg my audio_analysis python script is broken and gives me a cryptic error to do with ALSA. There isn't many replies to a lot of the issues, which is worrying. 

Later on in the day I fixed the cryptic error with ALSA by changing the device index. Essentia's monoloader still doesn't work. I've decided against using Essentia for now given difficulty of using it and frankly being unlikely to recieve help, plus I have what I want working in python. 

23rd Oct: I've added the following today: C++ reads the list of sound files saved out from python and creates a synapse for each one, placing all of them into a 'synapses' vector (the index-to-file is the same in this vector as the dataset index-to-file is in python) and it also reads the audio analysis python results which is a bunch of indexes (the indexes of the sound files nearest to the microphone input, this was accomplished last week talked about in previous entries). C++ then plays the synapses (plays their file-path and collects expression data from the user(s)) that the indexes point to. Effectivly, C++ plays the sound files, synapses, known to be nearest in similarity to the incoming microphone input and records user expression data while they play. 

I've mostly finialized whats been in the works for a few weeks now, adding for loops and vectors and connecting the audio analysis python script to C++ properly. Now to test how many much it can handle, I also need to get a working mircophone to properly test the user experience when it comes to audio-reflection. I can still start to work on the 'reccomendation' (checking synapses expression values, eg. play what is known to cause happiness/movement out of the nearest sounds to the mic input) system for the synapses in the mean time. 

24th Oct: I've fixed up the audio analysis script so that it saves out feature data using pickle and reads it using pickle(avoiding having to process large directories of sounds more than once) and it works perfectly. Even with a far larger amount of files than before, the PCA real-time analysis is pretty quick. Obviously the intial feature extraction is much longer, but with the saving-out feature, thats totally acceptable. Openframeworks' soundplayer is reponsive and can play multiple files at the same time, the interaction between the script and Openframeworks is very perfomative thanks to the simplicity of the information being exchanged. Right now, the system plays sounds similar to the microphone input (which is static noise at the moment due to some problems with my computers microphone) , 5 sounds at a time with each key press. I'll need to start creating the system which governs which of the nearest sounds to play based on their synapse information. 

24th Oct - 14th Nov: I waited to get a microphone before further testing (of audio_comparative_analysis.py) and have been testing the systems ability to correctly select similar sounds to microphone input for a few days. It wasn't easy to convert microphone input into something which librosa can use, I had to do a lot of digging and found this post https://blog.francoismaillet.com/epic-celebration/ which allowed me to rethink my method of microphone-to-buffer-to-librosa. I took the ring buffer in callback idea from here. Testing shows that the system detects things generally well now, whereas before it was chaotic and impossible to test, but further testing with a bigger dataset is required. The system will play drum-like sounds when listening to drummers, and will play human sounds when listening to humans, as predicted and intended the system will play sounds that are not immediately obviously related to what its listening to but shares qualities.

It was a bit of a shot in the dark, I didn't know if the methods I was using to store audio information into the buffer would come out the same as how the audio files are loaded in via librosa.load(). But because the epic-celebration blog post is doing something similar to me, I hoped that whatever the blogger is not showing about their system wouldn't be a problem and it doesn't seem to be. 

A few problems I could see occuring in the future is:

How exactly does the difference in time between the audio files effect the system? The epic-celebration system requires similarly timed inputs because all of its dataset uses the same amount of time. What am I sacrificing (or frankly completely missing) by ignoring time? I specficially chose a non-time based MFCC feature calculator to avoid time, it simply collects an average of all the MFCCs in a file at a set amount, but I do not know how this might effect the overrall system.

The system might be sensetive to different mircophones.

It seemed like the system factored in loudness too much, when I turned down the mic input it seemed to become more accurate, it might not be a bad idea to add information to the data so that its not just using MFCCs. Or perhaps even out the loudness of the data somehow. 

15th Nov: More testing has revealed that erroneous selections of (not)similar sounds was due to my low quality cheap microphone, the mic is extremely bass heavy and has a ringing high pitch as well as mild-to-loud noise. This was a great find! I was wondering why so often my system would choose 808s, noise, and high pitch piano sounds all the time. So to find out that it was actually working perfectly was great. In fact its selection of similar sounds diagnosed the problems with the microphone fairly accurately, neat! It is likely to became more accurate at low volumes (as noted yesterday) because the microphones signal issues would be less prominent. I could EQ out the problematic frequncies but I'm a music producer, why don't I have a good microphone yet? No more cheaping out on microphones for me.

16th Nov: Well, it wasn't 'working perfectly', pyaudio was mangling the audio -- I recorded out what audio data saved to the buffer to a .wav and it was pure glitchy noise. To fix this, I increased the buffer length in the pyaudio stream. Things now do actually work perfectly, although the microphone quality is still impressing itselfs onto the results far too much.
Today I've added a new feature, the script now will record 'abnormal' sounds, sounds which are above a preset distance-threshold ( the microphone inputs PCA coordinates collective distance from 5 nearest sounds are what is checked against, this is done for each ringbuffer) will have their features saved and the audio rendered to a .wav file. I've also added 5 more buffers, each capturing a different length of audio to give the system both short term and long term audio signals as inputs to extract features from and then to compare PCA coordinates with the dataset. The result is working and has interesting results, the system seems to 'ease' into the microphone and ignores the original dataset after awhile because the microphone has qualities which are not present in the original dataset, but are present in the abnormal sounds (which are added to the dataset). The system is very responsive to its environment including its sensory organ the microphone. 



16th Nov – December 30th: I got a lot done in this period and made signifiant changes to the data pipeline, it is mostly documented in the changes made on github rather than through this work log, so I am going to compare the changes I’ve made to the previous versions on github in another entry to this log when I get the time. Quick summary: I added my own method of segmenting the audio data in both the dataset and the input, the scripts now split audio into chunks of various sizes and extract features from the chunks. In the retrieval part of the system it compares the PCA (using feature vectors) as before, but now instead of whole files or buffers it now only compares  chunks of a similar size and retrieves the nearest chunks audio data.  This is very expensive computationally, especially for ‘real time’ analysis so I did a lot of optimizing (to the best of my ability, its still quite slow). I’ve found this is more accurate then before and has more interesting results e.g Kodama listens to pop music and brings up a selection of differently sized singing samples, drum samples, and samples which sound similar to the musics content. I still don’t know if it is ‘transform-independent’ for lack of a better term, if the features I’m using really capture the essence of the sound-content rather than room, distance, space.
