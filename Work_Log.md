WORK LOG https://github.com/ratmother

This is the work log which I write in at least once a week to keep track of progress and changes in development, it is not documentation of changes to the concept or theory of my application, although I will make mention of it. Documentation of that will be found in the Documentation.md file. 

June: Learned how to use Linux as it supported more addons I wanted to use and it seemed easier to work with python and tensorflow in it, attempted to debug ofxDarknet in Linux and failed (the addon seems to be abandoned by the creators, got a persistent segmentation fault issue which was unresolvable) and decided how to tackle the project. Change from Euius to Kodama (as well as project goals). Failed to debug ofxFacetracker 2, did not feel ofxFacetracker 1 or 2 could actually serve the projects goals well enough so sought other methods, discovered python emotional detection script. Had a rudimentary idea about what kind of emotional tracking I wanted to do. At this point I have changed the direction of the project from digital pet to desktop emotionally-aware DJ companion (detects emotion in user and plays music associated with that emotion, as well as music like it using audio feaute analysis). 

July 5th-18th: Because of the problems with ofxFacetracker 2 and its lack of negative emotion detection, decided to search for a new algorithm. I found a python emotion detection script and a Darknet python script, both work perfectly. I've linked the python emotion script to a rudimentary emotion-balance system in openframeworks with ofxFifio. The emotion-balance system keeps track of the average 'happiness' and 'sadness' of the user, e.g more 'happiness' detected causes a slower decline of a 'happiness' balance value, e.g sudden change from 'sad' to 'happy' causes drastic value change. The emotion-balance system tracks both sudden changes to mood and the stable mood. This is different from the python script which only checks for the immediate mood. I have succeeded  in debugging ofxAudioanalyer after following the errors through. I am now planning to figure out how to get realtime vocal information to the python emotion detection script with ofxFifo (python script only takes in one .wav file and does one analysis). I have started a project planner.

July 18th 2019: Sent pull request to ofxAudioAnalzyer at request of creator after discovering a fix for the addon (mentioned in last entry), it was not working on Linux because a functions input did not much the header functions input -- somehow this went by without notice on the working Mac version. Thread here: https://github.com/leozimmerman/ofxAudioAnalyzer/issues/17 . I've downloaded Bitwig (planning to create genrative music with OSC messages), direction of project changed slightly towards generative music desktop companion that can also act as a party/installation type system. 

Working idea: System tracks emotion and movement information in user(s) and decides which sounds (type of synth, beat tempo, brightness, key) to play based on this information (heavy fast music for high energy movement and 'happiness', slow ambient for light movement and 'sadness'. System can learn (using audio feature extraction and emotion-balance system) what types of sounds cause changes from high-emotion to low-emotion and leverage this learned information.

July 19th 2019: Got Bitwig up and running on Linux (the current plan is to utilize it for generative music creation), all the bells and whistles are going fine -- the audio engine crashes often however, I need to fix it before I start working seriously in Bitwig. The python tensorflow voice emotion detector is working and recieving information from ofxAudioanalyzer from openframeworks! I worked literally all day on this and am very pleased everything is working. Changed MELBANDS_BANDS_NUM from 24 to 54 and DCT_COEFF_NUM from 13 to 40 in order for it to fit the requirements of the keras model used in the python script LivePredictions.py. Most of my time was put into the modifications I made to this script in order to get it to work with ofxFifo. I created a new function called write_array to ofxFifo, this function transforms a vector<float> to a string and writes it to the pipe file for python to read. I plan to make more modifications to make it so that it does not set up the keras model upon every detection which is ineffiecent. 

July 20th-27th: I changed direction for the plans for the project again, going from party assisting AI DJ to content-association learner. I think I'll leave the ultimate utility of the application for now, but I'm heavily leaning towards a content-association learner. By that I mean a program which learns to associate certain behaviours with certain outcomes e.g behaviour 'showing a video of a lion' produces smiling or wakefulness (detected with Python tensorflow scripts). The target audience would seem to be the age group toddler and under. The video and audio content should be customizable however, which means it could be for any age group. Similar content can be learned about with machine learning, utilizing MFCCs and other features for audio, however for video I am unsure about using a 'similar content' finder as it could end up utilizing too much computer power. I figured it might be useful to try developing on Android as the new direction for the application seems suited to a mobile device with recording capabilities. With all but one of the python scripts using MIT license, I had the passing thought that this could be potentially commericially viable. I have gotten Openframeworks, along with ofxAudioanalyzer and ofxFifo, to build in Android Studio 3 and run very breifly on my Android phone -- however there is an error (I started a topic here for it https://forum.openframeworks.cc/t/android-studio-builds-but-crashes-on-run-couldnt-find-libofandroidapp-so/33089). With the hope that this will be fixed I am going to continue to develop for Linux for now. I have recently discovered a laughter and drowsiness detector Python script which I will merge with my main emotion detection script. 

Working idea: A collection of .wav files is stored in a folder, as is a collection of .mp4 files. These files are analyzed for similarities between eachother and labeled accordingly using machine learning. The program creates behaviours such as 'present ___.wav" or "play ____.mp3", then, using the emotional detection algorithm (using the Python scripts) decides which emotional label the behaviour gets e.g 'sad', 'happy', 'funny', 'exciting', 'relaxing' based on the reaction of the user(s). Mp4 and wav files get labeled with an emotion and excitibility. During the programs resting periods it adds any new mp4 or wav files as samples in the content classification algorithms (output is emotion and excitibility, this can be used to predict effect of unused sounds and video). 

July 28th: Researh into Audio feature extraction and what is known as 'audio TSNE' has been going okay, I've found some good resoures (https://medium.com/@LeonFedden/comparative-audio-analysis-with-wavenet-mfccs-umap-t-sne-and-pca-cb8237bfce2f and https://ml4a.github.io/guides/AudioTSNEViewer/), the challenge of audio analysis of longer clips may be a problem for me so I intend to follow these guides and create an audio TSNE. If I can manage to successfully create a script which processes variant length audio clips the fundamental building blocks of my application would be complete (as the other required building block, the emotion detection, is working perfectly currently). 

July 29th-August 5th: I've added a drowsiness detector to the emotion detection python script. Altogether, building on from the previous iterations of the script, the script now returns a classification for the users drowsiness amount, vocal emotion, and facial emotion. The drowsiness detector is usually used for detecting sleepiness in drivers, I think my utilization of it is quite novel. I should be able to label video files and sound files with their tendancy towards causing drowsiness and certain emotions as intended. Frustratingly I've recieved no help from the OF forums yet regarding my problem with Android Studio. I've managed to get a PCA analyzer of MFCCs working in a python script (I got it from the medium article in the July 28th entry), this will allow the system to be access similar sounds, e.g kicks with other kicks and high hats with other high hats. The usage in my program will be to give sounds in the data bin which are near in eachother in the PCA analysis a similar set of emotion labels (relative to the distance in the feature space). I am worried this method will not work with longer audio files, such as songs, it does however handle variant sizes in my current library of drum sounds just fine. I opted for PCA/MFCC because I am already using MFCC's and PCA seemed to be the fastest.

Augest 5th-Sep 23rd: I had some problems managing time for this period but I managed to get some stuff done. The list of emotion-detection attributes from python comes into the C++ program as a block of text 'happy, sad, 6', it is converted to a vector now, ['happy','sad', 6]. I've started designing the content (currently content is only sound files) learning algorithm, it currently works as follows: while content is playing add the current emotion level (e.g happiness being a positive number, sadness a negative) to a growing vector at regular intervals (or snapshots). This vector is averaged producing one number, at the end of the contents playtime (or when a certain amount of snapshots have been captured) add the averaged vector number as a data point in the contents struct object (called a synapse). As each sound has its own x, y position from the PCA analyzer, these emotion data points will be helpful in the learning system to be designed next. At this point, assuming nothing goes wrong with the voice emotion detection, which is untested, the emotion detection part of this project is mostly done.

25th Sep: Refactored code, now there are two .cpp/.h files, one is for a class called synapse which contains the name of the contents source file as well as emotion levels captured during its play time. The other file is the central environment for the application, containing the main update, setup and draw functions as well as audio analysis. Previously the audio player was contained within the synapse, this was changed due to the ineffiencitcy of each synapse having a player. A player system which has its own functions for playing synapses should be implemented as each synapse needs to be alerted to the start and end of content and multiple synapses should be able to run at the same time (not possible with one player, possibly inefficent with each synapse having a player).

27th Sep: I've added two ways of averaging the synapses data points (the data points are the averaged emotion values that are stored at the end of the content), one is a simplistic averaging function which is weighted towards new entries (e.g newest entry is multiplied by 1 and oldest by 0.1, if there are ten entries, before being averaged together), the other is an EMA with a period of one. I'm not certain which one is better, so I've commited both to github for future reference. The EMA function has shown to be better so far in testing and it is more intuitive. With EMA a new data point that is higher than average pulls the average up (or down if the new data point is lower than average).
