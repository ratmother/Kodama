WORK LOG https://github.com/ratmother

This is the work log which I write in at least once a week to keep track of progress and changes in development, it is not documentation of changes to the concept or theory of my application, although I will make mention of it. Documentation of that will be found in the Documentation.md file. 

June: Learned how to use Linux as it supported more addons I wanted to use and it seemed easier to work with python and tensorflow in it, attempted to debug ofxDarknet in Linux and failed (the addon seems to be abandoned by the creators, got a persistent segmentation fault issue which was unresolvable) and decided how to tackle the project. Change from Euius to Kodama (as well as project goals). Failed to debug ofxFacetracker 2, did not feel ofxFacetracker 1 or 2 could actually serve the projects goals well enough so sought other methods, discovered python emotional detection script. Had a rudimentary idea about what kind of emotional tracking I wanted to do. At this point I have changed the direction of the project from digital pet to desktop emotionally-aware DJ companion (detects emotion in user and plays music associated with that emotion, as well as music like it using audio feature analysis). 

July 5th-18th: Because of the problems with ofxFacetracker 2 and its lack of negative emotion detection, decided to search for a new algorithm. I found a python emotion detection script and a Darknet python script, both work perfectly. I've linked the python emotion script to a rudimentary emotion-balance system in openframeworks with ofxFifio. The emotion-balance system keeps track of the average 'happiness' and 'sadness' of the user, e.g more 'happiness' detected causes a slower decline of a 'happiness' balance value, e.g sudden change from 'sad' to 'happy' causes drastic value change. The emotion-balance system tracks both sudden changes to mood and the stable mood. This is different from the python script which only checks for the immediate mood. I have succeeded  in debugging ofxAudioanalyer after following the errors through. I am now planning to figure out how to get realtime vocal information to the python emotion detection script with ofxFifo (python script only takes in one .wav file and does one analysis). I have started a project planner.

July 18th 2019: Sent pull request to ofxAudioAnalzyer at request of creator after discovering a fix for the addon (mentioned in last entry), it was not working on Linux because a functions input did not much the header functions input -- somehow this went by without notice on the working Mac version. Thread here: https://github.com/leozimmerman/ofxAudioAnalyzer/issues/17 . I've downloaded Bitwig (planning to create genrative music with OSC messages), direction of project changed slightly towards generative music desktop companion that can also act as a party/installation type system. 

Working idea: System tracks emotion and movement information in user(s) and decides which sounds (type of synth, beat tempo, brightness, key) to play based on this information (heavy fast music for high energy movement and 'happiness', slow ambient for light movement and 'sadness'. System can learn (using audio feature extraction and emotion-balance system) what types of sounds cause changes from high-emotion to low-emotion and leverage this learned information.

July 19th 2019: Got Bitwig up and running on Linux (the current plan is to utilize it for generative music creation), all the bells and whistles are going fine -- the audio engine crashes often however, I need to fix it before I start working seriously in Bitwig. The python tensorflow voice emotion detector is working and recieving information from ofxAudioanalyzer from openframeworks! I worked literally all day on this and am very pleased everything is working. Changed MELBANDS_BANDS_NUM from 24 to 54 and DCT_COEFF_NUM from 13 to 40 in order for it to fit the requirements of the keras model used in the python script LivePredictions.py. Most of my time was put into the modifications I made to this script in order to get it to work with ofxFifo. I created a new function called write_array to ofxFifo, this function transforms a vector<float> to a string and writes it to the pipe file for python to read. I plan to make more modifications to make it so that it does not set up the keras model upon every detection which is inefficient. 

July 20th-27th: I changed direction for the plans for the project again, going from party assisting AI DJ to content-association learner. I think I'll leave the ultimate utility of the application for now, but I'm heavily leaning towards a content-association learner. By that I mean a program which learns to associate certain behaviours with certain outcomes e.g behaviour 'showing a video of a lion' produces smiling or wakefulness (detected with Python tensorflow scripts). The target audience would seem to be the age group toddler and under. The video and audio content should be customizable however, which means it could be for any age group. Similar content can be learned about with machine learning, utilizing MFCCs and other features for audio, however for video I am unsure about using a 'similar content' finder as it could end up utilizing too much computer power. I figured it might be useful to try developing on Android as the new direction for the application seems suited to a mobile device with recording capabilities. With all but one of the python scripts using MIT license, I had the passing thought that this could be potentially commercially viable. I have gotten Openframeworks, along with ofxAudioanalyzer and ofxFifo, to build in Android Studio 3 and run very breifly on my Android phone -- however there is an error (I started a topic here for it https://forum.openframeworks.cc/t/android-studio-builds-but-crashes-on-run-couldnt-find-libofandroidapp-so/33089). With the hope that this will be fixed I am going to continue to develop for Linux for now. I have recently discovered a laughter and drowsiness detector Python script which I will merge with my main emotion detection script. 

Working idea: A collection of .wav files is stored in a folder, as is a collection of .mp4 files. These files are analysed for similarities between each other and labelled accordingly using machine learning. The program creates behaviours such as 'present ___.wav" or "play ____.mp3", then, using the emotional detection algorithm (using the Python scripts) decides which emotional label the behaviour gets e.g 'sad', 'happy', 'funny', 'exciting', 'relaxing' based on the reaction of the user(s). Mp4 and wav files get labelled with an emotion and excitability. During the programs resting periods it adds any new mp4 or wav files as samples in the content classification algorithms (output is emotion and excitability, this can be used to predict effect of unused sounds and video). 

July 28th: Research into Audio feature extraction and what is known as 'audio TSNE' has been going okay, I've found some good resoures (https://medium.com/@LeonFedden/comparative-audio-analysis-with-wavenet-mfccs-umap-t-sne-and-pca-cb8237bfce2f and https://ml4a.github.io/guides/AudioTSNEViewer/), the challenge of audio analysis of longer clips may be a problem for me so I intend to follow these guides and create an audio TSNE. If I can manage to successfully create a script which processes variant length audio clips the fundamental building blocks of my application would be complete (as the other required building block, the emotion detection, is working perfectly currently). 

July 29th-August 5th: I've added a drowsiness detector to the emotion detection python script. Altogether, building on from the previous iterations of the script, the script now returns a classification for the users drowsiness amount, vocal emotion, and facial emotion. The drowsiness detector is usually used for detecting sleepiness in drivers, I think my utilization of it is quite novel. I should be able to label video files and sound files with their tendency towards causing drowsiness and certain emotions as intended. Frustratingly I've received no help from the OF forums yet regarding my problem with Android Studio. I've managed to get a PCA analyser of MFCCs working in a python script (I got it from the medium article in the July 28th entry), this will allow the system to be access similar sounds, e.g kicks with other kicks and high hats with other high hats. The usage in my program will be to give sounds in the data bin which are near in each other in the PCA analysis a similar set of emotion labels (relative to the distance in the feature space). I am worried this method will not work with longer audio files, such as songs, it does however handle variant sizes in my current library of drum sounds just fine. I opted for PCA/MFCC because I am already using MFCC's and PCA seemed to be the fastest.

August 5th-Sep 23rd: I had some problems managing time for this period but I managed to get some stuff done. The list of emotion-detection attributes from python comes into the C++ program as a block of text 'happy, sad, 6', it is converted to a vector now, ['happy','sad', 6]. I've started designing the content (currently content is only sound files) learning algorithm, it currently works as follows: while content is playing add the current emotion level (e.g happiness being a positive number, sadness a negative) to a growing vector at regular intervals (or snapshots). This vector is averaged producing one number, at the end of the contents playtime (or when a certain amount of snapshots have been captured) add the averaged vector number as a data point in the contents struct object (called a synapse). As each sound has its own x, y position from the PCA analyser, these emotion data points will be helpful in the learning system to be designed next. At this point, assuming nothing goes wrong with the voice emotion detection, which is untested, the emotion detection part of this project is mostly done.

25th Sep: Refactored code, now there are two .cpp/.h files, one is for a class called synapse which contains the name of the contents source file as well as emotion levels captured during its play time. The other file is the central environment for the application, containing the main update, setup and draw functions as well as audio analysis. Previously the audio player was contained within the synapse, this was changed due to the inefficiency of each synapse having a player. A player system which has its own functions for playing synapses should be implemented as each synapse needs to be alerted to the start and end of content and multiple synapses should be able to run at the same time (not possible with one player, possibly inefficient with each synapse having a player).

27th Sep: I've added two ways of averaging the synapses data points (the data points are the averaged emotion values that are stored at the end of the content), one is a simplistic averaging function which is weighted towards new entries (e.g newest entry is multiplied by 1 and oldest by 0.1, if there are ten entries, before being averaged together), the other is an EMA with a period of one. I'm not certain which one is better, so I've commited both to github for future reference. The EMA function has shown to be better so far in testing and it is more intuitive. With EMA a new data point that is higher than average pulls the average up (or down if the new data point is lower than average).

28th Sep: Neither of the two averaging functions I added worked very well, so I spent some time making a modified EMA(exponential moving average), my function simply is weighted in favour of the newest 15 (or however many is set) entries. This is done with the following formula (((avgNewEntries - avgAllEntries)/2) + avgAllEntries). I believe to make it more like a proper EMA I would need to replace 'avgAllEntries' with the result of this formula after the first pass.
Regardless, I now have a working averaging system for the input emotions (currently just facial expressions and drowsiness) during content playing, content (in this case a sound file) is now associated with an estimated emotional state and a drowsiness state (acquired by the emotion averaging system which is active when the sound file is playing). I've condensed and refactored the code as well, making it easier to understand and use. My plan next is to work on movement detection, attention detection and finish vocal emotion detection as well as add a simple function to set values for different emotional states. It will be easy to plug in these new detection inputs into the averaging system. 

28th Sep to Oct 21st: I forgot about my work log! Silly me. I got a lot done. Movement detection is working fine and the moving average is more refined, each synapse has its own collection of vectors which each have a weighted average wrt user detected information when that synapse was active (facial expression, movement, drowsiness). Each detected type, facial, drowsy, movement, has its own object which stores the value of that input as well as its stability. To process stability I made a function inpDiff which compares the distance between current input to the expression input types and compares it with the average value of that particular input type in the recent past, and then adds that to the types object -- which is then saved to whatever synapse is running, to be added to its internal averaging system. In effect, if there is a large amount of instability in the expression input the current running synapse will save the amount of instability -- meaning this synapse being active correlates to instability in that type of expression. 

Additionally, I've gotten the audio-comparative-analysis Python script function as originally intended, luckily, things worked relatively well without too much stress. After trying to make my own distance checker with the PCA coordinates I had from the previous iteration of the script, I decided to instead use a nearest neighbour algo (I was already using sklearn for the PCA, nearest neighbour is conveniently a part of the library). The script now functions to return similar sounds to microphone input (an array of floats is what librosa expects so I used pyaudio and then convert microhpone input to an array of floats), it does this with PCA and nearest neighbour algorithms in sklearn. Other than the sklearn functions, overall structure, and buffering algorithm, the script is two scripts hacked together -- one from a medium article about feature extraction for ML and another basic one for pyaudio mic recording. 

The script works like this: 
Set up:
Directory of sound-content --> features extracted by librosa --> PCA --> Collection of coordinates with file names (kick drums nearest to each other, etc)
Usage:
Input microphone recorded and placed into buffer --> converted to floats appended to list of float arrays --> concatenated --> features extracted by Librosa --> PCA --> nearest neighbour gives 5 nearest sounds

I have some potential worries about scaling, there might be problems down the road unless PCA remains very quick on a larger amount of sound files -- its fine if the initial set up takes a long time, but the real time PCA needs to be quick. At the moment, my microphone is not working and only picks up noise so I need to get a new mic, but I was able to (happily) confirm that the script works because it detected that the nearest sound was noise.wav. It was a very happy moment.

22nd Oct: Cleaned up code and added comments in preparation for some changes. First, I want to try to move the PCA/NNeighbour algos into C++, as I could use those algos with the synapses as it is too convoluted to move data back and forth to Python from C++.

Ok, so Ive decided to use Essentia to accomplish this goal but I've run into problems with building the library and using it, declaring an issue here https://github.com/MTG/essentia/issues/921 ... sadly in the process of installing ffemeg my audio_analysis python script is broken and gives me a cryptic error to do with ALSA. There isn't many replies to a lot of the issues, which is worrying. 

Later on in the day I fixed the cryptic error with ALSA by changing the device index. Essentia's monoloader still doesn't work. I've decided against using Essentia for now given difficulty of using it and frankly being unlikely to receive help, plus I have what I want working in python. 

23rd Oct: I've added the following today: C++ reads the list of sound files saved out from python and creates a synapse for each one, placing all of them into a 'synapses' vector (the index-to-file is the same in this vector as the dataset index-to-file is in python) and it also reads the audio analysis python results which is a bunch of indexes (the indexes of the sound files nearest to the microphone input, this was accomplished last week talked about in previous entries). C++ then plays the synapses (plays their file-path and collects expression data from the user(s)) that the indexes point to. Effectively, C++ plays the sound files, synapses, known to be nearest in similarity to the incoming microphone input and records user expression data while they play. 

I've mostly finalized whats been in the works for a few weeks now, adding for loops and vectors and connecting the audio analysis python script to C++ properly. Now to test how many much it can handle, I also need to get a working microphone to properly test the user experience when it comes to audio-reflection. I can still start to work on the 'recommendation' (checking synapses expression values, eg. play what is known to cause happiness/movement out of the nearest sounds to the mic input) system for the synapses in the mean time. 

24th Oct: I've fixed up the audio analysis script so that it saves out feature data using pickle and reads it using pickle(avoiding having to process large directories of sounds more than once) and it works perfectly. Even with a far larger amount of files than before, the PCA real-time analysis is pretty quick. Obviously the intial feature extraction is much longer, but with the saving-out feature, thats totally acceptable. Openframeworks' soundplayer is responsive and can play multiple files at the same time, the interaction between the script and Openframeworks is very performative thanks to the simplicity of the information being exchanged. Right now, the system plays sounds similar to the microphone input (which is static noise at the moment due to some problems with my computers microphone) , 5 sounds at a time with each key press. I'll need to start creating the system which governs which of the nearest sounds to play based on their synapse information. 

24th Oct - 14th Nov: I waited to get a microphone before further testing (of audio_comparative_analysis.py) and have been testing the systems ability to correctly select similar sounds to microphone input for a few days. It wasn't easy to convert microphone input into something which librosa can use, I had to do a lot of digging and found this post https://blog.francoismaillet.com/epic-celebration/ which allowed me to rethink my method of microphone-to-buffer-to-librosa. I took the ring buffer in callback idea from here. Testing shows that the system detects things generally well now, whereas before it was chaotic and impossible to test, but further testing with a bigger dataset is required. The system will play drum-like sounds when listening to drummers, and will play human sounds when listening to humans, as predicted and intended the system will play sounds that are not immediately obviously related to what its listening to but shares qualities.

It was a bit of a shot in the dark, I didn't know if the methods I was using to store audio information into the buffer would come out the same as how the audio files are loaded in via librosa.load(). But because the epic-celebration blog post is doing something similar to me, I hoped that whatever the blogger is not showing about their system wouldn't be a problem and it doesn't seem to be. 

A few problems I could see occurring in the future is:

How exactly does the difference in time between the audio files effect the system? The epic-celebration system requires similarly timed inputs because all of its dataset uses the same amount of time. What am I sacrificing (or frankly completely missing) by ignoring time? I specifically chose a non-time based MFCC feature calculator to avoid time, it simply collects an average of all the MFCCs in a file at a set amount, but I do not know how this might effect the overall system.

The system might be sensitive to different microphones.

It seemed like the system factored in loudness too much, when I turned down the mic input it seemed to become more accurate, it might not be a bad idea to add information to the data so that its not just using MFCCs. Or perhaps even out the loudness of the data somehow. 

15th Nov: More testing has revealed that erroneous selections of (not)similar sounds was due to my low quality cheap microphone, the mic is extremely bass heavy and has a ringing high pitch as well as mild-to-loud noise. This was a great find! I was wondering why so often my system would choose 808s, noise, and high pitch piano sounds all the time. So to find out that it was actually working perfectly was great. In fact its selection of similar sounds diagnosed the problems with the microphone fairly accurately, neat! It is likely to became more accurate at low volumes (as noted yesterday) because the microphones signal issues would be less prominent. I could EQ out the problematic frequencies but I'm a music producer, why don't I have a good microphone yet? No more cheapening out on microphones for me.

16th Nov: Well, it wasn't 'working perfectly', pyaudio was mangling the audio -- I recorded out what audio data saved to the buffer to a .wav and it was pure glitchy noise. To fix this, I increased the buffer length in the pyaudio stream. Things now do actually work perfectly, although the microphone quality is still impressing itself onto the results far too much.
Today I've added a new feature, the script now will record 'abnormal' sounds, sounds which are above a preset distance-threshold ( the microphone inputs PCA coordinates collective distance from 5 nearest sounds are what is checked against, this is done for each ringbuffer) will have their features saved and the audio rendered to a .wav file. I've also added 5 more buffers, each capturing a different length of audio to give the system both short term and long term audio signals as inputs to extract features from and then to compare PCA coordinates with the dataset. The result is working and has interesting results, the system seems to 'ease' into the microphone and ignores the original dataset after awhile because the microphone has qualities which are not present in the original dataset, but are present in the abnormal sounds (which are added to the dataset). The system is very responsive to its environment including its sensory organ the microphone. 



16th Nov – December 30th: I got a lot done in this period and made signifiant changes to the data pipeline, it is mostly documented in the changes made on github rather than through this work log, so I am going to compare the changes I’ve made to the previous versions on github in another entry to this log when I get the time. Quick summary: I added my own method of segmenting the audio data in both the dataset and the input, the scripts now split audio into chunks of various sizes and extract features from the chunks. In the retrieval part of the system it compares the PCA (using feature vectors) as before, but now instead of whole files or buffers it now only compares  chunks of a similar size and retrieves the nearest chunks audio data.  This is very expensive computationally, especially for ‘real time’ analysis so I did a lot of optimizing (to the best of my ability, its still quite slow). I’ve found this is more accurate then before and has more interesting results e.g Kodama listens to pop music and brings up a selection of differently sized singing samples, drum samples, and samples which sound similar to the musics content. I still don’t know if it is ‘transform-independent’ for lack of a better term, if the features I’m using really capture the essence of the sound-content rather than room, distance, space.

December 30th – Jan 13th: As mentioned in the previous entry I will now explain the some of the changes I’ve made to the system as well as new ones I’ve added in this period. I have reworked how I segment audio into slices and extract features from those slices to find the nearest neighbour in the dataset. Now only one slice-size (called dim in my code short for dimension, each data entry is sorted by the size of its slices) from the input can be saved to the dataset per audio buffer cycle(it is saved only if all the nearest neighbours to it are far away enough). I have replaced the feature extraction library I was using, Librosa, with Essentia. Essentia is a C++ audio analysis library with python bindings – it is much, much faster than Librosa. In my testing I’ve observed speed increases of around 40-60%. For more information about speeds see: https://www.ntnu.edu/documents/1001201110/1266017954/DAFx-15_submission_43_v2.pdf, which is a helpful guide to the available audio feature extraction toolboxes. I still use Librosa to split the audio into chunks initially, but this aspect of the code is likely to change in the future. Additionally, I have added threading to the python scripts and have observed speed increases of around 40%, despite python being limited by the global interpreter lock (GIL). I think that this is because the feature extraction and machine learning libraries depend on a lot of the processing to be done in languages other than python (probably C++), which frees the GIL for another thread to run. Here is some pseudo-code below for better understanding of the usage of threading I’ve deployed(segment-type being the ‘type’ of segmentation, e.g whether the audio is split into 8 slices or 2 slices etc. before feature extraction on each slice is computed):

INPUT → [Threader 1, Thread 1: Audio segmentation]
(Organize dataset D by segmentation-type/begin feature extraction process on input)
	FOR N segment-types:
		[Threader 2, Thread N(segment-type)] ...
		(Processes audio features) → [N][input_data] 
When Thread 1 is DONE: // We find the nearest entries of D[N] to [N].
	[Threader 1, Thread 2][Threader 1, Thread 3][Threader 1, Thread 4] …
      (N[1] nearest to D[1])(N[2] nearest to D[2])(N[1] nearest to D[2])
	(If N[n] has got no nearby neighbours in D[n] we add N[n] to D[n])
// D[n] and N[n] refer to a list of data entries sorted by the segment-type.


January: Attempted to move to Windows to use faster audio information retrieval algorithms and use certain audio generative C++ code or even Max MSP, it did not work out and was far more troublesome than nessesary. I moved back to Linux and I’ve decided to play the output audio in Supercollider (SC) and drop the C++ and Openframeworks side of the code. Technically the python wrapped Essentia is C++ but that is under the hood. Additionally I’ve implemented OSC (originally to communicate with Max MSP) and put it onto its own thread. I collect all the nearest dataset entries and append their file locations to a list and send it via OSC to SC to be played randomly. This is just to test how well the system is working in terms of accuracy and speed. I plan on making audio output come from SC and Python be the ‘brain’. 


February: I’ve added an immortality parameter to each sample, if the sample is not selected by DBSCAN as a cluster center (a type of clustering algorithm which finds clusters of varying sizes and is quite good at ignoring noise) then on each cycle it loses some immortality and can be pruned. This allows for better control of the amount of samples going in  while still allowing Kodama to retain samples which are at cluster centres for as long as they are ‘relevant’. After discussions with my supervisor I decided to focus on testing the system and making sure that my system was accurate. To do this I needed to use matplotlib to see what the DBSCAN clusters look like. I found that there was a problem after graphing the clusters, they were far too close to each other, this was due to a bug with how I was pruning input samples (it is supposed to only accept input samples which are distant to the nearest dataset samples and it was doing the opposite). Additionally, my SC code was not good as I do not know how to make efficient SC code, it wasn’t threadsafe and was unstable. Luckily my supervisor does and was able to provide me with a better foundation for my SC code. He suggested that I separate the dim-thread samples using chars, but I couldn’t manage to get that to work due to how I set up the communication. My plan is to have SC simply read the buffer length and deduce how to play it from its size rather than by Python sorting the samples by size for it. We also discussed adding feature weighting to the audio feature extraction function. 

March: I’ve changed the way audio is recorded, instead of soundfile recording audio of a fixed length and each cycle waiting for the recording to finish before starting my system is now much more efficient e.g dim-threads targeting samples of 1 second long waiting for 3 seconds (if 3 second long dim-threads were present) before starting its thread now only wait 1 second. This is achieved by using a ringbuffer, something which I had used in the past but removed due to bugs. I fixed these bugs by making the ‘blocksize’ in sounddevice equivalent to the sample rate as well as making the code revolve around each dim-thread reading the dataset and being more independent as a result. Another key change was putting the audio stream onto its own thread allowing for the dim-threads to rely solely on the ringbuffer.
In prior iterations of the code,a ‘cycle’ refers to the function ‘parcelization’ (now called ‘consolidate), this part of the code would start the recording process and read the dataset dim-wise, this function now acts as a waste manager. Each dim-thread decides what sample is to be deleted or kept, additionally a DBSCAN prunning function is called in the consolidate function as well, it working in much the same way as dim-threads and deciding what is kept or discarded. Consolidate reads the dataset and finds samples which are marked as ‘deleted’ and permanently removes the sample. 

I have also reworked the immortality system, it now works on probabilities and the scale of the dataset. Instead of cluster centres being granted immunity, cluster centres ‘immortality’ parameter are not added to, whereas non-cluster centres are added to. As the dataset grows the immortality value is scaled in proportion with the dataset – importantly this scaling is done to every sample not just new ones e.g a sample with immortality of 0.1 after not being a cluster centre for one cycle becomes 10.0 later on in the running of the system despite always being in the centre. Because all samples mortality scales with the dataset, we can control how large the dataset should be without limiting the ability for Kodama to take in new information and lose old information which is no longer relevant. 

All the code which deals with the emotion system or Openframeworks-Python communication has been removed from my github, I had changed direction months ago clearly but this makes it more official. 

April: Added file removal queue. In prior iterations I would remove files immediately upon detection that they are to be pruned from the dataset. This results in problems with the audio generation system as it queues files for playing and some of those files ended up deleted before use. This queue allows up to 100 files to be stored, and deletes older files past the 100 mark which gives plenty of time for the audio generation system to catch up. 
By sending SC arrays containing lists containing [file name, morality, size, etc] I can control SC completely from Python without relying on SC to manage control of the performance. Everything is working as expected however there is work to be done on the performative aspect. Because of my cheap microphone it is mostly noise I hear coming back which is unpleasant, I’ll try to find a way to alleviate this. The SC plays the correct sound files without error but the effect is a delayed response due to the queueing of files, this needs to be changed.

May:This month has mostly been about writing the dissertation, research and finalizing the code. I’ve made many small(ish) changes to the code such as fixing the graphical plotting issue that I did not notice until recently( I was not clearing the subplots so the sample points which were removed still appeared on the graph). I also added reverb effects to the SC code and the output sounds a couple magnitudes better. Additionally I added a longer onset to longer sounds in SC for a more relaxed effect. The audio output is very much like I had wanted it to be early on, an ambient echoing spirit sound, but I am somewhat disappointed that I did not make it more robust. If I carry on with developing Kodama after submitting it for marking I will focus on making a compositional system which takes the audio features of the input audio buffer and uses it to make more interesting choices (Perhaps using an audio sequencer which is aware of the recent sequence of audio features from the input and decides on the placement of sounds in the sequence based on this. A granulation process to soften or stretch sounds when the input audio is detected as soft and slow. Also pitch-shifting sounds to match the input pitch, speeding up or slowing down sounds to match the bpm of the input, reducing or increasing transients sounds to match the input - all based on the audio input buffers features). While writing the dissertation I did notice silly ‘features’ of my code that did not make sense which I promptly changed (e.g. input samples which pass the distance check and are stored cause the nearest neighbors to that input sample to not be sent to OSC, my reasoning was because they are more distant than nearest neighbors to input samples which did not pass the distance check they might sound more inaccurate – however this inaccuracy is desirable for creative moments in the performance of Kodama.
