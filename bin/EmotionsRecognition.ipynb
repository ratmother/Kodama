{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmotionsRecognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "CjWvnaQUrZmD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Emotion classification using the RAVDESS dataset"
      ]
    },
    {
      "metadata": {
        "id": "ldtHMhuLrewK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) is licensed under CC BY-NA-SC 4.0. and can be downloaded free of charge at https://zenodo.org/record/1188976.\n",
        "\n",
        "***Construction and Validation***\n",
        "\n",
        "Construction and validation of the RAVDESS is described in our paper: Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391.\n",
        "\n",
        "The RAVDESS contains 7356 files. Each file was rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity, interrater reliability, and test-retest intrarater reliability were reported. Validation data is open-access, and can be downloaded along with our paper from PLOS ONE.\n",
        "\n",
        "***Description***\n",
        "\n",
        "The dataset contains the complete set of 7356 RAVDESS files (total size: 24.8 GB). Each of the 24 actors consists of three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound).  Note, there are no song files for Actor_18.\n",
        "\n",
        "***Data***\n",
        "\n",
        "For this task, I have used 4948 samples from the RAVDESS dataset.\n",
        "\n",
        "The samples comes from:\n",
        "\n",
        "- Audio-only files;\n",
        "- Video + audio files: I have extracted the audio from each file using the script Mp4ToWav.py that you can find in the main directory of the project.\n",
        "\n",
        "***License information***\n",
        "\n",
        "The RAVDESS is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, CC BY-NA-SC 4.0\n",
        "\n",
        "***File naming convention***\n",
        "\n",
        "Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics:\n",
        "\n",
        "***Filename identifiers***\n",
        "\n",
        "- Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
        "- Vocal channel (01 = speech, 02 = song).\n",
        "- Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
        "- Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the ‘neutral’ emotion.\n",
        "- Statement (01 = “Kids are talking by the door”, 02 = “Dogs are sitting by the door”).\n",
        "- Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
        "- Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
        "\n",
        "Filename example: 02-01-06-01-02-01-12.mp4 \n",
        "\n",
        "- Video-only (02)\n",
        "- Speech (01)\n",
        "- Fearful (06)\n",
        "- Normal intensity (01)\n",
        "- Statement “dogs” (02)\n",
        "- 1st Repetition (01)\n",
        "- 12th Actor (12)\n",
        "- Female, as the actor ID number is even."
      ]
    },
    {
      "metadata": {
        "id": "JDNbxj45rkvB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "\n",
        "We are using Colab, a Google Cloud environment for jupyter, so we need to import our files from Google Drive and then install LibROSA, a python package for music and audio analysis.\n",
        "\n",
        "After the import, we will plot the signal of the first file."
      ]
    },
    {
      "metadata": {
        "id": "N-o2JI49WBAe",
        "colab_type": "code",
        "outputId": "6706e4dc-2ce5-4ab8-ddc5-5cf6093b855f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EgFwaDhMbJVm",
        "colab_type": "code",
        "outputId": "fc70f467-b88d-44ec-a767-ce29b4467a30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install librosa"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.6)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.20.2)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.3.2)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.11.0)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.1)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.40.1)\n",
            "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.27.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rxI4xzngdS-e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "from librosa import display\n",
        "\n",
        "data, sampling_rate = librosa.load('/content/drive/My Drive/Ravdess/03-01-01-01-01-01-01.wav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WgaSHtCIdtX2",
        "colab_type": "code",
        "outputId": "537afffa-9883-46ee-ac49-057c24791853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "cell_type": "code",
      "source": [
        "% pylab inline\n",
        "import os\n",
        "import pandas as pd\n",
        "import glob \n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.waveplot(data, sr=sampling_rate)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['display']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PolyCollection at 0x7fae5a768860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAEGCAYAAABxSsNVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "vCtNuVWlr5jL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load all files\n",
        "\n",
        "We will create our numpy array extracting Mel-frequency cepstral coefficients (MFCCs), while the classes to predict will be extracted from the name of the file (see the introductory section of this notebook to see the naming convention of the files of this dataset)."
      ]
    },
    {
      "metadata": {
        "id": "AKvuF--gd6F-",
        "colab_type": "code",
        "outputId": "6b065348-f2b0-4b46-f6c1-24df17681745",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "path = '/content/drive/My Drive/Ravdess/'\n",
        "lst = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for subdir, dirs, files in os.walk(path):\n",
        "  for file in files:\n",
        "      try:\n",
        "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
        "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
        "        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
        "        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
        "        file = int(file[7:8]) - 1 \n",
        "        arr = mfccs, file\n",
        "        lst.append(arr)\n",
        "      # If the file is not valid, skip it\n",
        "      except ValueError:\n",
        "        continue\n",
        "\n",
        "print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- 1253.8073086738586 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kLSggnF7kKY1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
        "X, y = zip(*lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VzvBRTJIlIE9",
        "colab_type": "code",
        "outputId": "705b5c38-4b7e-4232-a861-5aa6902e47da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "\n",
        "\n",
        "X.shape, y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4948, 40), (4948,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "xOutQiAlCjOY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Saving joblib files to not load them again with the loop above\n",
        "\n",
        "import joblib\n",
        "\n",
        "X_name = 'X.joblib'\n",
        "y_name = 'y.joblib'\n",
        "save_dir = '/content/drive/My Drive/Ravdess_model'\n",
        "\n",
        "savedX = joblib.dump(X, os.path.join(save_dir, X_name))\n",
        "savedy = joblib.dump(y, os.path.join(save_dir, y_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nIoFdycUXMxA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loading saved models\n",
        "\n",
        "X = joblib.load('/content/drive/My Drive/Ravdess_model/X.joblib')\n",
        "y = joblib.load('/content/drive/My Drive/Ravdess_model/y.joblib')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Agw-3KN1sDhh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Classifier\n",
        "\n",
        "To make a first attempt in accomplishing this classification task I chose a decision tree:"
      ]
    },
    {
      "metadata": {
        "id": "Q-Xgb5NslTBO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UshLOC1ClWL3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_BnCR52nlXw0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dtree = DecisionTreeClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qWyTownblZM0",
        "colab_type": "code",
        "outputId": "7eea5ae9-998e-4fd5-8ee9-87d4c94fc8dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "dtree.fit(X_train, y_train)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
              "            max_features=None, max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
              "            splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "metadata": {
        "id": "HEuw6TUQlr7C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = dtree.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_1v0i0V7sMw7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's go with our classification report.\n",
        "\n",
        "Before we start, a quick reminder of the classes we are trying to predict:\n",
        "\n",
        "emotions = {\n",
        "    \"neutral\": \"0\",\n",
        "    \"calm\": \"1\",\n",
        "    \"happy\": \"2\",\n",
        "    \"sad\": \"3\",\n",
        "    \"angry\": \"4\", \n",
        "    \"fearful\": \"5\", \n",
        "    \"disgust\": \"6\", \n",
        "    \"surprised\": \"7\"\n",
        "}"
      ]
    },
    {
      "metadata": {
        "id": "c4kNSYkAleIv",
        "colab_type": "code",
        "outputId": "e77f2590-34c4-49d2-c889-ce760dc1916b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.83      0.78       134\n",
            "           1       0.87      0.82      0.85       251\n",
            "           2       0.79      0.70      0.74       242\n",
            "           3       0.76      0.77      0.77       271\n",
            "           4       0.85      0.84      0.85       253\n",
            "           5       0.76      0.82      0.79       239\n",
            "           6       0.69      0.71      0.70       127\n",
            "           7       0.72      0.73      0.73       116\n",
            "\n",
            "   micro avg       0.78      0.78      0.78      1633\n",
            "   macro avg       0.77      0.78      0.78      1633\n",
            "weighted avg       0.79      0.78      0.78      1633\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lCVgjLj-gwE2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ]
    },
    {
      "metadata": {
        "id": "jfaTxzZ1w__y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this second approach, I switched to a random forest classifier and I made a gridsearch to make some hyperparameters tuning.\n",
        "\n",
        "The gridsearch is not shown in the code below otherwise the notebook will require too much time to run."
      ]
    },
    {
      "metadata": {
        "id": "wcov_DCXgs7v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3eo0ljqzg-KM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rforest = RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=\"log2\", \n",
        "                                 max_leaf_nodes = 100, min_samples_leaf = 3, min_samples_split = 20, \n",
        "                                 n_estimators= 22000, random_state= 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tg45qSOfg-26",
        "colab_type": "code",
        "outputId": "cd802660-45c5-4c5d-fae2-9ca6e597a100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "rforest.fit(X_train, y_train)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "            max_depth=10, max_features='log2', max_leaf_nodes=100,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=3, min_samples_split=20,\n",
              "            min_weight_fraction_leaf=0.0, n_estimators=22000, n_jobs=None,\n",
              "            oob_score=False, random_state=5, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "metadata": {
        "id": "aM8KU3qxhGBM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = rforest.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "296FW5sBdanI",
        "colab_type": "code",
        "outputId": "8d7d7c92-2ea2-402c-ae2e-7391e6acc72a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.54      0.70       134\n",
            "           1       0.66      0.96      0.78       251\n",
            "           2       0.86      0.71      0.78       242\n",
            "           3       0.81      0.64      0.71       271\n",
            "           4       0.89      0.88      0.88       253\n",
            "           5       0.70      0.80      0.75       239\n",
            "           6       0.73      0.61      0.66       127\n",
            "           7       0.60      0.78      0.68       116\n",
            "\n",
            "   micro avg       0.76      0.76      0.76      1633\n",
            "   macro avg       0.78      0.74      0.74      1633\n",
            "weighted avg       0.79      0.76      0.76      1633\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t9eqMHV3S8i6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural network"
      ]
    },
    {
      "metadata": {
        "id": "G-QscoyMxQtn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's build our neural network!\n",
        "\n",
        "To do so, we need to expand the dimensions of our array, adding a third one using the numpy \"expand_dims\" feature."
      ]
    },
    {
      "metadata": {
        "id": "W4i187-Pe-w5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_traincnn = np.expand_dims(X_train, axis=2)\n",
        "x_testcnn = np.expand_dims(X_test, axis=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vnvoCRX1gQCh",
        "colab_type": "code",
        "outputId": "a93a30f0-5d15-4b0b-c46f-62a3bf2e690c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_traincnn.shape, x_testcnn.shape"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3315, 40, 1), (1633, 40, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "metadata": {
        "id": "HZOGIpuefCd3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(128, 5,padding='same',\n",
        "                 input_shape=(40,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(MaxPooling1D(pool_size=(8)))\n",
        "model.add(Conv1D(128, 5,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(8))\n",
        "model.add(Activation('softmax'))\n",
        "opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LphftMIZzUvz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With *model.summary* we can see a recap of what we have build:"
      ]
    },
    {
      "metadata": {
        "id": "pIWPB4Zgfic7",
        "colab_type": "code",
        "outputId": "aaf3a674-dc56-4bbe-ee67-8c150f471a74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_3 (Conv1D)            (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 5, 128)            82048     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 8)                 5128      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 8)                 0         \n",
            "=================================================================\n",
            "Total params: 87,944\n",
            "Trainable params: 87,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5qQSBeBhzcLu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can compile and fit our model:"
      ]
    },
    {
      "metadata": {
        "id": "iNI1znbsfpTx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ktdF-nJKfq6F",
        "colab_type": "code",
        "outputId": "7519d132-59e0-4b35-958f-dd34362b1414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34034
        }
      },
      "cell_type": "code",
      "source": [
        "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3315 samples, validate on 1633 samples\n",
            "Epoch 1/1000\n",
            "3315/3315 [==============================] - 2s 642us/step - loss: 6.2861 - acc: 0.1508 - val_loss: 2.7693 - val_acc: 0.2107\n",
            "Epoch 2/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 5.1899 - acc: 0.1517 - val_loss: 2.4764 - val_acc: 0.2039\n",
            "Epoch 3/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 4.0180 - acc: 0.1768 - val_loss: 2.1228 - val_acc: 0.2278\n",
            "Epoch 4/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 3.1776 - acc: 0.1928 - val_loss: 1.9904 - val_acc: 0.1935\n",
            "Epoch 5/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 2.5915 - acc: 0.2157 - val_loss: 1.8975 - val_acc: 0.2603\n",
            "Epoch 6/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 2.2226 - acc: 0.2383 - val_loss: 1.8669 - val_acc: 0.2566\n",
            "Epoch 7/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 2.0262 - acc: 0.2486 - val_loss: 1.7460 - val_acc: 0.3301\n",
            "Epoch 8/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 1.9201 - acc: 0.2715 - val_loss: 1.7929 - val_acc: 0.3148\n",
            "Epoch 9/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 1.8562 - acc: 0.2899 - val_loss: 1.7372 - val_acc: 0.3778\n",
            "Epoch 10/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 1.7860 - acc: 0.3059 - val_loss: 1.7204 - val_acc: 0.3001\n",
            "Epoch 11/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 1.7668 - acc: 0.3300 - val_loss: 1.6682 - val_acc: 0.4005\n",
            "Epoch 12/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 1.7260 - acc: 0.3472 - val_loss: 1.6414 - val_acc: 0.4023\n",
            "Epoch 13/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 1.7063 - acc: 0.3430 - val_loss: 1.6324 - val_acc: 0.4121\n",
            "Epoch 14/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 1.6951 - acc: 0.3508 - val_loss: 1.5922 - val_acc: 0.4427\n",
            "Epoch 15/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 1.6698 - acc: 0.3611 - val_loss: 1.5908 - val_acc: 0.4207\n",
            "Epoch 16/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 1.6452 - acc: 0.3816 - val_loss: 1.5754 - val_acc: 0.4311\n",
            "Epoch 17/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 1.5977 - acc: 0.4075 - val_loss: 1.5343 - val_acc: 0.4691\n",
            "Epoch 18/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 1.5899 - acc: 0.4030 - val_loss: 1.5315 - val_acc: 0.4672\n",
            "Epoch 19/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 1.5693 - acc: 0.4090 - val_loss: 1.5101 - val_acc: 0.4868\n",
            "Epoch 20/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 1.5523 - acc: 0.4115 - val_loss: 1.5064 - val_acc: 0.4452\n",
            "Epoch 21/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 1.5304 - acc: 0.4262 - val_loss: 1.4655 - val_acc: 0.4917\n",
            "Epoch 22/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 1.5235 - acc: 0.4335 - val_loss: 1.4705 - val_acc: 0.4770\n",
            "Epoch 23/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 1.4948 - acc: 0.4407 - val_loss: 1.4749 - val_acc: 0.4574\n",
            "Epoch 24/1000\n",
            "3315/3315 [==============================] - 2s 485us/step - loss: 1.4862 - acc: 0.4504 - val_loss: 1.4596 - val_acc: 0.4832\n",
            "Epoch 25/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 1.4632 - acc: 0.4489 - val_loss: 1.4291 - val_acc: 0.4954\n",
            "Epoch 26/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 1.4607 - acc: 0.4416 - val_loss: 1.4098 - val_acc: 0.5089\n",
            "Epoch 27/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 1.4519 - acc: 0.4543 - val_loss: 1.4124 - val_acc: 0.5028\n",
            "Epoch 28/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 1.4402 - acc: 0.4688 - val_loss: 1.4431 - val_acc: 0.4568\n",
            "Epoch 29/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 1.4327 - acc: 0.4679 - val_loss: 1.3959 - val_acc: 0.5003\n",
            "Epoch 30/1000\n",
            "3315/3315 [==============================] - 2s 508us/step - loss: 1.4100 - acc: 0.4793 - val_loss: 1.3746 - val_acc: 0.5230\n",
            "Epoch 31/1000\n",
            "3315/3315 [==============================] - 2s 510us/step - loss: 1.4093 - acc: 0.4712 - val_loss: 1.3551 - val_acc: 0.5193\n",
            "Epoch 32/1000\n",
            "3315/3315 [==============================] - 2s 485us/step - loss: 1.3878 - acc: 0.4917 - val_loss: 1.3410 - val_acc: 0.5291\n",
            "Epoch 33/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 1.3773 - acc: 0.5011 - val_loss: 1.3482 - val_acc: 0.5309\n",
            "Epoch 34/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 1.3657 - acc: 0.4950 - val_loss: 1.3290 - val_acc: 0.5456\n",
            "Epoch 35/1000\n",
            "3315/3315 [==============================] - 2s 508us/step - loss: 1.3722 - acc: 0.5035 - val_loss: 1.3277 - val_acc: 0.5168\n",
            "Epoch 36/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 1.3600 - acc: 0.4965 - val_loss: 1.3240 - val_acc: 0.5095\n",
            "Epoch 37/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 1.3511 - acc: 0.5032 - val_loss: 1.3159 - val_acc: 0.5321\n",
            "Epoch 38/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 1.3234 - acc: 0.5062 - val_loss: 1.2910 - val_acc: 0.5419\n",
            "Epoch 39/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 1.3202 - acc: 0.5125 - val_loss: 1.2772 - val_acc: 0.5481\n",
            "Epoch 40/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 1.3108 - acc: 0.5155 - val_loss: 1.2903 - val_acc: 0.5468\n",
            "Epoch 41/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 1.3073 - acc: 0.5210 - val_loss: 1.2677 - val_acc: 0.5542\n",
            "Epoch 42/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 1.3073 - acc: 0.5173 - val_loss: 1.2614 - val_acc: 0.5542\n",
            "Epoch 43/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 1.2786 - acc: 0.5276 - val_loss: 1.2519 - val_acc: 0.5499\n",
            "Epoch 44/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 1.2770 - acc: 0.5321 - val_loss: 1.2607 - val_acc: 0.5456\n",
            "Epoch 45/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 1.2725 - acc: 0.5294 - val_loss: 1.2543 - val_acc: 0.5585\n",
            "Epoch 46/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 1.2671 - acc: 0.5391 - val_loss: 1.2411 - val_acc: 0.5634\n",
            "Epoch 47/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 1.2527 - acc: 0.5454 - val_loss: 1.2308 - val_acc: 0.5530\n",
            "Epoch 48/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 1.2595 - acc: 0.5351 - val_loss: 1.2096 - val_acc: 0.5928\n",
            "Epoch 49/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 1.2308 - acc: 0.5490 - val_loss: 1.2453 - val_acc: 0.5628\n",
            "Epoch 50/1000\n",
            "3315/3315 [==============================] - 2s 485us/step - loss: 1.2276 - acc: 0.5505 - val_loss: 1.2110 - val_acc: 0.5732\n",
            "Epoch 51/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 1.2154 - acc: 0.5478 - val_loss: 1.2097 - val_acc: 0.5701\n",
            "Epoch 52/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 1.2143 - acc: 0.5575 - val_loss: 1.1858 - val_acc: 0.5952\n",
            "Epoch 53/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 1.1962 - acc: 0.5581 - val_loss: 1.2067 - val_acc: 0.5652\n",
            "Epoch 54/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 1.1932 - acc: 0.5605 - val_loss: 1.1683 - val_acc: 0.5854\n",
            "Epoch 55/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 1.1873 - acc: 0.5689 - val_loss: 1.1670 - val_acc: 0.5915\n",
            "Epoch 56/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 1.1757 - acc: 0.5650 - val_loss: 1.1543 - val_acc: 0.5952\n",
            "Epoch 57/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 1.1700 - acc: 0.5710 - val_loss: 1.1538 - val_acc: 0.5915\n",
            "Epoch 58/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 1.1574 - acc: 0.5771 - val_loss: 1.1475 - val_acc: 0.5958\n",
            "Epoch 59/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 1.1704 - acc: 0.5729 - val_loss: 1.1377 - val_acc: 0.6044\n",
            "Epoch 60/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 1.1658 - acc: 0.5756 - val_loss: 1.1621 - val_acc: 0.5989\n",
            "Epoch 61/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 1.1546 - acc: 0.5741 - val_loss: 1.1271 - val_acc: 0.5989\n",
            "Epoch 62/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 1.1420 - acc: 0.5846 - val_loss: 1.1353 - val_acc: 0.5909\n",
            "Epoch 63/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 1.1459 - acc: 0.5798 - val_loss: 1.1345 - val_acc: 0.5928\n",
            "Epoch 64/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 1.1353 - acc: 0.5934 - val_loss: 1.1235 - val_acc: 0.6081\n",
            "Epoch 65/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 1.1209 - acc: 0.5876 - val_loss: 1.1221 - val_acc: 0.5897\n",
            "Epoch 66/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 1.1126 - acc: 0.5831 - val_loss: 1.1076 - val_acc: 0.6075\n",
            "Epoch 67/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 1.1107 - acc: 0.5922 - val_loss: 1.0963 - val_acc: 0.6173\n",
            "Epoch 68/1000\n",
            "3315/3315 [==============================] - 2s 509us/step - loss: 1.1084 - acc: 0.5994 - val_loss: 1.1084 - val_acc: 0.6020\n",
            "Epoch 69/1000\n",
            "3315/3315 [==============================] - 2s 507us/step - loss: 1.1134 - acc: 0.5922 - val_loss: 1.0914 - val_acc: 0.6142\n",
            "Epoch 70/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 1.1026 - acc: 0.5970 - val_loss: 1.1052 - val_acc: 0.6081\n",
            "Epoch 71/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 1.0922 - acc: 0.6045 - val_loss: 1.0879 - val_acc: 0.6099\n",
            "Epoch 72/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 1.0841 - acc: 0.6018 - val_loss: 1.0749 - val_acc: 0.6295\n",
            "Epoch 73/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 1.0744 - acc: 0.6066 - val_loss: 1.0878 - val_acc: 0.6222\n",
            "Epoch 74/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 1.0839 - acc: 0.6045 - val_loss: 1.0591 - val_acc: 0.6289\n",
            "Epoch 75/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 1.0775 - acc: 0.6106 - val_loss: 1.0620 - val_acc: 0.6173\n",
            "Epoch 76/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 1.0696 - acc: 0.6048 - val_loss: 1.0670 - val_acc: 0.6203\n",
            "Epoch 77/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 1.0595 - acc: 0.6211 - val_loss: 1.0522 - val_acc: 0.6265\n",
            "Epoch 78/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 1.0440 - acc: 0.6232 - val_loss: 1.0811 - val_acc: 0.6087\n",
            "Epoch 79/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 1.0455 - acc: 0.6163 - val_loss: 1.0296 - val_acc: 0.6454\n",
            "Epoch 80/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 1.0435 - acc: 0.6262 - val_loss: 1.0460 - val_acc: 0.6197\n",
            "Epoch 81/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 1.0395 - acc: 0.6281 - val_loss: 1.0352 - val_acc: 0.6277\n",
            "Epoch 82/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 1.0433 - acc: 0.6115 - val_loss: 1.0358 - val_acc: 0.6295\n",
            "Epoch 83/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 1.0325 - acc: 0.6253 - val_loss: 1.0429 - val_acc: 0.6295\n",
            "Epoch 84/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 1.0247 - acc: 0.6284 - val_loss: 1.0348 - val_acc: 0.6320\n",
            "Epoch 85/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 1.0146 - acc: 0.6338 - val_loss: 1.0238 - val_acc: 0.6497\n",
            "Epoch 86/1000\n",
            "3315/3315 [==============================] - 2s 485us/step - loss: 1.0272 - acc: 0.6205 - val_loss: 1.0083 - val_acc: 0.6369\n",
            "Epoch 87/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 1.0193 - acc: 0.6320 - val_loss: 1.0346 - val_acc: 0.6283\n",
            "Epoch 88/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 1.0141 - acc: 0.6268 - val_loss: 1.0196 - val_acc: 0.6399\n",
            "Epoch 89/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.9977 - acc: 0.6425 - val_loss: 1.0067 - val_acc: 0.6509\n",
            "Epoch 90/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.9913 - acc: 0.6341 - val_loss: 0.9940 - val_acc: 0.6546\n",
            "Epoch 91/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 1.0007 - acc: 0.6398 - val_loss: 0.9968 - val_acc: 0.6454\n",
            "Epoch 92/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.9766 - acc: 0.6498 - val_loss: 0.9841 - val_acc: 0.6601\n",
            "Epoch 93/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.9931 - acc: 0.6401 - val_loss: 0.9830 - val_acc: 0.6589\n",
            "Epoch 94/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.9777 - acc: 0.6446 - val_loss: 0.9871 - val_acc: 0.6418\n",
            "Epoch 95/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.9762 - acc: 0.6425 - val_loss: 0.9927 - val_acc: 0.6326\n",
            "Epoch 96/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.9619 - acc: 0.6489 - val_loss: 0.9732 - val_acc: 0.6430\n",
            "Epoch 97/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.9685 - acc: 0.6480 - val_loss: 0.9830 - val_acc: 0.6454\n",
            "Epoch 98/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.9661 - acc: 0.6437 - val_loss: 0.9827 - val_acc: 0.6516\n",
            "Epoch 99/1000\n",
            "3315/3315 [==============================] - 2s 518us/step - loss: 0.9676 - acc: 0.6486 - val_loss: 0.9776 - val_acc: 0.6589\n",
            "Epoch 100/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.9716 - acc: 0.6419 - val_loss: 0.9665 - val_acc: 0.6565\n",
            "Epoch 101/1000\n",
            "3315/3315 [==============================] - 2s 510us/step - loss: 0.9516 - acc: 0.6528 - val_loss: 0.9623 - val_acc: 0.6589\n",
            "Epoch 102/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 0.9538 - acc: 0.6597 - val_loss: 0.9557 - val_acc: 0.6687\n",
            "Epoch 103/1000\n",
            "3315/3315 [==============================] - 2s 509us/step - loss: 0.9452 - acc: 0.6600 - val_loss: 0.9616 - val_acc: 0.6638\n",
            "Epoch 104/1000\n",
            "3315/3315 [==============================] - 2s 512us/step - loss: 0.9404 - acc: 0.6591 - val_loss: 0.9631 - val_acc: 0.6436\n",
            "Epoch 105/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.9353 - acc: 0.6549 - val_loss: 0.9636 - val_acc: 0.6497\n",
            "Epoch 106/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.9393 - acc: 0.6588 - val_loss: 0.9561 - val_acc: 0.6485\n",
            "Epoch 107/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.9271 - acc: 0.6661 - val_loss: 0.9533 - val_acc: 0.6442\n",
            "Epoch 108/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.9237 - acc: 0.6694 - val_loss: 0.9472 - val_acc: 0.6712\n",
            "Epoch 109/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.9300 - acc: 0.6603 - val_loss: 0.9427 - val_acc: 0.6540\n",
            "Epoch 110/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.9212 - acc: 0.6661 - val_loss: 0.9658 - val_acc: 0.6350\n",
            "Epoch 111/1000\n",
            "3315/3315 [==============================] - 2s 485us/step - loss: 0.9192 - acc: 0.6570 - val_loss: 0.9339 - val_acc: 0.6614\n",
            "Epoch 112/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.9169 - acc: 0.6799 - val_loss: 0.9271 - val_acc: 0.6589\n",
            "Epoch 113/1000\n",
            "3315/3315 [==============================] - 2s 528us/step - loss: 0.8997 - acc: 0.6715 - val_loss: 0.9233 - val_acc: 0.6681\n",
            "Epoch 114/1000\n",
            "3315/3315 [==============================] - 2s 543us/step - loss: 0.9017 - acc: 0.6709 - val_loss: 0.9109 - val_acc: 0.6761\n",
            "Epoch 115/1000\n",
            "3315/3315 [==============================] - 2s 547us/step - loss: 0.8977 - acc: 0.6739 - val_loss: 0.9602 - val_acc: 0.6418\n",
            "Epoch 116/1000\n",
            "3315/3315 [==============================] - 2s 543us/step - loss: 0.9037 - acc: 0.6775 - val_loss: 0.9274 - val_acc: 0.6693\n",
            "Epoch 117/1000\n",
            "3315/3315 [==============================] - 2s 543us/step - loss: 0.8973 - acc: 0.6742 - val_loss: 0.9105 - val_acc: 0.6687\n",
            "Epoch 118/1000\n",
            "3315/3315 [==============================] - 2s 544us/step - loss: 0.8739 - acc: 0.6839 - val_loss: 0.9454 - val_acc: 0.6540\n",
            "Epoch 119/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.8850 - acc: 0.6869 - val_loss: 0.9393 - val_acc: 0.6528\n",
            "Epoch 120/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.8756 - acc: 0.6805 - val_loss: 0.9138 - val_acc: 0.6699\n",
            "Epoch 121/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.8838 - acc: 0.6836 - val_loss: 0.9098 - val_acc: 0.6583\n",
            "Epoch 122/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.8656 - acc: 0.6887 - val_loss: 0.8977 - val_acc: 0.6797\n",
            "Epoch 123/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.8613 - acc: 0.7026 - val_loss: 0.9354 - val_acc: 0.6644\n",
            "Epoch 124/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.8700 - acc: 0.6802 - val_loss: 0.8987 - val_acc: 0.6669\n",
            "Epoch 125/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 0.8497 - acc: 0.6959 - val_loss: 0.8858 - val_acc: 0.6846\n",
            "Epoch 126/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.8615 - acc: 0.6932 - val_loss: 0.9022 - val_acc: 0.6663\n",
            "Epoch 127/1000\n",
            "3315/3315 [==============================] - 2s 485us/step - loss: 0.8569 - acc: 0.6941 - val_loss: 0.8732 - val_acc: 0.6926\n",
            "Epoch 128/1000\n",
            "3315/3315 [==============================] - 2s 485us/step - loss: 0.8472 - acc: 0.6959 - val_loss: 0.8881 - val_acc: 0.6742\n",
            "Epoch 129/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.8478 - acc: 0.6923 - val_loss: 0.8852 - val_acc: 0.6754\n",
            "Epoch 130/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.8465 - acc: 0.6917 - val_loss: 0.8906 - val_acc: 0.6650\n",
            "Epoch 131/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.8428 - acc: 0.6956 - val_loss: 0.8763 - val_acc: 0.6803\n",
            "Epoch 132/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.8404 - acc: 0.6953 - val_loss: 0.8888 - val_acc: 0.6828\n",
            "Epoch 133/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.8311 - acc: 0.7011 - val_loss: 0.8773 - val_acc: 0.6840\n",
            "Epoch 134/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.8358 - acc: 0.7065 - val_loss: 0.8896 - val_acc: 0.6767\n",
            "Epoch 135/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.8178 - acc: 0.7032 - val_loss: 0.8638 - val_acc: 0.6877\n",
            "Epoch 136/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.8234 - acc: 0.7020 - val_loss: 0.8609 - val_acc: 0.6895\n",
            "Epoch 137/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.8164 - acc: 0.7098 - val_loss: 0.9042 - val_acc: 0.6577\n",
            "Epoch 138/1000\n",
            "3315/3315 [==============================] - 2s 511us/step - loss: 0.8187 - acc: 0.7002 - val_loss: 0.8652 - val_acc: 0.6889\n",
            "Epoch 139/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.8231 - acc: 0.7029 - val_loss: 0.8515 - val_acc: 0.6895\n",
            "Epoch 140/1000\n",
            "3315/3315 [==============================] - 2s 509us/step - loss: 0.8160 - acc: 0.7101 - val_loss: 0.8487 - val_acc: 0.7018\n",
            "Epoch 141/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.8010 - acc: 0.7140 - val_loss: 0.8641 - val_acc: 0.6822\n",
            "Epoch 142/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.8054 - acc: 0.7176 - val_loss: 0.8682 - val_acc: 0.6920\n",
            "Epoch 143/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.7938 - acc: 0.7164 - val_loss: 0.8409 - val_acc: 0.7036\n",
            "Epoch 144/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.7980 - acc: 0.7086 - val_loss: 0.8730 - val_acc: 0.6718\n",
            "Epoch 145/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.7862 - acc: 0.7095 - val_loss: 0.8625 - val_acc: 0.6852\n",
            "Epoch 146/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.7954 - acc: 0.7137 - val_loss: 0.8873 - val_acc: 0.6681\n",
            "Epoch 147/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.7950 - acc: 0.7059 - val_loss: 0.8294 - val_acc: 0.6999\n",
            "Epoch 148/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.7930 - acc: 0.7149 - val_loss: 0.8144 - val_acc: 0.7177\n",
            "Epoch 149/1000\n",
            "3315/3315 [==============================] - 2s 546us/step - loss: 0.7731 - acc: 0.7264 - val_loss: 0.8288 - val_acc: 0.6950\n",
            "Epoch 150/1000\n",
            "3315/3315 [==============================] - 2s 552us/step - loss: 0.7850 - acc: 0.7125 - val_loss: 0.8387 - val_acc: 0.6963\n",
            "Epoch 151/1000\n",
            "3315/3315 [==============================] - 2s 547us/step - loss: 0.7755 - acc: 0.7240 - val_loss: 0.8505 - val_acc: 0.6963\n",
            "Epoch 152/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.7629 - acc: 0.7240 - val_loss: 0.8417 - val_acc: 0.6938\n",
            "Epoch 153/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.7669 - acc: 0.7261 - val_loss: 0.8240 - val_acc: 0.6975\n",
            "Epoch 154/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.7712 - acc: 0.7225 - val_loss: 0.8089 - val_acc: 0.7171\n",
            "Epoch 155/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.7673 - acc: 0.7234 - val_loss: 0.8416 - val_acc: 0.6834\n",
            "Epoch 156/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 0.7644 - acc: 0.7219 - val_loss: 0.8250 - val_acc: 0.7036\n",
            "Epoch 157/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.7608 - acc: 0.7222 - val_loss: 0.8046 - val_acc: 0.7140\n",
            "Epoch 158/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.7548 - acc: 0.7312 - val_loss: 0.8009 - val_acc: 0.7097\n",
            "Epoch 159/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.7629 - acc: 0.7300 - val_loss: 0.8115 - val_acc: 0.7067\n",
            "Epoch 160/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.7556 - acc: 0.7279 - val_loss: 0.8257 - val_acc: 0.6987\n",
            "Epoch 161/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.7612 - acc: 0.7246 - val_loss: 0.7835 - val_acc: 0.7238\n",
            "Epoch 162/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.7490 - acc: 0.7312 - val_loss: 0.8031 - val_acc: 0.7073\n",
            "Epoch 163/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.7451 - acc: 0.7330 - val_loss: 0.7971 - val_acc: 0.7165\n",
            "Epoch 164/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.7426 - acc: 0.7246 - val_loss: 0.8055 - val_acc: 0.7030\n",
            "Epoch 165/1000\n",
            "3315/3315 [==============================] - 2s 485us/step - loss: 0.7380 - acc: 0.7348 - val_loss: 0.8096 - val_acc: 0.7036\n",
            "Epoch 166/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.7432 - acc: 0.7300 - val_loss: 0.7901 - val_acc: 0.7208\n",
            "Epoch 167/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.7412 - acc: 0.7297 - val_loss: 0.7719 - val_acc: 0.7318\n",
            "Epoch 168/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.7365 - acc: 0.7363 - val_loss: 0.8102 - val_acc: 0.7122\n",
            "Epoch 169/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.7330 - acc: 0.7357 - val_loss: 0.7848 - val_acc: 0.7165\n",
            "Epoch 170/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.7184 - acc: 0.7478 - val_loss: 0.7809 - val_acc: 0.7122\n",
            "Epoch 171/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 0.7276 - acc: 0.7382 - val_loss: 0.7927 - val_acc: 0.7128\n",
            "Epoch 172/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.7281 - acc: 0.7460 - val_loss: 0.7639 - val_acc: 0.7318\n",
            "Epoch 173/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.7141 - acc: 0.7451 - val_loss: 0.7745 - val_acc: 0.7116\n",
            "Epoch 174/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.7145 - acc: 0.7403 - val_loss: 0.7773 - val_acc: 0.7263\n",
            "Epoch 175/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 0.7099 - acc: 0.7499 - val_loss: 0.7772 - val_acc: 0.7220\n",
            "Epoch 176/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.7105 - acc: 0.7433 - val_loss: 0.7935 - val_acc: 0.7165\n",
            "Epoch 177/1000\n",
            "3315/3315 [==============================] - 2s 512us/step - loss: 0.7007 - acc: 0.7544 - val_loss: 0.7636 - val_acc: 0.7263\n",
            "Epoch 178/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.7066 - acc: 0.7397 - val_loss: 0.7609 - val_acc: 0.7373\n",
            "Epoch 179/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.7183 - acc: 0.7397 - val_loss: 0.8017 - val_acc: 0.7030\n",
            "Epoch 180/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.6935 - acc: 0.7457 - val_loss: 0.7499 - val_acc: 0.7257\n",
            "Epoch 181/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.7025 - acc: 0.7442 - val_loss: 0.7618 - val_acc: 0.7336\n",
            "Epoch 182/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.6996 - acc: 0.7409 - val_loss: 0.7473 - val_acc: 0.7428\n",
            "Epoch 183/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.7029 - acc: 0.7511 - val_loss: 0.7498 - val_acc: 0.7299\n",
            "Epoch 184/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.6780 - acc: 0.7611 - val_loss: 0.7444 - val_acc: 0.7342\n",
            "Epoch 185/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.6976 - acc: 0.7445 - val_loss: 0.7675 - val_acc: 0.7355\n",
            "Epoch 186/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.6889 - acc: 0.7538 - val_loss: 0.7377 - val_acc: 0.7404\n",
            "Epoch 187/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.6950 - acc: 0.7475 - val_loss: 0.7580 - val_acc: 0.7244\n",
            "Epoch 188/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.6880 - acc: 0.7548 - val_loss: 0.7551 - val_acc: 0.7214\n",
            "Epoch 189/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.6840 - acc: 0.7541 - val_loss: 0.7470 - val_acc: 0.7373\n",
            "Epoch 190/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.6832 - acc: 0.7514 - val_loss: 0.7344 - val_acc: 0.7465\n",
            "Epoch 191/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.6780 - acc: 0.7599 - val_loss: 0.7225 - val_acc: 0.7428\n",
            "Epoch 192/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.6719 - acc: 0.7566 - val_loss: 0.7432 - val_acc: 0.7232\n",
            "Epoch 193/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.6791 - acc: 0.7532 - val_loss: 0.7386 - val_acc: 0.7330\n",
            "Epoch 194/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.6789 - acc: 0.7560 - val_loss: 0.7319 - val_acc: 0.7489\n",
            "Epoch 195/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.6732 - acc: 0.7602 - val_loss: 0.7399 - val_acc: 0.7281\n",
            "Epoch 196/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.6626 - acc: 0.7602 - val_loss: 0.7340 - val_acc: 0.7355\n",
            "Epoch 197/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.6653 - acc: 0.7638 - val_loss: 0.7314 - val_acc: 0.7434\n",
            "Epoch 198/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.6565 - acc: 0.7596 - val_loss: 0.7528 - val_acc: 0.7306\n",
            "Epoch 199/1000\n",
            "3315/3315 [==============================] - 2s 513us/step - loss: 0.6578 - acc: 0.7538 - val_loss: 0.7296 - val_acc: 0.7355\n",
            "Epoch 200/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.6553 - acc: 0.7653 - val_loss: 0.7260 - val_acc: 0.7404\n",
            "Epoch 201/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.6546 - acc: 0.7629 - val_loss: 0.7657 - val_acc: 0.7287\n",
            "Epoch 202/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.6569 - acc: 0.7677 - val_loss: 0.7141 - val_acc: 0.7538\n",
            "Epoch 203/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.6491 - acc: 0.7683 - val_loss: 0.7094 - val_acc: 0.7502\n",
            "Epoch 204/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.6435 - acc: 0.7668 - val_loss: 0.7259 - val_acc: 0.7355\n",
            "Epoch 205/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.6328 - acc: 0.7735 - val_loss: 0.7364 - val_acc: 0.7336\n",
            "Epoch 206/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.6352 - acc: 0.7680 - val_loss: 0.7543 - val_acc: 0.7250\n",
            "Epoch 207/1000\n",
            "3315/3315 [==============================] - 2s 509us/step - loss: 0.6346 - acc: 0.7753 - val_loss: 0.7152 - val_acc: 0.7397\n",
            "Epoch 208/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.6219 - acc: 0.7801 - val_loss: 0.7072 - val_acc: 0.7477\n",
            "Epoch 209/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.6366 - acc: 0.7695 - val_loss: 0.7174 - val_acc: 0.7391\n",
            "Epoch 210/1000\n",
            "3315/3315 [==============================] - 2s 511us/step - loss: 0.6361 - acc: 0.7789 - val_loss: 0.6889 - val_acc: 0.7685\n",
            "Epoch 211/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.6332 - acc: 0.7719 - val_loss: 0.7164 - val_acc: 0.7459\n",
            "Epoch 212/1000\n",
            "3315/3315 [==============================] - 2s 514us/step - loss: 0.6242 - acc: 0.7816 - val_loss: 0.6970 - val_acc: 0.7551\n",
            "Epoch 213/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.6236 - acc: 0.7765 - val_loss: 0.6966 - val_acc: 0.7514\n",
            "Epoch 214/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.6289 - acc: 0.7704 - val_loss: 0.6954 - val_acc: 0.7612\n",
            "Epoch 215/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.6292 - acc: 0.7738 - val_loss: 0.6999 - val_acc: 0.7489\n",
            "Epoch 216/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.6115 - acc: 0.7834 - val_loss: 0.7299 - val_acc: 0.7232\n",
            "Epoch 217/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.6092 - acc: 0.7789 - val_loss: 0.6926 - val_acc: 0.7532\n",
            "Epoch 218/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.6137 - acc: 0.7819 - val_loss: 0.6988 - val_acc: 0.7532\n",
            "Epoch 219/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.6081 - acc: 0.7828 - val_loss: 0.7259 - val_acc: 0.7373\n",
            "Epoch 220/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.6065 - acc: 0.7813 - val_loss: 0.6991 - val_acc: 0.7569\n",
            "Epoch 221/1000\n",
            "3315/3315 [==============================] - 2s 485us/step - loss: 0.6186 - acc: 0.7771 - val_loss: 0.6710 - val_acc: 0.7685\n",
            "Epoch 222/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.6053 - acc: 0.7843 - val_loss: 0.7276 - val_acc: 0.7299\n",
            "Epoch 223/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.6157 - acc: 0.7783 - val_loss: 0.6829 - val_acc: 0.7612\n",
            "Epoch 224/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.6029 - acc: 0.7864 - val_loss: 0.6784 - val_acc: 0.7587\n",
            "Epoch 225/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.5953 - acc: 0.7894 - val_loss: 0.7063 - val_acc: 0.7526\n",
            "Epoch 226/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.6021 - acc: 0.7849 - val_loss: 0.7132 - val_acc: 0.7330\n",
            "Epoch 227/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.5948 - acc: 0.7900 - val_loss: 0.6824 - val_acc: 0.7600\n",
            "Epoch 228/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.5950 - acc: 0.7894 - val_loss: 0.6897 - val_acc: 0.7538\n",
            "Epoch 229/1000\n",
            "3315/3315 [==============================] - 2s 485us/step - loss: 0.5876 - acc: 0.7885 - val_loss: 0.6726 - val_acc: 0.7612\n",
            "Epoch 230/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.5965 - acc: 0.7861 - val_loss: 0.6895 - val_acc: 0.7600\n",
            "Epoch 231/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.5850 - acc: 0.7903 - val_loss: 0.6966 - val_acc: 0.7563\n",
            "Epoch 232/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.5927 - acc: 0.7897 - val_loss: 0.6643 - val_acc: 0.7753\n",
            "Epoch 233/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.5853 - acc: 0.7885 - val_loss: 0.6635 - val_acc: 0.7734\n",
            "Epoch 234/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.5873 - acc: 0.7885 - val_loss: 0.6853 - val_acc: 0.7532\n",
            "Epoch 235/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.5744 - acc: 0.7934 - val_loss: 0.6803 - val_acc: 0.7624\n",
            "Epoch 236/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.5703 - acc: 0.7946 - val_loss: 0.7222 - val_acc: 0.7404\n",
            "Epoch 237/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.5867 - acc: 0.7897 - val_loss: 0.6774 - val_acc: 0.7661\n",
            "Epoch 238/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.5669 - acc: 0.7952 - val_loss: 0.6648 - val_acc: 0.7691\n",
            "Epoch 239/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.5833 - acc: 0.7919 - val_loss: 0.6559 - val_acc: 0.7746\n",
            "Epoch 240/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.5694 - acc: 0.7910 - val_loss: 0.6465 - val_acc: 0.7765\n",
            "Epoch 241/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.5757 - acc: 0.7922 - val_loss: 0.6633 - val_acc: 0.7685\n",
            "Epoch 242/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.5670 - acc: 0.8045 - val_loss: 0.6933 - val_acc: 0.7477\n",
            "Epoch 243/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.5617 - acc: 0.8048 - val_loss: 0.6827 - val_acc: 0.7465\n",
            "Epoch 244/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.5620 - acc: 0.8051 - val_loss: 0.6528 - val_acc: 0.7759\n",
            "Epoch 245/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.5683 - acc: 0.7991 - val_loss: 0.6570 - val_acc: 0.7789\n",
            "Epoch 246/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.5708 - acc: 0.7943 - val_loss: 0.6611 - val_acc: 0.7753\n",
            "Epoch 247/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 0.5666 - acc: 0.7970 - val_loss: 0.6500 - val_acc: 0.7753\n",
            "Epoch 248/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.5709 - acc: 0.8003 - val_loss: 0.6475 - val_acc: 0.7777\n",
            "Epoch 249/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 0.5640 - acc: 0.8009 - val_loss: 0.6337 - val_acc: 0.7863\n",
            "Epoch 250/1000\n",
            "3315/3315 [==============================] - 2s 513us/step - loss: 0.5597 - acc: 0.7973 - val_loss: 0.6634 - val_acc: 0.7606\n",
            "Epoch 251/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.5503 - acc: 0.7979 - val_loss: 0.6920 - val_acc: 0.7514\n",
            "Epoch 252/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.5620 - acc: 0.7967 - val_loss: 0.6461 - val_acc: 0.7783\n",
            "Epoch 253/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.5523 - acc: 0.7979 - val_loss: 0.6322 - val_acc: 0.7820\n",
            "Epoch 254/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.5404 - acc: 0.8078 - val_loss: 0.6332 - val_acc: 0.7808\n",
            "Epoch 255/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.5444 - acc: 0.8006 - val_loss: 0.6502 - val_acc: 0.7783\n",
            "Epoch 256/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.5483 - acc: 0.8042 - val_loss: 0.6360 - val_acc: 0.7783\n",
            "Epoch 257/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.5405 - acc: 0.8063 - val_loss: 0.6318 - val_acc: 0.7746\n",
            "Epoch 258/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.5332 - acc: 0.8097 - val_loss: 0.6387 - val_acc: 0.7844\n",
            "Epoch 259/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.5342 - acc: 0.8097 - val_loss: 0.6266 - val_acc: 0.7802\n",
            "Epoch 260/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.5494 - acc: 0.8009 - val_loss: 0.6527 - val_acc: 0.7685\n",
            "Epoch 261/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.5396 - acc: 0.8054 - val_loss: 0.6629 - val_acc: 0.7673\n",
            "Epoch 262/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.5340 - acc: 0.8066 - val_loss: 0.6320 - val_acc: 0.7746\n",
            "Epoch 263/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.5391 - acc: 0.8075 - val_loss: 0.6320 - val_acc: 0.7783\n",
            "Epoch 264/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 0.5315 - acc: 0.8106 - val_loss: 0.6063 - val_acc: 0.7875\n",
            "Epoch 265/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.5308 - acc: 0.8163 - val_loss: 0.6190 - val_acc: 0.7814\n",
            "Epoch 266/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.5372 - acc: 0.8112 - val_loss: 0.6336 - val_acc: 0.7820\n",
            "Epoch 267/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.5267 - acc: 0.8139 - val_loss: 0.6147 - val_acc: 0.7881\n",
            "Epoch 268/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.5330 - acc: 0.8097 - val_loss: 0.6075 - val_acc: 0.7924\n",
            "Epoch 269/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.5215 - acc: 0.8190 - val_loss: 0.6146 - val_acc: 0.7900\n",
            "Epoch 270/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.5246 - acc: 0.8184 - val_loss: 0.6435 - val_acc: 0.7771\n",
            "Epoch 271/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.5208 - acc: 0.8154 - val_loss: 0.6222 - val_acc: 0.7783\n",
            "Epoch 272/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.5198 - acc: 0.8109 - val_loss: 0.6355 - val_acc: 0.7906\n",
            "Epoch 273/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.5161 - acc: 0.8157 - val_loss: 0.6372 - val_acc: 0.7697\n",
            "Epoch 274/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.5286 - acc: 0.8181 - val_loss: 0.6341 - val_acc: 0.7734\n",
            "Epoch 275/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.4974 - acc: 0.8214 - val_loss: 0.6084 - val_acc: 0.7857\n",
            "Epoch 276/1000\n",
            "3315/3315 [==============================] - 2s 485us/step - loss: 0.5085 - acc: 0.8244 - val_loss: 0.5990 - val_acc: 0.7955\n",
            "Epoch 277/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.5224 - acc: 0.8060 - val_loss: 0.6113 - val_acc: 0.7838\n",
            "Epoch 278/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.5172 - acc: 0.8097 - val_loss: 0.5989 - val_acc: 0.8004\n",
            "Epoch 279/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.4995 - acc: 0.8265 - val_loss: 0.6124 - val_acc: 0.7955\n",
            "Epoch 280/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.5089 - acc: 0.8184 - val_loss: 0.6108 - val_acc: 0.7900\n",
            "Epoch 281/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.5010 - acc: 0.8247 - val_loss: 0.6215 - val_acc: 0.7844\n",
            "Epoch 282/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.5102 - acc: 0.8223 - val_loss: 0.6091 - val_acc: 0.7881\n",
            "Epoch 283/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.5056 - acc: 0.8214 - val_loss: 0.6238 - val_acc: 0.7746\n",
            "Epoch 284/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.4894 - acc: 0.8253 - val_loss: 0.5926 - val_acc: 0.8010\n",
            "Epoch 285/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.5001 - acc: 0.8284 - val_loss: 0.6183 - val_acc: 0.7863\n",
            "Epoch 286/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.5031 - acc: 0.8211 - val_loss: 0.6058 - val_acc: 0.7851\n",
            "Epoch 287/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.4930 - acc: 0.8299 - val_loss: 0.6090 - val_acc: 0.7740\n",
            "Epoch 288/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.4965 - acc: 0.8299 - val_loss: 0.6065 - val_acc: 0.8016\n",
            "Epoch 289/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.4812 - acc: 0.8347 - val_loss: 0.6140 - val_acc: 0.7912\n",
            "Epoch 290/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.4809 - acc: 0.8284 - val_loss: 0.5834 - val_acc: 0.8010\n",
            "Epoch 291/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.5051 - acc: 0.8259 - val_loss: 0.5983 - val_acc: 0.7967\n",
            "Epoch 292/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.4860 - acc: 0.8232 - val_loss: 0.5790 - val_acc: 0.8059\n",
            "Epoch 293/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.4857 - acc: 0.8287 - val_loss: 0.6035 - val_acc: 0.7869\n",
            "Epoch 294/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.4821 - acc: 0.8281 - val_loss: 0.6053 - val_acc: 0.7887\n",
            "Epoch 295/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.4755 - acc: 0.8281 - val_loss: 0.5860 - val_acc: 0.8059\n",
            "Epoch 296/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 0.4865 - acc: 0.8338 - val_loss: 0.5685 - val_acc: 0.8120\n",
            "Epoch 297/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.4830 - acc: 0.8356 - val_loss: 0.6014 - val_acc: 0.7936\n",
            "Epoch 298/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.4826 - acc: 0.8347 - val_loss: 0.5961 - val_acc: 0.7973\n",
            "Epoch 299/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.4720 - acc: 0.8392 - val_loss: 0.5837 - val_acc: 0.8132\n",
            "Epoch 300/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.4686 - acc: 0.8305 - val_loss: 0.5760 - val_acc: 0.8010\n",
            "Epoch 301/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.4586 - acc: 0.8404 - val_loss: 0.5914 - val_acc: 0.7930\n",
            "Epoch 302/1000\n",
            "3315/3315 [==============================] - 2s 520us/step - loss: 0.4680 - acc: 0.8428 - val_loss: 0.5776 - val_acc: 0.7985\n",
            "Epoch 303/1000\n",
            "3315/3315 [==============================] - 2s 546us/step - loss: 0.4696 - acc: 0.8368 - val_loss: 0.5873 - val_acc: 0.7979\n",
            "Epoch 304/1000\n",
            "3315/3315 [==============================] - 2s 554us/step - loss: 0.4589 - acc: 0.8392 - val_loss: 0.5762 - val_acc: 0.8004\n",
            "Epoch 305/1000\n",
            "3315/3315 [==============================] - 2s 553us/step - loss: 0.4581 - acc: 0.8407 - val_loss: 0.5779 - val_acc: 0.8059\n",
            "Epoch 306/1000\n",
            "3315/3315 [==============================] - 2s 551us/step - loss: 0.4771 - acc: 0.8278 - val_loss: 0.5835 - val_acc: 0.7955\n",
            "Epoch 307/1000\n",
            "3315/3315 [==============================] - 2s 552us/step - loss: 0.4676 - acc: 0.8332 - val_loss: 0.5590 - val_acc: 0.8200\n",
            "Epoch 308/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.4653 - acc: 0.8386 - val_loss: 0.5791 - val_acc: 0.7936\n",
            "Epoch 309/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.4543 - acc: 0.8377 - val_loss: 0.5755 - val_acc: 0.8022\n",
            "Epoch 310/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.4653 - acc: 0.8353 - val_loss: 0.5620 - val_acc: 0.8187\n",
            "Epoch 311/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.4658 - acc: 0.8341 - val_loss: 0.5667 - val_acc: 0.8053\n",
            "Epoch 312/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.4618 - acc: 0.8419 - val_loss: 0.5946 - val_acc: 0.7900\n",
            "Epoch 313/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.4529 - acc: 0.8398 - val_loss: 0.5676 - val_acc: 0.8059\n",
            "Epoch 314/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.4703 - acc: 0.8305 - val_loss: 0.5630 - val_acc: 0.8040\n",
            "Epoch 315/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.4577 - acc: 0.8449 - val_loss: 0.5677 - val_acc: 0.8065\n",
            "Epoch 316/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.4584 - acc: 0.8341 - val_loss: 0.5509 - val_acc: 0.8249\n",
            "Epoch 317/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.4575 - acc: 0.8395 - val_loss: 0.5609 - val_acc: 0.8065\n",
            "Epoch 318/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.4426 - acc: 0.8480 - val_loss: 0.5504 - val_acc: 0.8169\n",
            "Epoch 319/1000\n",
            "3315/3315 [==============================] - 2s 509us/step - loss: 0.4475 - acc: 0.8398 - val_loss: 0.5739 - val_acc: 0.8053\n",
            "Epoch 320/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.4650 - acc: 0.8377 - val_loss: 0.5359 - val_acc: 0.8267\n",
            "Epoch 321/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.4561 - acc: 0.8419 - val_loss: 0.5445 - val_acc: 0.8175\n",
            "Epoch 322/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.4426 - acc: 0.8416 - val_loss: 0.5503 - val_acc: 0.8181\n",
            "Epoch 323/1000\n",
            "3315/3315 [==============================] - 2s 511us/step - loss: 0.4572 - acc: 0.8407 - val_loss: 0.5473 - val_acc: 0.8120\n",
            "Epoch 324/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.4567 - acc: 0.8452 - val_loss: 0.5492 - val_acc: 0.8200\n",
            "Epoch 325/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.4532 - acc: 0.8401 - val_loss: 0.5480 - val_acc: 0.8200\n",
            "Epoch 326/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.4392 - acc: 0.8465 - val_loss: 0.5441 - val_acc: 0.8218\n",
            "Epoch 327/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.4299 - acc: 0.8480 - val_loss: 0.5459 - val_acc: 0.8181\n",
            "Epoch 328/1000\n",
            "3315/3315 [==============================] - 2s 510us/step - loss: 0.4452 - acc: 0.8416 - val_loss: 0.5512 - val_acc: 0.8151\n",
            "Epoch 329/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.4405 - acc: 0.8383 - val_loss: 0.5605 - val_acc: 0.8114\n",
            "Epoch 330/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.4373 - acc: 0.8486 - val_loss: 0.5375 - val_acc: 0.8206\n",
            "Epoch 331/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.4462 - acc: 0.8431 - val_loss: 0.5595 - val_acc: 0.8083\n",
            "Epoch 332/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.4205 - acc: 0.8516 - val_loss: 0.5581 - val_acc: 0.8145\n",
            "Epoch 333/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.4256 - acc: 0.8519 - val_loss: 0.5418 - val_acc: 0.8236\n",
            "Epoch 334/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.4240 - acc: 0.8534 - val_loss: 0.5397 - val_acc: 0.8175\n",
            "Epoch 335/1000\n",
            "3315/3315 [==============================] - 2s 552us/step - loss: 0.4256 - acc: 0.8549 - val_loss: 0.5502 - val_acc: 0.8077\n",
            "Epoch 336/1000\n",
            "3315/3315 [==============================] - 2s 549us/step - loss: 0.4401 - acc: 0.8510 - val_loss: 0.5772 - val_acc: 0.7998\n",
            "Epoch 337/1000\n",
            "3315/3315 [==============================] - 2s 542us/step - loss: 0.4353 - acc: 0.8492 - val_loss: 0.5157 - val_acc: 0.8359\n",
            "Epoch 338/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.4137 - acc: 0.8465 - val_loss: 0.5697 - val_acc: 0.7991\n",
            "Epoch 339/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.4184 - acc: 0.8552 - val_loss: 0.5381 - val_acc: 0.8230\n",
            "Epoch 340/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.4165 - acc: 0.8549 - val_loss: 0.5594 - val_acc: 0.8126\n",
            "Epoch 341/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.4347 - acc: 0.8486 - val_loss: 0.5417 - val_acc: 0.8194\n",
            "Epoch 342/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.4287 - acc: 0.8498 - val_loss: 0.5504 - val_acc: 0.8132\n",
            "Epoch 343/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.4172 - acc: 0.8471 - val_loss: 0.5311 - val_acc: 0.8218\n",
            "Epoch 344/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.4326 - acc: 0.8504 - val_loss: 0.5466 - val_acc: 0.8145\n",
            "Epoch 345/1000\n",
            "3315/3315 [==============================] - 2s 511us/step - loss: 0.4199 - acc: 0.8534 - val_loss: 0.5337 - val_acc: 0.8187\n",
            "Epoch 346/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.4274 - acc: 0.8510 - val_loss: 0.5217 - val_acc: 0.8255\n",
            "Epoch 347/1000\n",
            "3315/3315 [==============================] - 2s 485us/step - loss: 0.4066 - acc: 0.8549 - val_loss: 0.5321 - val_acc: 0.8242\n",
            "Epoch 348/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.4006 - acc: 0.8585 - val_loss: 0.5368 - val_acc: 0.8157\n",
            "Epoch 349/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.4089 - acc: 0.8594 - val_loss: 0.5335 - val_acc: 0.8200\n",
            "Epoch 350/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.4174 - acc: 0.8525 - val_loss: 0.5340 - val_acc: 0.8230\n",
            "Epoch 351/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.4206 - acc: 0.8543 - val_loss: 0.5357 - val_acc: 0.8163\n",
            "Epoch 352/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.4096 - acc: 0.8585 - val_loss: 0.5190 - val_acc: 0.8230\n",
            "Epoch 353/1000\n",
            "3315/3315 [==============================] - 2s 508us/step - loss: 0.4022 - acc: 0.8661 - val_loss: 0.5522 - val_acc: 0.8120\n",
            "Epoch 354/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.4105 - acc: 0.8582 - val_loss: 0.5183 - val_acc: 0.8328\n",
            "Epoch 355/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.4091 - acc: 0.8558 - val_loss: 0.5192 - val_acc: 0.8310\n",
            "Epoch 356/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.4025 - acc: 0.8679 - val_loss: 0.5163 - val_acc: 0.8224\n",
            "Epoch 357/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.4088 - acc: 0.8655 - val_loss: 0.5184 - val_acc: 0.8255\n",
            "Epoch 358/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.4061 - acc: 0.8591 - val_loss: 0.5139 - val_acc: 0.8371\n",
            "Epoch 359/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.4038 - acc: 0.8612 - val_loss: 0.5242 - val_acc: 0.8334\n",
            "Epoch 360/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 0.3973 - acc: 0.8643 - val_loss: 0.5438 - val_acc: 0.8181\n",
            "Epoch 361/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.4086 - acc: 0.8615 - val_loss: 0.5221 - val_acc: 0.8310\n",
            "Epoch 362/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.4103 - acc: 0.8489 - val_loss: 0.5063 - val_acc: 0.8310\n",
            "Epoch 363/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.3959 - acc: 0.8649 - val_loss: 0.5468 - val_acc: 0.8138\n",
            "Epoch 364/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.3940 - acc: 0.8652 - val_loss: 0.5313 - val_acc: 0.8230\n",
            "Epoch 365/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.3967 - acc: 0.8664 - val_loss: 0.5138 - val_acc: 0.8267\n",
            "Epoch 366/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.4046 - acc: 0.8637 - val_loss: 0.5209 - val_acc: 0.8236\n",
            "Epoch 367/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.3935 - acc: 0.8697 - val_loss: 0.5223 - val_acc: 0.8249\n",
            "Epoch 368/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.3943 - acc: 0.8676 - val_loss: 0.5134 - val_acc: 0.8249\n",
            "Epoch 369/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.3824 - acc: 0.8715 - val_loss: 0.5091 - val_acc: 0.8396\n",
            "Epoch 370/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.3841 - acc: 0.8682 - val_loss: 0.5189 - val_acc: 0.8236\n",
            "Epoch 371/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.3795 - acc: 0.8736 - val_loss: 0.5138 - val_acc: 0.8365\n",
            "Epoch 372/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.3957 - acc: 0.8649 - val_loss: 0.5514 - val_acc: 0.8187\n",
            "Epoch 373/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.3771 - acc: 0.8609 - val_loss: 0.4987 - val_acc: 0.8463\n",
            "Epoch 374/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.3864 - acc: 0.8630 - val_loss: 0.5121 - val_acc: 0.8291\n",
            "Epoch 375/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.3781 - acc: 0.8646 - val_loss: 0.4970 - val_acc: 0.8383\n",
            "Epoch 376/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.3855 - acc: 0.8627 - val_loss: 0.4975 - val_acc: 0.8371\n",
            "Epoch 377/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.3935 - acc: 0.8621 - val_loss: 0.5048 - val_acc: 0.8298\n",
            "Epoch 378/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.3802 - acc: 0.8670 - val_loss: 0.5144 - val_acc: 0.8194\n",
            "Epoch 379/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.3821 - acc: 0.8637 - val_loss: 0.5434 - val_acc: 0.8151\n",
            "Epoch 380/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.3801 - acc: 0.8627 - val_loss: 0.4913 - val_acc: 0.8500\n",
            "Epoch 381/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.3749 - acc: 0.8709 - val_loss: 0.5115 - val_acc: 0.8396\n",
            "Epoch 382/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 0.3799 - acc: 0.8694 - val_loss: 0.4924 - val_acc: 0.8438\n",
            "Epoch 383/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.3886 - acc: 0.8667 - val_loss: 0.5270 - val_acc: 0.8291\n",
            "Epoch 384/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.3633 - acc: 0.8658 - val_loss: 0.5808 - val_acc: 0.7991\n",
            "Epoch 385/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.3757 - acc: 0.8679 - val_loss: 0.4922 - val_acc: 0.8469\n",
            "Epoch 386/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.3768 - acc: 0.8694 - val_loss: 0.4972 - val_acc: 0.8396\n",
            "Epoch 387/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.3772 - acc: 0.8676 - val_loss: 0.5041 - val_acc: 0.8353\n",
            "Epoch 388/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.3686 - acc: 0.8739 - val_loss: 0.4918 - val_acc: 0.8445\n",
            "Epoch 389/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.3780 - acc: 0.8664 - val_loss: 0.4958 - val_acc: 0.8426\n",
            "Epoch 390/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.3770 - acc: 0.8685 - val_loss: 0.4874 - val_acc: 0.8377\n",
            "Epoch 391/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.3541 - acc: 0.8724 - val_loss: 0.4819 - val_acc: 0.8457\n",
            "Epoch 392/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.3628 - acc: 0.8733 - val_loss: 0.4847 - val_acc: 0.8469\n",
            "Epoch 393/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.3754 - acc: 0.8706 - val_loss: 0.4920 - val_acc: 0.8420\n",
            "Epoch 394/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.3577 - acc: 0.8682 - val_loss: 0.5246 - val_acc: 0.8255\n",
            "Epoch 395/1000\n",
            "3315/3315 [==============================] - 2s 518us/step - loss: 0.3647 - acc: 0.8751 - val_loss: 0.4886 - val_acc: 0.8353\n",
            "Epoch 396/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.3732 - acc: 0.8697 - val_loss: 0.5019 - val_acc: 0.8334\n",
            "Epoch 397/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.3531 - acc: 0.8824 - val_loss: 0.4881 - val_acc: 0.8389\n",
            "Epoch 398/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.3457 - acc: 0.8824 - val_loss: 0.4807 - val_acc: 0.8475\n",
            "Epoch 399/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.3600 - acc: 0.8757 - val_loss: 0.4879 - val_acc: 0.8457\n",
            "Epoch 400/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.3567 - acc: 0.8760 - val_loss: 0.4980 - val_acc: 0.8273\n",
            "Epoch 401/1000\n",
            "3315/3315 [==============================] - 2s 512us/step - loss: 0.3587 - acc: 0.8784 - val_loss: 0.4719 - val_acc: 0.8438\n",
            "Epoch 402/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.3542 - acc: 0.8745 - val_loss: 0.4885 - val_acc: 0.8396\n",
            "Epoch 403/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.3716 - acc: 0.8733 - val_loss: 0.4734 - val_acc: 0.8445\n",
            "Epoch 404/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.3553 - acc: 0.8730 - val_loss: 0.4799 - val_acc: 0.8445\n",
            "Epoch 405/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 0.3475 - acc: 0.8787 - val_loss: 0.4849 - val_acc: 0.8408\n",
            "Epoch 406/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.3526 - acc: 0.8769 - val_loss: 0.5012 - val_acc: 0.8298\n",
            "Epoch 407/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.3450 - acc: 0.8842 - val_loss: 0.5160 - val_acc: 0.8267\n",
            "Epoch 408/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.3566 - acc: 0.8778 - val_loss: 0.4881 - val_acc: 0.8371\n",
            "Epoch 409/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.3520 - acc: 0.8802 - val_loss: 0.4746 - val_acc: 0.8420\n",
            "Epoch 410/1000\n",
            "3315/3315 [==============================] - 2s 485us/step - loss: 0.3566 - acc: 0.8775 - val_loss: 0.4747 - val_acc: 0.8457\n",
            "Epoch 411/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.3400 - acc: 0.8830 - val_loss: 0.4775 - val_acc: 0.8389\n",
            "Epoch 412/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.3490 - acc: 0.8827 - val_loss: 0.4763 - val_acc: 0.8481\n",
            "Epoch 413/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.3389 - acc: 0.8833 - val_loss: 0.4682 - val_acc: 0.8487\n",
            "Epoch 414/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.3593 - acc: 0.8718 - val_loss: 0.4868 - val_acc: 0.8426\n",
            "Epoch 415/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.3536 - acc: 0.8757 - val_loss: 0.4786 - val_acc: 0.8432\n",
            "Epoch 416/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.3401 - acc: 0.8860 - val_loss: 0.4799 - val_acc: 0.8371\n",
            "Epoch 417/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.3461 - acc: 0.8817 - val_loss: 0.4623 - val_acc: 0.8530\n",
            "Epoch 418/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.3434 - acc: 0.8817 - val_loss: 0.4640 - val_acc: 0.8579\n",
            "Epoch 419/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.3418 - acc: 0.8811 - val_loss: 0.4600 - val_acc: 0.8518\n",
            "Epoch 420/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.3316 - acc: 0.8899 - val_loss: 0.4773 - val_acc: 0.8377\n",
            "Epoch 421/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.3387 - acc: 0.8893 - val_loss: 0.4652 - val_acc: 0.8549\n",
            "Epoch 422/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.3329 - acc: 0.8851 - val_loss: 0.4554 - val_acc: 0.8579\n",
            "Epoch 423/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 0.3407 - acc: 0.8821 - val_loss: 0.4815 - val_acc: 0.8408\n",
            "Epoch 424/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.3242 - acc: 0.8938 - val_loss: 0.4724 - val_acc: 0.8481\n",
            "Epoch 425/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.3507 - acc: 0.8757 - val_loss: 0.4715 - val_acc: 0.8475\n",
            "Epoch 426/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.3433 - acc: 0.8817 - val_loss: 0.4631 - val_acc: 0.8585\n",
            "Epoch 427/1000\n",
            "3315/3315 [==============================] - 2s 509us/step - loss: 0.3358 - acc: 0.8869 - val_loss: 0.4627 - val_acc: 0.8579\n",
            "Epoch 428/1000\n",
            "3315/3315 [==============================] - 2s 520us/step - loss: 0.3297 - acc: 0.8869 - val_loss: 0.4734 - val_acc: 0.8408\n",
            "Epoch 429/1000\n",
            "3315/3315 [==============================] - 2s 515us/step - loss: 0.3423 - acc: 0.8839 - val_loss: 0.4524 - val_acc: 0.8500\n",
            "Epoch 430/1000\n",
            "3315/3315 [==============================] - 2s 512us/step - loss: 0.3491 - acc: 0.8905 - val_loss: 0.4596 - val_acc: 0.8549\n",
            "Epoch 431/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.3199 - acc: 0.8947 - val_loss: 0.4560 - val_acc: 0.8487\n",
            "Epoch 432/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 0.3456 - acc: 0.8821 - val_loss: 0.4334 - val_acc: 0.8677\n",
            "Epoch 433/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.3226 - acc: 0.8890 - val_loss: 0.4600 - val_acc: 0.8487\n",
            "Epoch 434/1000\n",
            "3315/3315 [==============================] - 2s 507us/step - loss: 0.3216 - acc: 0.8953 - val_loss: 0.4518 - val_acc: 0.8604\n",
            "Epoch 435/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.3306 - acc: 0.8833 - val_loss: 0.4793 - val_acc: 0.8420\n",
            "Epoch 436/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.3405 - acc: 0.8817 - val_loss: 0.4654 - val_acc: 0.8506\n",
            "Epoch 437/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.3342 - acc: 0.8890 - val_loss: 0.4699 - val_acc: 0.8494\n",
            "Epoch 438/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.3179 - acc: 0.8917 - val_loss: 0.4456 - val_acc: 0.8653\n",
            "Epoch 439/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.3312 - acc: 0.8848 - val_loss: 0.4591 - val_acc: 0.8518\n",
            "Epoch 440/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.3294 - acc: 0.8811 - val_loss: 0.4562 - val_acc: 0.8579\n",
            "Epoch 441/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 0.3248 - acc: 0.8854 - val_loss: 0.4506 - val_acc: 0.8555\n",
            "Epoch 442/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.3321 - acc: 0.8851 - val_loss: 0.4373 - val_acc: 0.8659\n",
            "Epoch 443/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.3304 - acc: 0.8872 - val_loss: 0.4438 - val_acc: 0.8518\n",
            "Epoch 444/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.3098 - acc: 0.8890 - val_loss: 0.4821 - val_acc: 0.8340\n",
            "Epoch 445/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.3202 - acc: 0.8944 - val_loss: 0.5176 - val_acc: 0.8218\n",
            "Epoch 446/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.3082 - acc: 0.8995 - val_loss: 0.4574 - val_acc: 0.8469\n",
            "Epoch 447/1000\n",
            "3315/3315 [==============================] - 2s 485us/step - loss: 0.3159 - acc: 0.8896 - val_loss: 0.4863 - val_acc: 0.8389\n",
            "Epoch 448/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.3157 - acc: 0.8914 - val_loss: 0.4588 - val_acc: 0.8494\n",
            "Epoch 449/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.3123 - acc: 0.8863 - val_loss: 0.4413 - val_acc: 0.8622\n",
            "Epoch 450/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.3184 - acc: 0.8878 - val_loss: 0.4579 - val_acc: 0.8494\n",
            "Epoch 451/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.3099 - acc: 0.8917 - val_loss: 0.4384 - val_acc: 0.8555\n",
            "Epoch 452/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.3172 - acc: 0.8920 - val_loss: 0.4693 - val_acc: 0.8506\n",
            "Epoch 453/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.3076 - acc: 0.8956 - val_loss: 0.4506 - val_acc: 0.8530\n",
            "Epoch 454/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.3038 - acc: 0.8938 - val_loss: 0.4684 - val_acc: 0.8506\n",
            "Epoch 455/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.3122 - acc: 0.8950 - val_loss: 0.4431 - val_acc: 0.8585\n",
            "Epoch 456/1000\n",
            "3315/3315 [==============================] - 2s 508us/step - loss: 0.3190 - acc: 0.8896 - val_loss: 0.4408 - val_acc: 0.8641\n",
            "Epoch 457/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.3074 - acc: 0.8950 - val_loss: 0.4500 - val_acc: 0.8512\n",
            "Epoch 458/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.3026 - acc: 0.8971 - val_loss: 0.4360 - val_acc: 0.8481\n",
            "Epoch 459/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.3125 - acc: 0.8929 - val_loss: 0.4551 - val_acc: 0.8616\n",
            "Epoch 460/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.2921 - acc: 0.9017 - val_loss: 0.4490 - val_acc: 0.8530\n",
            "Epoch 461/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.3157 - acc: 0.8905 - val_loss: 0.4434 - val_acc: 0.8671\n",
            "Epoch 462/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.3273 - acc: 0.8905 - val_loss: 0.4680 - val_acc: 0.8524\n",
            "Epoch 463/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.3109 - acc: 0.8902 - val_loss: 0.4492 - val_acc: 0.8524\n",
            "Epoch 464/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.2876 - acc: 0.9041 - val_loss: 0.4327 - val_acc: 0.8585\n",
            "Epoch 465/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.3066 - acc: 0.8956 - val_loss: 0.4436 - val_acc: 0.8653\n",
            "Epoch 466/1000\n",
            "3315/3315 [==============================] - 2s 507us/step - loss: 0.2997 - acc: 0.9050 - val_loss: 0.4474 - val_acc: 0.8500\n",
            "Epoch 467/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.3117 - acc: 0.8908 - val_loss: 0.4518 - val_acc: 0.8481\n",
            "Epoch 468/1000\n",
            "3315/3315 [==============================] - 2s 515us/step - loss: 0.2992 - acc: 0.8992 - val_loss: 0.4268 - val_acc: 0.8628\n",
            "Epoch 469/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.2922 - acc: 0.9002 - val_loss: 0.4822 - val_acc: 0.8396\n",
            "Epoch 470/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.3092 - acc: 0.8953 - val_loss: 0.4435 - val_acc: 0.8549\n",
            "Epoch 471/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.3143 - acc: 0.8983 - val_loss: 0.4541 - val_acc: 0.8604\n",
            "Epoch 472/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.3121 - acc: 0.8932 - val_loss: 0.4238 - val_acc: 0.8665\n",
            "Epoch 473/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.3101 - acc: 0.8890 - val_loss: 0.4198 - val_acc: 0.8659\n",
            "Epoch 474/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.2993 - acc: 0.8983 - val_loss: 0.4261 - val_acc: 0.8647\n",
            "Epoch 475/1000\n",
            "3315/3315 [==============================] - 2s 485us/step - loss: 0.2966 - acc: 0.9038 - val_loss: 0.4331 - val_acc: 0.8641\n",
            "Epoch 476/1000\n",
            "3315/3315 [==============================] - 2s 509us/step - loss: 0.3017 - acc: 0.8932 - val_loss: 0.4425 - val_acc: 0.8628\n",
            "Epoch 477/1000\n",
            "3315/3315 [==============================] - 2s 514us/step - loss: 0.2869 - acc: 0.8971 - val_loss: 0.4611 - val_acc: 0.8481\n",
            "Epoch 478/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.2943 - acc: 0.9050 - val_loss: 0.4336 - val_acc: 0.8726\n",
            "Epoch 479/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.2971 - acc: 0.8929 - val_loss: 0.4400 - val_acc: 0.8561\n",
            "Epoch 480/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.2868 - acc: 0.9017 - val_loss: 0.4330 - val_acc: 0.8622\n",
            "Epoch 481/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.2955 - acc: 0.8956 - val_loss: 0.4277 - val_acc: 0.8702\n",
            "Epoch 482/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.2893 - acc: 0.8974 - val_loss: 0.4340 - val_acc: 0.8622\n",
            "Epoch 483/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.2839 - acc: 0.9017 - val_loss: 0.4063 - val_acc: 0.8683\n",
            "Epoch 484/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.2838 - acc: 0.9014 - val_loss: 0.4270 - val_acc: 0.8610\n",
            "Epoch 485/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.2878 - acc: 0.9071 - val_loss: 0.4525 - val_acc: 0.8506\n",
            "Epoch 486/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.2958 - acc: 0.8989 - val_loss: 0.4455 - val_acc: 0.8573\n",
            "Epoch 487/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.2905 - acc: 0.9029 - val_loss: 0.4218 - val_acc: 0.8781\n",
            "Epoch 488/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.2876 - acc: 0.9032 - val_loss: 0.4400 - val_acc: 0.8555\n",
            "Epoch 489/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.2961 - acc: 0.8995 - val_loss: 0.4403 - val_acc: 0.8622\n",
            "Epoch 490/1000\n",
            "3315/3315 [==============================] - 2s 514us/step - loss: 0.2907 - acc: 0.9041 - val_loss: 0.4361 - val_acc: 0.8622\n",
            "Epoch 491/1000\n",
            "3315/3315 [==============================] - 2s 551us/step - loss: 0.2789 - acc: 0.9074 - val_loss: 0.4161 - val_acc: 0.8653\n",
            "Epoch 492/1000\n",
            "3315/3315 [==============================] - 2s 550us/step - loss: 0.2808 - acc: 0.9014 - val_loss: 0.4588 - val_acc: 0.8585\n",
            "Epoch 493/1000\n",
            "3315/3315 [==============================] - 2s 554us/step - loss: 0.2768 - acc: 0.9056 - val_loss: 0.4263 - val_acc: 0.8634\n",
            "Epoch 494/1000\n",
            "3315/3315 [==============================] - 2s 561us/step - loss: 0.2929 - acc: 0.8947 - val_loss: 0.4280 - val_acc: 0.8610\n",
            "Epoch 495/1000\n",
            "3315/3315 [==============================] - 2s 548us/step - loss: 0.2978 - acc: 0.8950 - val_loss: 0.4254 - val_acc: 0.8653\n",
            "Epoch 496/1000\n",
            "3315/3315 [==============================] - 2s 507us/step - loss: 0.2872 - acc: 0.9014 - val_loss: 0.4288 - val_acc: 0.8677\n",
            "Epoch 497/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.2847 - acc: 0.9008 - val_loss: 0.4174 - val_acc: 0.8708\n",
            "Epoch 498/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.2833 - acc: 0.9032 - val_loss: 0.4263 - val_acc: 0.8567\n",
            "Epoch 499/1000\n",
            "3315/3315 [==============================] - 2s 507us/step - loss: 0.2845 - acc: 0.9035 - val_loss: 0.4487 - val_acc: 0.8579\n",
            "Epoch 500/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 0.2756 - acc: 0.9053 - val_loss: 0.4069 - val_acc: 0.8702\n",
            "Epoch 501/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 0.2735 - acc: 0.9095 - val_loss: 0.4455 - val_acc: 0.8543\n",
            "Epoch 502/1000\n",
            "3315/3315 [==============================] - 2s 507us/step - loss: 0.2871 - acc: 0.9008 - val_loss: 0.4056 - val_acc: 0.8788\n",
            "Epoch 503/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.2799 - acc: 0.9017 - val_loss: 0.4244 - val_acc: 0.8616\n",
            "Epoch 504/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.2789 - acc: 0.9083 - val_loss: 0.4262 - val_acc: 0.8610\n",
            "Epoch 505/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.2764 - acc: 0.9065 - val_loss: 0.4218 - val_acc: 0.8628\n",
            "Epoch 506/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.2721 - acc: 0.9140 - val_loss: 0.4807 - val_acc: 0.8420\n",
            "Epoch 507/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.2843 - acc: 0.9092 - val_loss: 0.4342 - val_acc: 0.8567\n",
            "Epoch 508/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.2818 - acc: 0.9020 - val_loss: 0.4401 - val_acc: 0.8567\n",
            "Epoch 509/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.2907 - acc: 0.9014 - val_loss: 0.4027 - val_acc: 0.8708\n",
            "Epoch 510/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.2737 - acc: 0.9080 - val_loss: 0.4238 - val_acc: 0.8616\n",
            "Epoch 511/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.2762 - acc: 0.9008 - val_loss: 0.4262 - val_acc: 0.8677\n",
            "Epoch 512/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.2763 - acc: 0.9065 - val_loss: 0.4163 - val_acc: 0.8628\n",
            "Epoch 513/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.2823 - acc: 0.9017 - val_loss: 0.4154 - val_acc: 0.8714\n",
            "Epoch 514/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.2831 - acc: 0.9086 - val_loss: 0.4188 - val_acc: 0.8671\n",
            "Epoch 515/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.2784 - acc: 0.9005 - val_loss: 0.4068 - val_acc: 0.8739\n",
            "Epoch 516/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.2675 - acc: 0.9074 - val_loss: 0.4116 - val_acc: 0.8634\n",
            "Epoch 517/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.2927 - acc: 0.8980 - val_loss: 0.4175 - val_acc: 0.8604\n",
            "Epoch 518/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.2724 - acc: 0.9062 - val_loss: 0.4241 - val_acc: 0.8732\n",
            "Epoch 519/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.2591 - acc: 0.9122 - val_loss: 0.4463 - val_acc: 0.8506\n",
            "Epoch 520/1000\n",
            "3315/3315 [==============================] - 2s 572us/step - loss: 0.2715 - acc: 0.9044 - val_loss: 0.4074 - val_acc: 0.8757\n",
            "Epoch 521/1000\n",
            "3315/3315 [==============================] - 2s 565us/step - loss: 0.2697 - acc: 0.9104 - val_loss: 0.3985 - val_acc: 0.8806\n",
            "Epoch 522/1000\n",
            "3315/3315 [==============================] - 2s 557us/step - loss: 0.2725 - acc: 0.9041 - val_loss: 0.4251 - val_acc: 0.8690\n",
            "Epoch 523/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.2728 - acc: 0.9050 - val_loss: 0.4265 - val_acc: 0.8665\n",
            "Epoch 524/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.2670 - acc: 0.9071 - val_loss: 0.4266 - val_acc: 0.8659\n",
            "Epoch 525/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.2750 - acc: 0.9029 - val_loss: 0.4095 - val_acc: 0.8732\n",
            "Epoch 526/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.2583 - acc: 0.9119 - val_loss: 0.4091 - val_acc: 0.8726\n",
            "Epoch 527/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.2750 - acc: 0.9083 - val_loss: 0.4334 - val_acc: 0.8524\n",
            "Epoch 528/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.2633 - acc: 0.9146 - val_loss: 0.4138 - val_acc: 0.8739\n",
            "Epoch 529/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.2653 - acc: 0.9068 - val_loss: 0.3844 - val_acc: 0.8836\n",
            "Epoch 530/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.2681 - acc: 0.9080 - val_loss: 0.4235 - val_acc: 0.8696\n",
            "Epoch 531/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.2656 - acc: 0.9077 - val_loss: 0.4089 - val_acc: 0.8781\n",
            "Epoch 532/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.2562 - acc: 0.9083 - val_loss: 0.4056 - val_acc: 0.8726\n",
            "Epoch 533/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.2603 - acc: 0.9101 - val_loss: 0.4016 - val_acc: 0.8800\n",
            "Epoch 534/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.2632 - acc: 0.9071 - val_loss: 0.3982 - val_acc: 0.8739\n",
            "Epoch 535/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.2511 - acc: 0.9137 - val_loss: 0.4029 - val_acc: 0.8732\n",
            "Epoch 536/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.2517 - acc: 0.9170 - val_loss: 0.4098 - val_acc: 0.8665\n",
            "Epoch 537/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.2567 - acc: 0.9137 - val_loss: 0.4030 - val_acc: 0.8757\n",
            "Epoch 538/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.2489 - acc: 0.9158 - val_loss: 0.3926 - val_acc: 0.8843\n",
            "Epoch 539/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.2603 - acc: 0.9077 - val_loss: 0.4224 - val_acc: 0.8628\n",
            "Epoch 540/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 0.2592 - acc: 0.9059 - val_loss: 0.4073 - val_acc: 0.8757\n",
            "Epoch 541/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.2716 - acc: 0.9089 - val_loss: 0.4121 - val_acc: 0.8763\n",
            "Epoch 542/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.2528 - acc: 0.9101 - val_loss: 0.4083 - val_acc: 0.8708\n",
            "Epoch 543/1000\n",
            "3315/3315 [==============================] - 2s 519us/step - loss: 0.2598 - acc: 0.9116 - val_loss: 0.4073 - val_acc: 0.8702\n",
            "Epoch 544/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.2608 - acc: 0.9074 - val_loss: 0.4078 - val_acc: 0.8720\n",
            "Epoch 545/1000\n",
            "3315/3315 [==============================] - 2s 509us/step - loss: 0.2591 - acc: 0.9074 - val_loss: 0.4070 - val_acc: 0.8739\n",
            "Epoch 546/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.2536 - acc: 0.9158 - val_loss: 0.4058 - val_acc: 0.8745\n",
            "Epoch 547/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.2454 - acc: 0.9167 - val_loss: 0.4169 - val_acc: 0.8720\n",
            "Epoch 548/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.2470 - acc: 0.9152 - val_loss: 0.4053 - val_acc: 0.8757\n",
            "Epoch 549/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.2557 - acc: 0.9158 - val_loss: 0.4038 - val_acc: 0.8757\n",
            "Epoch 550/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.2577 - acc: 0.9089 - val_loss: 0.4155 - val_acc: 0.8616\n",
            "Epoch 551/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.2498 - acc: 0.9146 - val_loss: 0.4118 - val_acc: 0.8720\n",
            "Epoch 552/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.2379 - acc: 0.9201 - val_loss: 0.3916 - val_acc: 0.8794\n",
            "Epoch 553/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.2540 - acc: 0.9167 - val_loss: 0.3987 - val_acc: 0.8739\n",
            "Epoch 554/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.2520 - acc: 0.9068 - val_loss: 0.4007 - val_acc: 0.8800\n",
            "Epoch 555/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.2495 - acc: 0.9098 - val_loss: 0.3905 - val_acc: 0.8843\n",
            "Epoch 556/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.2524 - acc: 0.9128 - val_loss: 0.3999 - val_acc: 0.8757\n",
            "Epoch 557/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.2638 - acc: 0.9104 - val_loss: 0.4129 - val_acc: 0.8726\n",
            "Epoch 558/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.2494 - acc: 0.9119 - val_loss: 0.3941 - val_acc: 0.8861\n",
            "Epoch 559/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.2439 - acc: 0.9161 - val_loss: 0.3951 - val_acc: 0.8800\n",
            "Epoch 560/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.2309 - acc: 0.9216 - val_loss: 0.4012 - val_acc: 0.8788\n",
            "Epoch 561/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.2432 - acc: 0.9189 - val_loss: 0.3896 - val_acc: 0.8763\n",
            "Epoch 562/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.2512 - acc: 0.9189 - val_loss: 0.4465 - val_acc: 0.8598\n",
            "Epoch 563/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.2666 - acc: 0.9080 - val_loss: 0.4111 - val_acc: 0.8806\n",
            "Epoch 564/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.2315 - acc: 0.9222 - val_loss: 0.3877 - val_acc: 0.8788\n",
            "Epoch 565/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.2474 - acc: 0.9161 - val_loss: 0.3917 - val_acc: 0.8769\n",
            "Epoch 566/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.2516 - acc: 0.9137 - val_loss: 0.4064 - val_acc: 0.8739\n",
            "Epoch 567/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.2327 - acc: 0.9243 - val_loss: 0.3802 - val_acc: 0.8843\n",
            "Epoch 568/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.2425 - acc: 0.9186 - val_loss: 0.3964 - val_acc: 0.8775\n",
            "Epoch 569/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.2356 - acc: 0.9201 - val_loss: 0.4246 - val_acc: 0.8616\n",
            "Epoch 570/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.2406 - acc: 0.9155 - val_loss: 0.3933 - val_acc: 0.8751\n",
            "Epoch 571/1000\n",
            "3315/3315 [==============================] - 2s 514us/step - loss: 0.2370 - acc: 0.9237 - val_loss: 0.3891 - val_acc: 0.8812\n",
            "Epoch 572/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.2274 - acc: 0.9219 - val_loss: 0.4013 - val_acc: 0.8830\n",
            "Epoch 573/1000\n",
            "3315/3315 [==============================] - 2s 523us/step - loss: 0.2440 - acc: 0.9179 - val_loss: 0.3827 - val_acc: 0.8885\n",
            "Epoch 574/1000\n",
            "3315/3315 [==============================] - 2s 515us/step - loss: 0.2267 - acc: 0.9176 - val_loss: 0.4044 - val_acc: 0.8781\n",
            "Epoch 575/1000\n",
            "3315/3315 [==============================] - 2s 519us/step - loss: 0.2500 - acc: 0.9198 - val_loss: 0.3908 - val_acc: 0.8836\n",
            "Epoch 576/1000\n",
            "3315/3315 [==============================] - 2s 517us/step - loss: 0.2312 - acc: 0.9258 - val_loss: 0.3938 - val_acc: 0.8726\n",
            "Epoch 577/1000\n",
            "3315/3315 [==============================] - 2s 522us/step - loss: 0.2457 - acc: 0.9204 - val_loss: 0.3967 - val_acc: 0.8775\n",
            "Epoch 578/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.2311 - acc: 0.9234 - val_loss: 0.4225 - val_acc: 0.8641\n",
            "Epoch 579/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.2415 - acc: 0.9204 - val_loss: 0.3856 - val_acc: 0.8788\n",
            "Epoch 580/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.2448 - acc: 0.9207 - val_loss: 0.3861 - val_acc: 0.8824\n",
            "Epoch 581/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.2317 - acc: 0.9276 - val_loss: 0.3821 - val_acc: 0.8855\n",
            "Epoch 582/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.2349 - acc: 0.9149 - val_loss: 0.3825 - val_acc: 0.8800\n",
            "Epoch 583/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 0.2235 - acc: 0.9219 - val_loss: 0.3944 - val_acc: 0.8806\n",
            "Epoch 584/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.2384 - acc: 0.9213 - val_loss: 0.3960 - val_acc: 0.8916\n",
            "Epoch 585/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.2265 - acc: 0.9222 - val_loss: 0.3943 - val_acc: 0.8757\n",
            "Epoch 586/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.2278 - acc: 0.9264 - val_loss: 0.4347 - val_acc: 0.8610\n",
            "Epoch 587/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.2446 - acc: 0.9155 - val_loss: 0.3895 - val_acc: 0.8794\n",
            "Epoch 588/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.2247 - acc: 0.9258 - val_loss: 0.3951 - val_acc: 0.8843\n",
            "Epoch 589/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.2375 - acc: 0.9189 - val_loss: 0.3934 - val_acc: 0.8800\n",
            "Epoch 590/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.2370 - acc: 0.9170 - val_loss: 0.3921 - val_acc: 0.8812\n",
            "Epoch 591/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.2339 - acc: 0.9186 - val_loss: 0.4168 - val_acc: 0.8628\n",
            "Epoch 592/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.2411 - acc: 0.9216 - val_loss: 0.3685 - val_acc: 0.8892\n",
            "Epoch 593/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.2406 - acc: 0.9131 - val_loss: 0.3981 - val_acc: 0.8683\n",
            "Epoch 594/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.2246 - acc: 0.9231 - val_loss: 0.3883 - val_acc: 0.8898\n",
            "Epoch 595/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.2172 - acc: 0.9249 - val_loss: 0.3832 - val_acc: 0.8824\n",
            "Epoch 596/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.2284 - acc: 0.9195 - val_loss: 0.3995 - val_acc: 0.8830\n",
            "Epoch 597/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.2340 - acc: 0.9198 - val_loss: 0.3832 - val_acc: 0.8849\n",
            "Epoch 598/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.2236 - acc: 0.9234 - val_loss: 0.4012 - val_acc: 0.8904\n",
            "Epoch 599/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.2131 - acc: 0.9294 - val_loss: 0.3789 - val_acc: 0.8836\n",
            "Epoch 600/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.2322 - acc: 0.9176 - val_loss: 0.3925 - val_acc: 0.8757\n",
            "Epoch 601/1000\n",
            "3315/3315 [==============================] - 2s 515us/step - loss: 0.2217 - acc: 0.9240 - val_loss: 0.3846 - val_acc: 0.8806\n",
            "Epoch 602/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 0.2209 - acc: 0.9270 - val_loss: 0.3916 - val_acc: 0.8861\n",
            "Epoch 603/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.2204 - acc: 0.9264 - val_loss: 0.3813 - val_acc: 0.8843\n",
            "Epoch 604/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.2191 - acc: 0.9276 - val_loss: 0.3796 - val_acc: 0.8824\n",
            "Epoch 605/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.2295 - acc: 0.9192 - val_loss: 0.3867 - val_acc: 0.8812\n",
            "Epoch 606/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.2253 - acc: 0.9231 - val_loss: 0.3831 - val_acc: 0.8855\n",
            "Epoch 607/1000\n",
            "3315/3315 [==============================] - 2s 510us/step - loss: 0.2269 - acc: 0.9228 - val_loss: 0.3751 - val_acc: 0.8892\n",
            "Epoch 608/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.2281 - acc: 0.9219 - val_loss: 0.3955 - val_acc: 0.8794\n",
            "Epoch 609/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.2266 - acc: 0.9195 - val_loss: 0.3711 - val_acc: 0.8910\n",
            "Epoch 610/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.2273 - acc: 0.9252 - val_loss: 0.3714 - val_acc: 0.8934\n",
            "Epoch 611/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.2176 - acc: 0.9210 - val_loss: 0.4027 - val_acc: 0.8739\n",
            "Epoch 612/1000\n",
            "3315/3315 [==============================] - 2s 511us/step - loss: 0.2119 - acc: 0.9294 - val_loss: 0.4004 - val_acc: 0.8806\n",
            "Epoch 613/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.2143 - acc: 0.9291 - val_loss: 0.3855 - val_acc: 0.8910\n",
            "Epoch 614/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.2281 - acc: 0.9222 - val_loss: 0.3746 - val_acc: 0.8898\n",
            "Epoch 615/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.2303 - acc: 0.9192 - val_loss: 0.3838 - val_acc: 0.8824\n",
            "Epoch 616/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.2214 - acc: 0.9228 - val_loss: 0.3649 - val_acc: 0.8959\n",
            "Epoch 617/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.2145 - acc: 0.9222 - val_loss: 0.3665 - val_acc: 0.8910\n",
            "Epoch 618/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.2106 - acc: 0.9240 - val_loss: 0.3950 - val_acc: 0.8843\n",
            "Epoch 619/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.2278 - acc: 0.9173 - val_loss: 0.3824 - val_acc: 0.8910\n",
            "Epoch 620/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.2295 - acc: 0.9189 - val_loss: 0.3825 - val_acc: 0.8861\n",
            "Epoch 621/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.2263 - acc: 0.9231 - val_loss: 0.3863 - val_acc: 0.8849\n",
            "Epoch 622/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.2315 - acc: 0.9204 - val_loss: 0.3814 - val_acc: 0.8818\n",
            "Epoch 623/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.2207 - acc: 0.9210 - val_loss: 0.3815 - val_acc: 0.8873\n",
            "Epoch 624/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.2124 - acc: 0.9351 - val_loss: 0.3593 - val_acc: 0.8922\n",
            "Epoch 625/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.2116 - acc: 0.9237 - val_loss: 0.3837 - val_acc: 0.8867\n",
            "Epoch 626/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.2105 - acc: 0.9264 - val_loss: 0.3785 - val_acc: 0.8830\n",
            "Epoch 627/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.2232 - acc: 0.9264 - val_loss: 0.3675 - val_acc: 0.8910\n",
            "Epoch 628/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.2132 - acc: 0.9243 - val_loss: 0.3767 - val_acc: 0.8898\n",
            "Epoch 629/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.2199 - acc: 0.9282 - val_loss: 0.3738 - val_acc: 0.8947\n",
            "Epoch 630/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.2124 - acc: 0.9267 - val_loss: 0.3620 - val_acc: 0.8934\n",
            "Epoch 631/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 0.2249 - acc: 0.9228 - val_loss: 0.3641 - val_acc: 0.8965\n",
            "Epoch 632/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.2116 - acc: 0.9264 - val_loss: 0.3950 - val_acc: 0.8836\n",
            "Epoch 633/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.2092 - acc: 0.9315 - val_loss: 0.3766 - val_acc: 0.8867\n",
            "Epoch 634/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.2201 - acc: 0.9321 - val_loss: 0.3717 - val_acc: 0.8910\n",
            "Epoch 635/1000\n",
            "3315/3315 [==============================] - 2s 485us/step - loss: 0.2205 - acc: 0.9228 - val_loss: 0.3866 - val_acc: 0.8898\n",
            "Epoch 636/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.1986 - acc: 0.9297 - val_loss: 0.3725 - val_acc: 0.8910\n",
            "Epoch 637/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.2146 - acc: 0.9264 - val_loss: 0.3848 - val_acc: 0.8818\n",
            "Epoch 638/1000\n",
            "3315/3315 [==============================] - 2s 520us/step - loss: 0.2129 - acc: 0.9312 - val_loss: 0.3971 - val_acc: 0.8788\n",
            "Epoch 639/1000\n",
            "3315/3315 [==============================] - 2s 507us/step - loss: 0.2063 - acc: 0.9276 - val_loss: 0.3693 - val_acc: 0.8898\n",
            "Epoch 640/1000\n",
            "3315/3315 [==============================] - 2s 507us/step - loss: 0.2101 - acc: 0.9306 - val_loss: 0.3674 - val_acc: 0.8916\n",
            "Epoch 641/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 0.2173 - acc: 0.9246 - val_loss: 0.3764 - val_acc: 0.8867\n",
            "Epoch 642/1000\n",
            "3315/3315 [==============================] - 2s 515us/step - loss: 0.2144 - acc: 0.9297 - val_loss: 0.3727 - val_acc: 0.8965\n",
            "Epoch 643/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.2118 - acc: 0.9282 - val_loss: 0.3812 - val_acc: 0.8934\n",
            "Epoch 644/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.2016 - acc: 0.9330 - val_loss: 0.3980 - val_acc: 0.8861\n",
            "Epoch 645/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.2049 - acc: 0.9315 - val_loss: 0.3736 - val_acc: 0.8843\n",
            "Epoch 646/1000\n",
            "3315/3315 [==============================] - 2s 508us/step - loss: 0.2034 - acc: 0.9330 - val_loss: 0.3706 - val_acc: 0.8879\n",
            "Epoch 647/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.1960 - acc: 0.9370 - val_loss: 0.3917 - val_acc: 0.8855\n",
            "Epoch 648/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.2083 - acc: 0.9246 - val_loss: 0.3734 - val_acc: 0.8928\n",
            "Epoch 649/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.2048 - acc: 0.9276 - val_loss: 0.3585 - val_acc: 0.8965\n",
            "Epoch 650/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.2188 - acc: 0.9270 - val_loss: 0.3827 - val_acc: 0.8861\n",
            "Epoch 651/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.2043 - acc: 0.9285 - val_loss: 0.3956 - val_acc: 0.8781\n",
            "Epoch 652/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.2032 - acc: 0.9294 - val_loss: 0.3791 - val_acc: 0.8910\n",
            "Epoch 653/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.2094 - acc: 0.9297 - val_loss: 0.3674 - val_acc: 0.8941\n",
            "Epoch 654/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.1972 - acc: 0.9327 - val_loss: 0.3705 - val_acc: 0.8953\n",
            "Epoch 655/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.2125 - acc: 0.9276 - val_loss: 0.3630 - val_acc: 0.8953\n",
            "Epoch 656/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 0.2028 - acc: 0.9330 - val_loss: 0.3754 - val_acc: 0.8892\n",
            "Epoch 657/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.2170 - acc: 0.9252 - val_loss: 0.3566 - val_acc: 0.8959\n",
            "Epoch 658/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.2121 - acc: 0.9327 - val_loss: 0.3719 - val_acc: 0.8959\n",
            "Epoch 659/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.2031 - acc: 0.9367 - val_loss: 0.3747 - val_acc: 0.8934\n",
            "Epoch 660/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.2038 - acc: 0.9321 - val_loss: 0.3716 - val_acc: 0.8892\n",
            "Epoch 661/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.2065 - acc: 0.9300 - val_loss: 0.3683 - val_acc: 0.8965\n",
            "Epoch 662/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.2167 - acc: 0.9264 - val_loss: 0.3739 - val_acc: 0.8971\n",
            "Epoch 663/1000\n",
            "3315/3315 [==============================] - 2s 485us/step - loss: 0.2206 - acc: 0.9291 - val_loss: 0.3670 - val_acc: 0.8971\n",
            "Epoch 664/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.2010 - acc: 0.9324 - val_loss: 0.3844 - val_acc: 0.8806\n",
            "Epoch 665/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.1946 - acc: 0.9400 - val_loss: 0.3923 - val_acc: 0.8849\n",
            "Epoch 666/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.2078 - acc: 0.9228 - val_loss: 0.3645 - val_acc: 0.8941\n",
            "Epoch 667/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.2068 - acc: 0.9288 - val_loss: 0.3747 - val_acc: 0.8947\n",
            "Epoch 668/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.2088 - acc: 0.9276 - val_loss: 0.3843 - val_acc: 0.8879\n",
            "Epoch 669/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.1994 - acc: 0.9339 - val_loss: 0.3740 - val_acc: 0.8916\n",
            "Epoch 670/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.2046 - acc: 0.9333 - val_loss: 0.3597 - val_acc: 0.8990\n",
            "Epoch 671/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.2096 - acc: 0.9249 - val_loss: 0.3815 - val_acc: 0.8922\n",
            "Epoch 672/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.1898 - acc: 0.9330 - val_loss: 0.3692 - val_acc: 0.8941\n",
            "Epoch 673/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.1992 - acc: 0.9321 - val_loss: 0.3772 - val_acc: 0.8990\n",
            "Epoch 674/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.2054 - acc: 0.9285 - val_loss: 0.3670 - val_acc: 0.9002\n",
            "Epoch 675/1000\n",
            "3315/3315 [==============================] - 2s 509us/step - loss: 0.2022 - acc: 0.9348 - val_loss: 0.3739 - val_acc: 0.8959\n",
            "Epoch 676/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.2072 - acc: 0.9285 - val_loss: 0.3747 - val_acc: 0.8965\n",
            "Epoch 677/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.1946 - acc: 0.9342 - val_loss: 0.3686 - val_acc: 0.8959\n",
            "Epoch 678/1000\n",
            "3315/3315 [==============================] - 2s 551us/step - loss: 0.2014 - acc: 0.9297 - val_loss: 0.3581 - val_acc: 0.9014\n",
            "Epoch 679/1000\n",
            "3315/3315 [==============================] - 2s 544us/step - loss: 0.2052 - acc: 0.9261 - val_loss: 0.3590 - val_acc: 0.8959\n",
            "Epoch 680/1000\n",
            "3315/3315 [==============================] - 2s 555us/step - loss: 0.2168 - acc: 0.9261 - val_loss: 0.3648 - val_acc: 0.8904\n",
            "Epoch 681/1000\n",
            "3315/3315 [==============================] - 2s 554us/step - loss: 0.1875 - acc: 0.9403 - val_loss: 0.3704 - val_acc: 0.8941\n",
            "Epoch 682/1000\n",
            "3315/3315 [==============================] - 2s 555us/step - loss: 0.1883 - acc: 0.9354 - val_loss: 0.3620 - val_acc: 0.8916\n",
            "Epoch 683/1000\n",
            "3315/3315 [==============================] - 2s 549us/step - loss: 0.1931 - acc: 0.9324 - val_loss: 0.3588 - val_acc: 0.8892\n",
            "Epoch 684/1000\n",
            "3315/3315 [==============================] - 2s 517us/step - loss: 0.1858 - acc: 0.9379 - val_loss: 0.3658 - val_acc: 0.8953\n",
            "Epoch 685/1000\n",
            "3315/3315 [==============================] - 2s 513us/step - loss: 0.2008 - acc: 0.9336 - val_loss: 0.3562 - val_acc: 0.8916\n",
            "Epoch 686/1000\n",
            "3315/3315 [==============================] - 2s 516us/step - loss: 0.1850 - acc: 0.9342 - val_loss: 0.3680 - val_acc: 0.8953\n",
            "Epoch 687/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 0.1968 - acc: 0.9306 - val_loss: 0.3618 - val_acc: 0.8934\n",
            "Epoch 688/1000\n",
            "3315/3315 [==============================] - 2s 513us/step - loss: 0.1845 - acc: 0.9388 - val_loss: 0.3647 - val_acc: 0.8843\n",
            "Epoch 689/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.2001 - acc: 0.9285 - val_loss: 0.3750 - val_acc: 0.8934\n",
            "Epoch 690/1000\n",
            "3315/3315 [==============================] - 2s 512us/step - loss: 0.1896 - acc: 0.9363 - val_loss: 0.3532 - val_acc: 0.9020\n",
            "Epoch 691/1000\n",
            "3315/3315 [==============================] - 2s 507us/step - loss: 0.1979 - acc: 0.9354 - val_loss: 0.3616 - val_acc: 0.8941\n",
            "Epoch 692/1000\n",
            "3315/3315 [==============================] - 2s 517us/step - loss: 0.1927 - acc: 0.9324 - val_loss: 0.3665 - val_acc: 0.8983\n",
            "Epoch 693/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 0.1893 - acc: 0.9351 - val_loss: 0.3836 - val_acc: 0.8898\n",
            "Epoch 694/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.1963 - acc: 0.9306 - val_loss: 0.3612 - val_acc: 0.8959\n",
            "Epoch 695/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.1899 - acc: 0.9382 - val_loss: 0.3633 - val_acc: 0.8996\n",
            "Epoch 696/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.1875 - acc: 0.9354 - val_loss: 0.3432 - val_acc: 0.8971\n",
            "Epoch 697/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 0.1991 - acc: 0.9306 - val_loss: 0.3476 - val_acc: 0.9014\n",
            "Epoch 698/1000\n",
            "3315/3315 [==============================] - 2s 508us/step - loss: 0.1889 - acc: 0.9376 - val_loss: 0.3717 - val_acc: 0.8965\n",
            "Epoch 699/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.1875 - acc: 0.9382 - val_loss: 0.3676 - val_acc: 0.8953\n",
            "Epoch 700/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.1750 - acc: 0.9412 - val_loss: 0.3564 - val_acc: 0.8953\n",
            "Epoch 701/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.1866 - acc: 0.9309 - val_loss: 0.3947 - val_acc: 0.8873\n",
            "Epoch 702/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.1877 - acc: 0.9360 - val_loss: 0.3584 - val_acc: 0.9026\n",
            "Epoch 703/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.1887 - acc: 0.9354 - val_loss: 0.3675 - val_acc: 0.8959\n",
            "Epoch 704/1000\n",
            "3315/3315 [==============================] - 2s 544us/step - loss: 0.1852 - acc: 0.9363 - val_loss: 0.3669 - val_acc: 0.8928\n",
            "Epoch 705/1000\n",
            "3315/3315 [==============================] - 2s 540us/step - loss: 0.1823 - acc: 0.9315 - val_loss: 0.3534 - val_acc: 0.9045\n",
            "Epoch 706/1000\n",
            "3315/3315 [==============================] - 2s 565us/step - loss: 0.1775 - acc: 0.9409 - val_loss: 0.3879 - val_acc: 0.8861\n",
            "Epoch 707/1000\n",
            "3315/3315 [==============================] - 2s 514us/step - loss: 0.1975 - acc: 0.9312 - val_loss: 0.3537 - val_acc: 0.8934\n",
            "Epoch 708/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.1866 - acc: 0.9339 - val_loss: 0.3583 - val_acc: 0.9051\n",
            "Epoch 709/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.1839 - acc: 0.9357 - val_loss: 0.3658 - val_acc: 0.8892\n",
            "Epoch 710/1000\n",
            "3315/3315 [==============================] - 2s 518us/step - loss: 0.1934 - acc: 0.9327 - val_loss: 0.3509 - val_acc: 0.9032\n",
            "Epoch 711/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.1735 - acc: 0.9397 - val_loss: 0.3805 - val_acc: 0.8885\n",
            "Epoch 712/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.1833 - acc: 0.9415 - val_loss: 0.3661 - val_acc: 0.9014\n",
            "Epoch 713/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.1859 - acc: 0.9385 - val_loss: 0.3843 - val_acc: 0.8953\n",
            "Epoch 714/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.1672 - acc: 0.9460 - val_loss: 0.3493 - val_acc: 0.8996\n",
            "Epoch 715/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.1874 - acc: 0.9345 - val_loss: 0.3468 - val_acc: 0.8996\n",
            "Epoch 716/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.1879 - acc: 0.9321 - val_loss: 0.3529 - val_acc: 0.8971\n",
            "Epoch 717/1000\n",
            "3315/3315 [==============================] - 2s 510us/step - loss: 0.1751 - acc: 0.9418 - val_loss: 0.3654 - val_acc: 0.8934\n",
            "Epoch 718/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.1840 - acc: 0.9363 - val_loss: 0.3566 - val_acc: 0.8971\n",
            "Epoch 719/1000\n",
            "3315/3315 [==============================] - 2s 508us/step - loss: 0.1897 - acc: 0.9394 - val_loss: 0.3634 - val_acc: 0.8959\n",
            "Epoch 720/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.1855 - acc: 0.9354 - val_loss: 0.3728 - val_acc: 0.8959\n",
            "Epoch 721/1000\n",
            "3315/3315 [==============================] - 2s 516us/step - loss: 0.1816 - acc: 0.9430 - val_loss: 0.3566 - val_acc: 0.9045\n",
            "Epoch 722/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.1751 - acc: 0.9376 - val_loss: 0.3473 - val_acc: 0.9063\n",
            "Epoch 723/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.1732 - acc: 0.9406 - val_loss: 0.3650 - val_acc: 0.8965\n",
            "Epoch 724/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.1870 - acc: 0.9379 - val_loss: 0.3522 - val_acc: 0.9032\n",
            "Epoch 725/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.2010 - acc: 0.9370 - val_loss: 0.3737 - val_acc: 0.8885\n",
            "Epoch 726/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.1753 - acc: 0.9439 - val_loss: 0.3674 - val_acc: 0.8965\n",
            "Epoch 727/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.1800 - acc: 0.9445 - val_loss: 0.3498 - val_acc: 0.9008\n",
            "Epoch 728/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.1686 - acc: 0.9460 - val_loss: 0.3595 - val_acc: 0.9026\n",
            "Epoch 729/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.1645 - acc: 0.9448 - val_loss: 0.3577 - val_acc: 0.8947\n",
            "Epoch 730/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.1988 - acc: 0.9336 - val_loss: 0.3614 - val_acc: 0.8965\n",
            "Epoch 731/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.1788 - acc: 0.9373 - val_loss: 0.3573 - val_acc: 0.8941\n",
            "Epoch 732/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.1832 - acc: 0.9409 - val_loss: 0.3522 - val_acc: 0.8953\n",
            "Epoch 733/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.1827 - acc: 0.9397 - val_loss: 0.3612 - val_acc: 0.9020\n",
            "Epoch 734/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.1872 - acc: 0.9376 - val_loss: 0.3590 - val_acc: 0.8996\n",
            "Epoch 735/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.1819 - acc: 0.9373 - val_loss: 0.3616 - val_acc: 0.8983\n",
            "Epoch 736/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.1860 - acc: 0.9357 - val_loss: 0.3400 - val_acc: 0.9063\n",
            "Epoch 737/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.1781 - acc: 0.9427 - val_loss: 0.3508 - val_acc: 0.9020\n",
            "Epoch 738/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.1666 - acc: 0.9439 - val_loss: 0.3573 - val_acc: 0.8983\n",
            "Epoch 739/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.1799 - acc: 0.9379 - val_loss: 0.3576 - val_acc: 0.8977\n",
            "Epoch 740/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.1804 - acc: 0.9333 - val_loss: 0.3403 - val_acc: 0.9063\n",
            "Epoch 741/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.1769 - acc: 0.9385 - val_loss: 0.3566 - val_acc: 0.9026\n",
            "Epoch 742/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.1688 - acc: 0.9445 - val_loss: 0.3695 - val_acc: 0.8953\n",
            "Epoch 743/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.1780 - acc: 0.9403 - val_loss: 0.3774 - val_acc: 0.8898\n",
            "Epoch 744/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.1819 - acc: 0.9315 - val_loss: 0.3439 - val_acc: 0.9057\n",
            "Epoch 745/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 0.1788 - acc: 0.9363 - val_loss: 0.3635 - val_acc: 0.8983\n",
            "Epoch 746/1000\n",
            "3315/3315 [==============================] - 2s 511us/step - loss: 0.1764 - acc: 0.9367 - val_loss: 0.3530 - val_acc: 0.9057\n",
            "Epoch 747/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.1687 - acc: 0.9472 - val_loss: 0.3494 - val_acc: 0.9088\n",
            "Epoch 748/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.1817 - acc: 0.9388 - val_loss: 0.3845 - val_acc: 0.8892\n",
            "Epoch 749/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.1781 - acc: 0.9415 - val_loss: 0.4049 - val_acc: 0.8739\n",
            "Epoch 750/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.1700 - acc: 0.9409 - val_loss: 0.3678 - val_acc: 0.8983\n",
            "Epoch 751/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.1712 - acc: 0.9388 - val_loss: 0.3597 - val_acc: 0.9008\n",
            "Epoch 752/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.1799 - acc: 0.9342 - val_loss: 0.3812 - val_acc: 0.8934\n",
            "Epoch 753/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.1853 - acc: 0.9409 - val_loss: 0.3513 - val_acc: 0.9002\n",
            "Epoch 754/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.1816 - acc: 0.9403 - val_loss: 0.3437 - val_acc: 0.8965\n",
            "Epoch 755/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.1758 - acc: 0.9376 - val_loss: 0.3471 - val_acc: 0.9014\n",
            "Epoch 756/1000\n",
            "3315/3315 [==============================] - 2s 508us/step - loss: 0.1722 - acc: 0.9418 - val_loss: 0.3412 - val_acc: 0.9057\n",
            "Epoch 757/1000\n",
            "3315/3315 [==============================] - 2s 511us/step - loss: 0.1702 - acc: 0.9412 - val_loss: 0.3680 - val_acc: 0.8983\n",
            "Epoch 758/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.1676 - acc: 0.9418 - val_loss: 0.3696 - val_acc: 0.8922\n",
            "Epoch 759/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.1657 - acc: 0.9463 - val_loss: 0.3402 - val_acc: 0.9100\n",
            "Epoch 760/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.1700 - acc: 0.9433 - val_loss: 0.3491 - val_acc: 0.9014\n",
            "Epoch 761/1000\n",
            "3315/3315 [==============================] - 2s 510us/step - loss: 0.1652 - acc: 0.9409 - val_loss: 0.3730 - val_acc: 0.8977\n",
            "Epoch 762/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.1766 - acc: 0.9397 - val_loss: 0.3379 - val_acc: 0.9075\n",
            "Epoch 763/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.1640 - acc: 0.9490 - val_loss: 0.3637 - val_acc: 0.8879\n",
            "Epoch 764/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.1833 - acc: 0.9354 - val_loss: 0.3519 - val_acc: 0.8928\n",
            "Epoch 765/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.1684 - acc: 0.9466 - val_loss: 0.3535 - val_acc: 0.9008\n",
            "Epoch 766/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.1682 - acc: 0.9427 - val_loss: 0.3641 - val_acc: 0.8996\n",
            "Epoch 767/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.1787 - acc: 0.9424 - val_loss: 0.3396 - val_acc: 0.9081\n",
            "Epoch 768/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.1677 - acc: 0.9460 - val_loss: 0.3517 - val_acc: 0.9045\n",
            "Epoch 769/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.1750 - acc: 0.9388 - val_loss: 0.3558 - val_acc: 0.8977\n",
            "Epoch 770/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.1713 - acc: 0.9439 - val_loss: 0.3547 - val_acc: 0.9014\n",
            "Epoch 771/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.1638 - acc: 0.9457 - val_loss: 0.3610 - val_acc: 0.9069\n",
            "Epoch 772/1000\n",
            "3315/3315 [==============================] - 2s 485us/step - loss: 0.1699 - acc: 0.9403 - val_loss: 0.3509 - val_acc: 0.9026\n",
            "Epoch 773/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.1753 - acc: 0.9406 - val_loss: 0.3633 - val_acc: 0.9014\n",
            "Epoch 774/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.1702 - acc: 0.9454 - val_loss: 0.3597 - val_acc: 0.9032\n",
            "Epoch 775/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.1543 - acc: 0.9499 - val_loss: 0.3668 - val_acc: 0.9002\n",
            "Epoch 776/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.1649 - acc: 0.9412 - val_loss: 0.3545 - val_acc: 0.8977\n",
            "Epoch 777/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.1540 - acc: 0.9502 - val_loss: 0.3515 - val_acc: 0.9088\n",
            "Epoch 778/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.1607 - acc: 0.9472 - val_loss: 0.3423 - val_acc: 0.9088\n",
            "Epoch 779/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.1536 - acc: 0.9466 - val_loss: 0.3564 - val_acc: 0.9057\n",
            "Epoch 780/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.1588 - acc: 0.9466 - val_loss: 0.3571 - val_acc: 0.9008\n",
            "Epoch 781/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.1499 - acc: 0.9487 - val_loss: 0.3478 - val_acc: 0.9002\n",
            "Epoch 782/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.1646 - acc: 0.9481 - val_loss: 0.3522 - val_acc: 0.9094\n",
            "Epoch 783/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.1564 - acc: 0.9451 - val_loss: 0.3509 - val_acc: 0.9026\n",
            "Epoch 784/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.1668 - acc: 0.9394 - val_loss: 0.3539 - val_acc: 0.9039\n",
            "Epoch 785/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.1655 - acc: 0.9442 - val_loss: 0.3398 - val_acc: 0.9051\n",
            "Epoch 786/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.1631 - acc: 0.9451 - val_loss: 0.3674 - val_acc: 0.8996\n",
            "Epoch 787/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.1558 - acc: 0.9463 - val_loss: 0.3420 - val_acc: 0.9039\n",
            "Epoch 788/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.1657 - acc: 0.9460 - val_loss: 0.3463 - val_acc: 0.9045\n",
            "Epoch 789/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.1487 - acc: 0.9478 - val_loss: 0.3680 - val_acc: 0.8977\n",
            "Epoch 790/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.1582 - acc: 0.9472 - val_loss: 0.3441 - val_acc: 0.9100\n",
            "Epoch 791/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.1678 - acc: 0.9436 - val_loss: 0.3374 - val_acc: 0.9088\n",
            "Epoch 792/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.1623 - acc: 0.9427 - val_loss: 0.3401 - val_acc: 0.9051\n",
            "Epoch 793/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.1557 - acc: 0.9469 - val_loss: 0.3468 - val_acc: 0.9026\n",
            "Epoch 794/1000\n",
            "3315/3315 [==============================] - 2s 517us/step - loss: 0.1618 - acc: 0.9478 - val_loss: 0.3476 - val_acc: 0.9088\n",
            "Epoch 795/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.1649 - acc: 0.9418 - val_loss: 0.3631 - val_acc: 0.8965\n",
            "Epoch 796/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.1689 - acc: 0.9457 - val_loss: 0.3569 - val_acc: 0.9026\n",
            "Epoch 797/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.1675 - acc: 0.9421 - val_loss: 0.3532 - val_acc: 0.9075\n",
            "Epoch 798/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.1574 - acc: 0.9469 - val_loss: 0.3626 - val_acc: 0.9039\n",
            "Epoch 799/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.1641 - acc: 0.9466 - val_loss: 0.3469 - val_acc: 0.9063\n",
            "Epoch 800/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.1605 - acc: 0.9451 - val_loss: 0.3479 - val_acc: 0.9032\n",
            "Epoch 801/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.1694 - acc: 0.9397 - val_loss: 0.3382 - val_acc: 0.9143\n",
            "Epoch 802/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.1557 - acc: 0.9439 - val_loss: 0.3466 - val_acc: 0.9039\n",
            "Epoch 803/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.1564 - acc: 0.9469 - val_loss: 0.3464 - val_acc: 0.9026\n",
            "Epoch 804/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.1552 - acc: 0.9487 - val_loss: 0.3408 - val_acc: 0.9137\n",
            "Epoch 805/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.1575 - acc: 0.9472 - val_loss: 0.3463 - val_acc: 0.9039\n",
            "Epoch 806/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.1500 - acc: 0.9472 - val_loss: 0.3513 - val_acc: 0.8996\n",
            "Epoch 807/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.1565 - acc: 0.9487 - val_loss: 0.3432 - val_acc: 0.9039\n",
            "Epoch 808/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.1666 - acc: 0.9439 - val_loss: 0.3544 - val_acc: 0.9081\n",
            "Epoch 809/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.1649 - acc: 0.9433 - val_loss: 0.3406 - val_acc: 0.9057\n",
            "Epoch 810/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.1525 - acc: 0.9508 - val_loss: 0.3397 - val_acc: 0.9057\n",
            "Epoch 811/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.1554 - acc: 0.9475 - val_loss: 0.3549 - val_acc: 0.9020\n",
            "Epoch 812/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.1570 - acc: 0.9430 - val_loss: 0.3532 - val_acc: 0.8990\n",
            "Epoch 813/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.1626 - acc: 0.9463 - val_loss: 0.3418 - val_acc: 0.8971\n",
            "Epoch 814/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.1499 - acc: 0.9472 - val_loss: 0.3491 - val_acc: 0.9124\n",
            "Epoch 815/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.1588 - acc: 0.9445 - val_loss: 0.3617 - val_acc: 0.8910\n",
            "Epoch 816/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.1548 - acc: 0.9460 - val_loss: 0.3648 - val_acc: 0.9020\n",
            "Epoch 817/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.1557 - acc: 0.9484 - val_loss: 0.3507 - val_acc: 0.9057\n",
            "Epoch 818/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.1692 - acc: 0.9460 - val_loss: 0.3548 - val_acc: 0.9008\n",
            "Epoch 819/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.1550 - acc: 0.9457 - val_loss: 0.3353 - val_acc: 0.9094\n",
            "Epoch 820/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.1623 - acc: 0.9436 - val_loss: 0.3215 - val_acc: 0.9106\n",
            "Epoch 821/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.1619 - acc: 0.9457 - val_loss: 0.3397 - val_acc: 0.9039\n",
            "Epoch 822/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.1580 - acc: 0.9487 - val_loss: 0.3395 - val_acc: 0.9002\n",
            "Epoch 823/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.1553 - acc: 0.9481 - val_loss: 0.3351 - val_acc: 0.9100\n",
            "Epoch 824/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.1666 - acc: 0.9418 - val_loss: 0.3527 - val_acc: 0.9020\n",
            "Epoch 825/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.1463 - acc: 0.9508 - val_loss: 0.3436 - val_acc: 0.9002\n",
            "Epoch 826/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.1512 - acc: 0.9529 - val_loss: 0.3285 - val_acc: 0.9075\n",
            "Epoch 827/1000\n",
            "3315/3315 [==============================] - 2s 515us/step - loss: 0.1554 - acc: 0.9466 - val_loss: 0.3500 - val_acc: 0.9051\n",
            "Epoch 828/1000\n",
            "3315/3315 [==============================] - 2s 515us/step - loss: 0.1546 - acc: 0.9478 - val_loss: 0.3258 - val_acc: 0.9149\n",
            "Epoch 829/1000\n",
            "3315/3315 [==============================] - 2s 514us/step - loss: 0.1430 - acc: 0.9517 - val_loss: 0.3411 - val_acc: 0.9057\n",
            "Epoch 830/1000\n",
            "3315/3315 [==============================] - 2s 521us/step - loss: 0.1572 - acc: 0.9484 - val_loss: 0.3438 - val_acc: 0.9063\n",
            "Epoch 831/1000\n",
            "3315/3315 [==============================] - 2s 510us/step - loss: 0.1442 - acc: 0.9460 - val_loss: 0.3626 - val_acc: 0.8971\n",
            "Epoch 832/1000\n",
            "3315/3315 [==============================] - 2s 510us/step - loss: 0.1470 - acc: 0.9551 - val_loss: 0.3451 - val_acc: 0.9039\n",
            "Epoch 833/1000\n",
            "3315/3315 [==============================] - 2s 508us/step - loss: 0.1564 - acc: 0.9454 - val_loss: 0.3451 - val_acc: 0.9032\n",
            "Epoch 834/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.1496 - acc: 0.9499 - val_loss: 0.3512 - val_acc: 0.9014\n",
            "Epoch 835/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.1593 - acc: 0.9445 - val_loss: 0.3572 - val_acc: 0.9014\n",
            "Epoch 836/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.1462 - acc: 0.9526 - val_loss: 0.3674 - val_acc: 0.9081\n",
            "Epoch 837/1000\n",
            "3315/3315 [==============================] - 2s 507us/step - loss: 0.1634 - acc: 0.9478 - val_loss: 0.3622 - val_acc: 0.9032\n",
            "Epoch 838/1000\n",
            "3315/3315 [==============================] - 2s 511us/step - loss: 0.1587 - acc: 0.9469 - val_loss: 0.3459 - val_acc: 0.9032\n",
            "Epoch 839/1000\n",
            "3315/3315 [==============================] - 2s 508us/step - loss: 0.1521 - acc: 0.9484 - val_loss: 0.3400 - val_acc: 0.9069\n",
            "Epoch 840/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.1369 - acc: 0.9508 - val_loss: 0.3319 - val_acc: 0.9106\n",
            "Epoch 841/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.1451 - acc: 0.9514 - val_loss: 0.3321 - val_acc: 0.9063\n",
            "Epoch 842/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.1524 - acc: 0.9511 - val_loss: 0.3492 - val_acc: 0.9063\n",
            "Epoch 843/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.1387 - acc: 0.9490 - val_loss: 0.3380 - val_acc: 0.9143\n",
            "Epoch 844/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.1436 - acc: 0.9523 - val_loss: 0.3515 - val_acc: 0.9026\n",
            "Epoch 845/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 0.1530 - acc: 0.9529 - val_loss: 0.3410 - val_acc: 0.9088\n",
            "Epoch 846/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.1501 - acc: 0.9481 - val_loss: 0.3580 - val_acc: 0.9014\n",
            "Epoch 847/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.1360 - acc: 0.9551 - val_loss: 0.3317 - val_acc: 0.9088\n",
            "Epoch 848/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.1466 - acc: 0.9490 - val_loss: 0.3565 - val_acc: 0.9051\n",
            "Epoch 849/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.1470 - acc: 0.9514 - val_loss: 0.3536 - val_acc: 0.9057\n",
            "Epoch 850/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 0.1464 - acc: 0.9511 - val_loss: 0.3574 - val_acc: 0.8990\n",
            "Epoch 851/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.1537 - acc: 0.9454 - val_loss: 0.3461 - val_acc: 0.8965\n",
            "Epoch 852/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.1554 - acc: 0.9454 - val_loss: 0.3541 - val_acc: 0.9112\n",
            "Epoch 853/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 0.1417 - acc: 0.9544 - val_loss: 0.3671 - val_acc: 0.9051\n",
            "Epoch 854/1000\n",
            "3315/3315 [==============================] - 2s 514us/step - loss: 0.1428 - acc: 0.9505 - val_loss: 0.3541 - val_acc: 0.9081\n",
            "Epoch 855/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.1521 - acc: 0.9460 - val_loss: 0.3444 - val_acc: 0.9063\n",
            "Epoch 856/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.1559 - acc: 0.9484 - val_loss: 0.3688 - val_acc: 0.9088\n",
            "Epoch 857/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.1601 - acc: 0.9493 - val_loss: 0.3523 - val_acc: 0.9124\n",
            "Epoch 858/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.1514 - acc: 0.9490 - val_loss: 0.3297 - val_acc: 0.9143\n",
            "Epoch 859/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.1544 - acc: 0.9433 - val_loss: 0.3282 - val_acc: 0.9173\n",
            "Epoch 860/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.1460 - acc: 0.9502 - val_loss: 0.3524 - val_acc: 0.9088\n",
            "Epoch 861/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.1356 - acc: 0.9514 - val_loss: 0.3565 - val_acc: 0.9057\n",
            "Epoch 862/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.1503 - acc: 0.9505 - val_loss: 0.3472 - val_acc: 0.9051\n",
            "Epoch 863/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.1441 - acc: 0.9514 - val_loss: 0.3462 - val_acc: 0.9118\n",
            "Epoch 864/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.1633 - acc: 0.9445 - val_loss: 0.3486 - val_acc: 0.9032\n",
            "Epoch 865/1000\n",
            "3315/3315 [==============================] - 2s 555us/step - loss: 0.1476 - acc: 0.9481 - val_loss: 0.3323 - val_acc: 0.9124\n",
            "Epoch 866/1000\n",
            "3315/3315 [==============================] - 2s 573us/step - loss: 0.1422 - acc: 0.9529 - val_loss: 0.3445 - val_acc: 0.9063\n",
            "Epoch 867/1000\n",
            "3315/3315 [==============================] - 2s 578us/step - loss: 0.1481 - acc: 0.9481 - val_loss: 0.3487 - val_acc: 0.9051\n",
            "Epoch 868/1000\n",
            "3315/3315 [==============================] - 2s 556us/step - loss: 0.1542 - acc: 0.9457 - val_loss: 0.3551 - val_acc: 0.9069\n",
            "Epoch 869/1000\n",
            "3315/3315 [==============================] - 2s 546us/step - loss: 0.1513 - acc: 0.9478 - val_loss: 0.3690 - val_acc: 0.9020\n",
            "Epoch 870/1000\n",
            "3315/3315 [==============================] - 2s 541us/step - loss: 0.1510 - acc: 0.9514 - val_loss: 0.3593 - val_acc: 0.8983\n",
            "Epoch 871/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.1458 - acc: 0.9484 - val_loss: 0.3427 - val_acc: 0.9143\n",
            "Epoch 872/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.1586 - acc: 0.9511 - val_loss: 0.3636 - val_acc: 0.8990\n",
            "Epoch 873/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.1335 - acc: 0.9535 - val_loss: 0.3478 - val_acc: 0.9143\n",
            "Epoch 874/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.1411 - acc: 0.9517 - val_loss: 0.3539 - val_acc: 0.9002\n",
            "Epoch 875/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.1475 - acc: 0.9469 - val_loss: 0.3411 - val_acc: 0.9106\n",
            "Epoch 876/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.1521 - acc: 0.9487 - val_loss: 0.3455 - val_acc: 0.9014\n",
            "Epoch 877/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.1300 - acc: 0.9532 - val_loss: 0.3488 - val_acc: 0.9137\n",
            "Epoch 878/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.1445 - acc: 0.9499 - val_loss: 0.3429 - val_acc: 0.9081\n",
            "Epoch 879/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.1407 - acc: 0.9535 - val_loss: 0.3572 - val_acc: 0.9032\n",
            "Epoch 880/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.1588 - acc: 0.9493 - val_loss: 0.3371 - val_acc: 0.9088\n",
            "Epoch 881/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.1377 - acc: 0.9532 - val_loss: 0.3474 - val_acc: 0.9118\n",
            "Epoch 882/1000\n",
            "3315/3315 [==============================] - 2s 485us/step - loss: 0.1446 - acc: 0.9514 - val_loss: 0.3351 - val_acc: 0.9112\n",
            "Epoch 883/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.1422 - acc: 0.9523 - val_loss: 0.3465 - val_acc: 0.9118\n",
            "Epoch 884/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.1493 - acc: 0.9508 - val_loss: 0.3407 - val_acc: 0.9094\n",
            "Epoch 885/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.1442 - acc: 0.9511 - val_loss: 0.3575 - val_acc: 0.9026\n",
            "Epoch 886/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.1502 - acc: 0.9484 - val_loss: 0.3405 - val_acc: 0.9161\n",
            "Epoch 887/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.1507 - acc: 0.9484 - val_loss: 0.3415 - val_acc: 0.9069\n",
            "Epoch 888/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.1494 - acc: 0.9475 - val_loss: 0.3639 - val_acc: 0.9069\n",
            "Epoch 889/1000\n",
            "3315/3315 [==============================] - 2s 553us/step - loss: 0.1419 - acc: 0.9548 - val_loss: 0.3383 - val_acc: 0.9130\n",
            "Epoch 890/1000\n",
            "3315/3315 [==============================] - 2s 555us/step - loss: 0.1488 - acc: 0.9523 - val_loss: 0.3465 - val_acc: 0.9137\n",
            "Epoch 891/1000\n",
            "3315/3315 [==============================] - 2s 558us/step - loss: 0.1441 - acc: 0.9517 - val_loss: 0.3344 - val_acc: 0.9137\n",
            "Epoch 892/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.1294 - acc: 0.9557 - val_loss: 0.3742 - val_acc: 0.8971\n",
            "Epoch 893/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.1354 - acc: 0.9575 - val_loss: 0.3477 - val_acc: 0.9112\n",
            "Epoch 894/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.1348 - acc: 0.9514 - val_loss: 0.3514 - val_acc: 0.9088\n",
            "Epoch 895/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.1464 - acc: 0.9478 - val_loss: 0.3395 - val_acc: 0.9100\n",
            "Epoch 896/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.1461 - acc: 0.9514 - val_loss: 0.3417 - val_acc: 0.9149\n",
            "Epoch 897/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.1369 - acc: 0.9511 - val_loss: 0.3380 - val_acc: 0.9179\n",
            "Epoch 898/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.1281 - acc: 0.9578 - val_loss: 0.3311 - val_acc: 0.9155\n",
            "Epoch 899/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.1318 - acc: 0.9538 - val_loss: 0.3264 - val_acc: 0.9124\n",
            "Epoch 900/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.1407 - acc: 0.9505 - val_loss: 0.3554 - val_acc: 0.9112\n",
            "Epoch 901/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.1357 - acc: 0.9557 - val_loss: 0.3428 - val_acc: 0.9155\n",
            "Epoch 902/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.1433 - acc: 0.9532 - val_loss: 0.3302 - val_acc: 0.9051\n",
            "Epoch 903/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.1269 - acc: 0.9584 - val_loss: 0.3487 - val_acc: 0.9100\n",
            "Epoch 904/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.1325 - acc: 0.9548 - val_loss: 0.3307 - val_acc: 0.9210\n",
            "Epoch 905/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.1370 - acc: 0.9538 - val_loss: 0.3329 - val_acc: 0.9106\n",
            "Epoch 906/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.1449 - acc: 0.9526 - val_loss: 0.3610 - val_acc: 0.9032\n",
            "Epoch 907/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.1383 - acc: 0.9563 - val_loss: 0.3436 - val_acc: 0.9155\n",
            "Epoch 908/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.1475 - acc: 0.9493 - val_loss: 0.3598 - val_acc: 0.9057\n",
            "Epoch 909/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.1463 - acc: 0.9548 - val_loss: 0.3599 - val_acc: 0.9026\n",
            "Epoch 910/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.1367 - acc: 0.9590 - val_loss: 0.3335 - val_acc: 0.9204\n",
            "Epoch 911/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 0.1371 - acc: 0.9514 - val_loss: 0.3813 - val_acc: 0.8990\n",
            "Epoch 912/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.1304 - acc: 0.9572 - val_loss: 0.3550 - val_acc: 0.9057\n",
            "Epoch 913/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.1438 - acc: 0.9484 - val_loss: 0.3333 - val_acc: 0.9100\n",
            "Epoch 914/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.1435 - acc: 0.9538 - val_loss: 0.3427 - val_acc: 0.9112\n",
            "Epoch 915/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.1305 - acc: 0.9575 - val_loss: 0.3409 - val_acc: 0.9081\n",
            "Epoch 916/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.1305 - acc: 0.9526 - val_loss: 0.3469 - val_acc: 0.9045\n",
            "Epoch 917/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.1383 - acc: 0.9554 - val_loss: 0.3419 - val_acc: 0.9063\n",
            "Epoch 918/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 0.1212 - acc: 0.9596 - val_loss: 0.3474 - val_acc: 0.9118\n",
            "Epoch 919/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.1336 - acc: 0.9544 - val_loss: 0.3430 - val_acc: 0.9143\n",
            "Epoch 920/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.1411 - acc: 0.9532 - val_loss: 0.3539 - val_acc: 0.9051\n",
            "Epoch 921/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.1282 - acc: 0.9587 - val_loss: 0.3582 - val_acc: 0.9118\n",
            "Epoch 922/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.1433 - acc: 0.9520 - val_loss: 0.3434 - val_acc: 0.9094\n",
            "Epoch 923/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.1337 - acc: 0.9505 - val_loss: 0.3461 - val_acc: 0.9045\n",
            "Epoch 924/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.1369 - acc: 0.9544 - val_loss: 0.3359 - val_acc: 0.9143\n",
            "Epoch 925/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.1460 - acc: 0.9487 - val_loss: 0.3327 - val_acc: 0.9088\n",
            "Epoch 926/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.1325 - acc: 0.9569 - val_loss: 0.3493 - val_acc: 0.9130\n",
            "Epoch 927/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.1277 - acc: 0.9584 - val_loss: 0.3492 - val_acc: 0.9063\n",
            "Epoch 928/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.1135 - acc: 0.9638 - val_loss: 0.3515 - val_acc: 0.9032\n",
            "Epoch 929/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 0.1607 - acc: 0.9472 - val_loss: 0.3469 - val_acc: 0.9124\n",
            "Epoch 930/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.1344 - acc: 0.9508 - val_loss: 0.3502 - val_acc: 0.9088\n",
            "Epoch 931/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.1376 - acc: 0.9554 - val_loss: 0.3561 - val_acc: 0.9106\n",
            "Epoch 932/1000\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.1436 - acc: 0.9541 - val_loss: 0.3531 - val_acc: 0.9124\n",
            "Epoch 933/1000\n",
            "3315/3315 [==============================] - 2s 511us/step - loss: 0.1355 - acc: 0.9557 - val_loss: 0.3349 - val_acc: 0.9167\n",
            "Epoch 934/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.1328 - acc: 0.9541 - val_loss: 0.3613 - val_acc: 0.9081\n",
            "Epoch 935/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.1371 - acc: 0.9575 - val_loss: 0.3719 - val_acc: 0.9020\n",
            "Epoch 936/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.1393 - acc: 0.9541 - val_loss: 0.3540 - val_acc: 0.9026\n",
            "Epoch 937/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.1370 - acc: 0.9554 - val_loss: 0.3505 - val_acc: 0.9137\n",
            "Epoch 938/1000\n",
            "3315/3315 [==============================] - 2s 508us/step - loss: 0.1260 - acc: 0.9608 - val_loss: 0.3567 - val_acc: 0.9100\n",
            "Epoch 939/1000\n",
            "3315/3315 [==============================] - 2s 507us/step - loss: 0.1389 - acc: 0.9535 - val_loss: 0.3747 - val_acc: 0.8990\n",
            "Epoch 940/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.1316 - acc: 0.9529 - val_loss: 0.3557 - val_acc: 0.9075\n",
            "Epoch 941/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.1354 - acc: 0.9548 - val_loss: 0.3593 - val_acc: 0.9081\n",
            "Epoch 942/1000\n",
            "3315/3315 [==============================] - 2s 485us/step - loss: 0.1337 - acc: 0.9566 - val_loss: 0.3536 - val_acc: 0.9075\n",
            "Epoch 943/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.1324 - acc: 0.9551 - val_loss: 0.3556 - val_acc: 0.9106\n",
            "Epoch 944/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.1375 - acc: 0.9557 - val_loss: 0.3501 - val_acc: 0.9137\n",
            "Epoch 945/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.1329 - acc: 0.9569 - val_loss: 0.3614 - val_acc: 0.9081\n",
            "Epoch 946/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.1325 - acc: 0.9548 - val_loss: 0.3388 - val_acc: 0.9130\n",
            "Epoch 947/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.1274 - acc: 0.9532 - val_loss: 0.3624 - val_acc: 0.9051\n",
            "Epoch 948/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.1443 - acc: 0.9538 - val_loss: 0.3560 - val_acc: 0.9094\n",
            "Epoch 949/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.1249 - acc: 0.9557 - val_loss: 0.3550 - val_acc: 0.9112\n",
            "Epoch 950/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.1354 - acc: 0.9535 - val_loss: 0.3380 - val_acc: 0.9094\n",
            "Epoch 951/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.1279 - acc: 0.9584 - val_loss: 0.3417 - val_acc: 0.9081\n",
            "Epoch 952/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.1261 - acc: 0.9572 - val_loss: 0.3498 - val_acc: 0.9149\n",
            "Epoch 953/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.1233 - acc: 0.9581 - val_loss: 0.3472 - val_acc: 0.9094\n",
            "Epoch 954/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 0.1350 - acc: 0.9517 - val_loss: 0.3238 - val_acc: 0.9253\n",
            "Epoch 955/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.1318 - acc: 0.9532 - val_loss: 0.3448 - val_acc: 0.9063\n",
            "Epoch 956/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.1330 - acc: 0.9563 - val_loss: 0.3197 - val_acc: 0.9222\n",
            "Epoch 957/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.1430 - acc: 0.9490 - val_loss: 0.3373 - val_acc: 0.9179\n",
            "Epoch 958/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.1230 - acc: 0.9596 - val_loss: 0.3369 - val_acc: 0.9161\n",
            "Epoch 959/1000\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.1228 - acc: 0.9581 - val_loss: 0.3437 - val_acc: 0.9149\n",
            "Epoch 960/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.1195 - acc: 0.9593 - val_loss: 0.3376 - val_acc: 0.9063\n",
            "Epoch 961/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.1372 - acc: 0.9520 - val_loss: 0.3401 - val_acc: 0.9124\n",
            "Epoch 962/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.1293 - acc: 0.9529 - val_loss: 0.3362 - val_acc: 0.9161\n",
            "Epoch 963/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.1312 - acc: 0.9538 - val_loss: 0.3455 - val_acc: 0.9081\n",
            "Epoch 964/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.1313 - acc: 0.9560 - val_loss: 0.3454 - val_acc: 0.9173\n",
            "Epoch 965/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.1125 - acc: 0.9638 - val_loss: 0.3739 - val_acc: 0.9075\n",
            "Epoch 966/1000\n",
            "3315/3315 [==============================] - 2s 511us/step - loss: 0.1400 - acc: 0.9541 - val_loss: 0.3535 - val_acc: 0.9039\n",
            "Epoch 967/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.1435 - acc: 0.9520 - val_loss: 0.3562 - val_acc: 0.9112\n",
            "Epoch 968/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.1278 - acc: 0.9538 - val_loss: 0.3371 - val_acc: 0.9161\n",
            "Epoch 969/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.1276 - acc: 0.9581 - val_loss: 0.3441 - val_acc: 0.9149\n",
            "Epoch 970/1000\n",
            "3315/3315 [==============================] - 2s 511us/step - loss: 0.1295 - acc: 0.9544 - val_loss: 0.3533 - val_acc: 0.9100\n",
            "Epoch 971/1000\n",
            "3315/3315 [==============================] - 2s 507us/step - loss: 0.1360 - acc: 0.9563 - val_loss: 0.3466 - val_acc: 0.9100\n",
            "Epoch 972/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 0.1192 - acc: 0.9605 - val_loss: 0.3340 - val_acc: 0.9137\n",
            "Epoch 973/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.1375 - acc: 0.9517 - val_loss: 0.3668 - val_acc: 0.9057\n",
            "Epoch 974/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.1252 - acc: 0.9581 - val_loss: 0.3401 - val_acc: 0.9161\n",
            "Epoch 975/1000\n",
            "3315/3315 [==============================] - 2s 515us/step - loss: 0.1291 - acc: 0.9563 - val_loss: 0.3515 - val_acc: 0.9094\n",
            "Epoch 976/1000\n",
            "3315/3315 [==============================] - 2s 505us/step - loss: 0.1275 - acc: 0.9572 - val_loss: 0.3433 - val_acc: 0.9118\n",
            "Epoch 977/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.1291 - acc: 0.9532 - val_loss: 0.3318 - val_acc: 0.9179\n",
            "Epoch 978/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.1252 - acc: 0.9563 - val_loss: 0.3447 - val_acc: 0.9112\n",
            "Epoch 979/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.1369 - acc: 0.9538 - val_loss: 0.3229 - val_acc: 0.9173\n",
            "Epoch 980/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.1259 - acc: 0.9557 - val_loss: 0.3470 - val_acc: 0.9106\n",
            "Epoch 981/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.1309 - acc: 0.9566 - val_loss: 0.3599 - val_acc: 0.9094\n",
            "Epoch 982/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.1079 - acc: 0.9656 - val_loss: 0.3396 - val_acc: 0.9124\n",
            "Epoch 983/1000\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.1217 - acc: 0.9578 - val_loss: 0.3666 - val_acc: 0.9063\n",
            "Epoch 984/1000\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.1374 - acc: 0.9496 - val_loss: 0.3488 - val_acc: 0.9137\n",
            "Epoch 985/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.1334 - acc: 0.9554 - val_loss: 0.3592 - val_acc: 0.9118\n",
            "Epoch 986/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.1255 - acc: 0.9566 - val_loss: 0.3366 - val_acc: 0.9088\n",
            "Epoch 987/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.1155 - acc: 0.9590 - val_loss: 0.3608 - val_acc: 0.9069\n",
            "Epoch 988/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.1342 - acc: 0.9554 - val_loss: 0.3469 - val_acc: 0.9100\n",
            "Epoch 989/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.1304 - acc: 0.9538 - val_loss: 0.3348 - val_acc: 0.9179\n",
            "Epoch 990/1000\n",
            "3315/3315 [==============================] - 2s 494us/step - loss: 0.1107 - acc: 0.9638 - val_loss: 0.3483 - val_acc: 0.9124\n",
            "Epoch 991/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.1339 - acc: 0.9554 - val_loss: 0.3617 - val_acc: 0.9094\n",
            "Epoch 992/1000\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.1268 - acc: 0.9554 - val_loss: 0.3442 - val_acc: 0.9161\n",
            "Epoch 993/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.1132 - acc: 0.9599 - val_loss: 0.3340 - val_acc: 0.9063\n",
            "Epoch 994/1000\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 0.1288 - acc: 0.9557 - val_loss: 0.3234 - val_acc: 0.9222\n",
            "Epoch 995/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.1137 - acc: 0.9638 - val_loss: 0.3372 - val_acc: 0.9186\n",
            "Epoch 996/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 0.1347 - acc: 0.9532 - val_loss: 0.3578 - val_acc: 0.9106\n",
            "Epoch 997/1000\n",
            "3315/3315 [==============================] - 2s 496us/step - loss: 0.1281 - acc: 0.9569 - val_loss: 0.3536 - val_acc: 0.9137\n",
            "Epoch 998/1000\n",
            "3315/3315 [==============================] - 2s 499us/step - loss: 0.1297 - acc: 0.9578 - val_loss: 0.3184 - val_acc: 0.9143\n",
            "Epoch 999/1000\n",
            "3315/3315 [==============================] - 2s 506us/step - loss: 0.1217 - acc: 0.9608 - val_loss: 0.3239 - val_acc: 0.9216\n",
            "Epoch 1000/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 0.1131 - acc: 0.9590 - val_loss: 0.3428 - val_acc: 0.9186\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mFytY6LDzgJ0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's plot the loss:"
      ]
    },
    {
      "metadata": {
        "id": "TFz4ClZov9gZ",
        "colab_type": "code",
        "outputId": "9aa55f52-22d6-4f4d-c6b6-26ddf2f80d0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.plot(cnnhistory.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEVCAYAAAAb/KWvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Vf1W7LgP2DA5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "And now let's plot the accuracy:"
      ]
    },
    {
      "metadata": {
        "id": "8yyFBt7ASPUe",
        "colab_type": "code",
        "outputId": "7bc8eb5c-ac78-409a-a5e2-720d250923a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(cnnhistory.history['acc'])\n",
        "plt.plot(cnnhistory.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npBmgR6MZAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "gaZONl1mD8XD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now create a classification report to review the f1-score of the model per class.\n",
        "To do so, we have to:\n",
        "- Create a variable predictions that will contain the model.predict_classes outcome\n",
        "- Convert our y_test (array of strings with our classes) to an array of int called new_Ytest, otherwise it will not be comparable to the predictions by the classification report."
      ]
    },
    {
      "metadata": {
        "id": "EO25uIL-9vqx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict_classes(x_testcnn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1i06grlBBSrn",
        "colab_type": "code",
        "outputId": "5d41fbd6-2b99-419a-e93a-37aad390aeb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 4, 3, ..., 1, 4, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "metadata": {
        "id": "HUHshx93CM_6",
        "colab_type": "code",
        "outputId": "e7d37848-5f08-4c9f-b703-b98bf59fb6bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 4, 3, ..., 1, 4, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "metadata": {
        "id": "tMxojpvWCxOs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_Ytest = y_test.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W07EQaC8DE6i",
        "colab_type": "code",
        "outputId": "fbde01c6-4edc-45e4-b39b-f620c34db96e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "new_Ytest"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 4, 3, ..., 1, 4, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "metadata": {
        "id": "FW2XHdTtEedk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Okay, now we can display the classification report:"
      ]
    },
    {
      "metadata": {
        "id": "IfVSRmMu96rC",
        "colab_type": "code",
        "outputId": "29ba887e-73f7-4f4e-e732-d0c1c8f301bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(new_Ytest, predictions)\n",
        "print(report)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.96      0.94       134\n",
            "           1       0.97      0.90      0.93       251\n",
            "           2       0.92      0.91      0.91       242\n",
            "           3       0.86      0.89      0.88       271\n",
            "           4       0.96      0.96      0.96       253\n",
            "           5       0.94      0.90      0.92       239\n",
            "           6       0.85      0.95      0.90       127\n",
            "           7       0.88      0.91      0.90       116\n",
            "\n",
            "   micro avg       0.92      0.92      0.92      1633\n",
            "   macro avg       0.91      0.92      0.92      1633\n",
            "weighted avg       0.92      0.92      0.92      1633\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hu1S5IowfSDG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And now, the confusion matrix: it will show us the misclassified samples"
      ]
    },
    {
      "metadata": {
        "id": "fdy09SCEd7Cl",
        "colab_type": "code",
        "outputId": "f8d72fcc-819d-457e-f458-b0dd41ea6616",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "matrix = confusion_matrix(new_Ytest, predictions)\n",
        "print (matrix)\n",
        "\n",
        "# 0 = neutral, 1 = calm, 2 = happy, 3 = sad, 4 = angry, 5 = fearful, 6 = disgust, 7 = surprised"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[128   2   0   3   0   0   1   0]\n",
            " [  2 226   7   8   0   0   8   0]\n",
            " [  4   1 220   4   5   6   0   2]\n",
            " [  2   2   2 241   3   5   7   9]\n",
            " [  2   0   2   2 244   0   1   2]\n",
            " [  1   0   2  19   0 214   2   1]\n",
            " [  0   0   2   2   2   0 121   0]\n",
            " [  0   2   4   0   0   2   2 106]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x_ySPOyHxkZ3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Save the model"
      ]
    },
    {
      "metadata": {
        "id": "f5kRmoD-sdHj",
        "colab_type": "code",
        "outputId": "a3674fa2-2288-4f5b-fc3a-e50b4ab40018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
        "save_dir = '/content/drive/My Drive/Ravdess_model'\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved trained model at /content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MNUiznKNwUtJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reloading the model to test it"
      ]
    },
    {
      "metadata": {
        "id": "T4oAv6Kx8RBE",
        "colab_type": "code",
        "outputId": "aff2e485-12a4-4bc9-ceac-d4de159e6a27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "cell_type": "code",
      "source": [
        "loaded_model = keras.models.load_model('/content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5')\n",
        "loaded_model.summary()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_3 (Conv1D)            (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 5, 128)            82048     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 8)                 5128      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 8)                 0         \n",
            "=================================================================\n",
            "Total params: 87,944\n",
            "Trainable params: 87,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FHtPzc0Y8hfZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Checking the accuracy of the loaded model"
      ]
    },
    {
      "metadata": {
        "id": "qUi-Zjuf8hDB",
        "colab_type": "code",
        "outputId": "489d4963-c033-48fb-8a82-48e870dac521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "loss, acc = loaded_model.evaluate(x_testcnn, y_test)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1633/1633 [==============================] - 0s 141us/step\n",
            "Restored model, accuracy: 91.86%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8pXH3y7S9A1N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Thank you for your attention! To be continued.."
      ]
    }
  ]
}